{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Dict, Tuple\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff427ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_summary_prompt():\n",
    "    \"\"\"Create the prompt template for job description summarization\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are an expert HR professional and recruiter. Please analyze the following job description and provide a comprehensive summary in JSON format.\n",
    "\n",
    "    Job Description:\n",
    "    {job_description}\n",
    "\n",
    "    Please extract and organize the information into the following structure:\n",
    "\n",
    "    **Job Information**\n",
    "    - Extract job title from the description\n",
    "    - Determine job level based on responsibilities and requirements (Entry/Junior/Mid-level/Senior/Lead/Executive)\n",
    "    - Determine work type: \"Remote\", \"Hybrid\", or \"On-site\"\n",
    "\n",
    "    **Compensation**\n",
    "    - Extract salary information:\n",
    "      - Min salary: minimum salary as number without commas\n",
    "      - Max salary: maximum salary as number without commas\n",
    "      - If range given (e.g., \"$120,000 - $160,000\"), extract both values\n",
    "      - If single value given, use same for min and max\n",
    "      - If not mentioned, use null for both\n",
    "\n",
    "    **Overview**\n",
    "    - Provide a brief 1-3 sentence summary of the position and its primary purpose\n",
    "\n",
    "    **Responsibilities**\n",
    "    - List the 3-5 most important responsibilities/duties\n",
    "    - Focus on core functions, not minor tasks\n",
    "\n",
    "    **Requirements\n",
    "    - Skills: Extract all mentioned technical skills, programming languages, platforms (no commas in values), don't include soft skills\n",
    "    - Experience: Determine years_min(as numbers), and level (junior/mid/senior)\n",
    "    - Qualifications: List requirements, experience, education\n",
    "    \n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        input_variables=[\"job_description\"],\n",
    "        template=template,\n",
    "        partial_variables={\"format_instructions\": \"{format_instructions}\"}\n",
    "    )\n",
    "\n",
    "class JobDescriptionSummarizer:\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\", temperature=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the job description summarizer\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): OpenAI model to use\n",
    "            temperature (float): Temperature for text generation (0.0-2.0)\n",
    "                                0.0 = deterministic, focused responses\n",
    "                                1.0 = balanced creativity and consistency  \n",
    "                                2.0 = highly creative, unpredictable responses\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Define the expected JSON schema\n",
    "        self.json_schema = {\n",
    "            \"job_id\": \"Job ID to be added later\",\n",
    "            \"job_title\": \"Extracted job title\",\n",
    "            \"job_level\": \"Junior/Mid-level/Senior/Lead/Executive\",\n",
    "            \"workplace_type\": \"Remote/Hybrid/On-site\",\n",
    "            \"compensation\": {\n",
    "                \"salary\": {\n",
    "                    \"min\": \"Minimum salary as number or null\",\n",
    "                    \"max\": \"Maximum salary as number or null\"\n",
    "                }\n",
    "            },\n",
    "            \"overview\": \"Brief 1-3 sentence summary\",\n",
    "            \"responsibilities\": [\"List of 3-5 key responsibilities\"],\n",
    "            \"requirements\": {\n",
    "                \"skills\": [\"Technical skills array\"],\n",
    "                \"experience\": {\n",
    "                \"years_min\": \"Minimum years as number\",\n",
    "                \"level\": \"entry/junior/mid/senior\"\n",
    "                },\n",
    "                \"qualifications\": [\"Essential qualifications array\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize JSON parser with schema\n",
    "        self.parser = JsonOutputParser(pydantic_object=None)\n",
    "        self.prompt = create_job_summary_prompt()\n",
    "        \n",
    "        # Create the chain with format instructions\n",
    "        self.chain = self.prompt.partial(format_instructions=self.parser.get_format_instructions()) | self.llm | self.parser\n",
    "    \n",
    "    def summarize(self, job_description: str) -> dict:\n",
    "        \"\"\"\n",
    "        Summarize a job description\n",
    "        \n",
    "        Args:\n",
    "            job_description (str): The full job description text\n",
    "            \n",
    "        Returns:\n",
    "            dict: Structured summary of the job description\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate summary using LCEL chain with JSON parser\n",
    "            result = self.chain.invoke({\"job_description\": job_description})\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"summary\": result,\n",
    "                \"raw_output\": str(result)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"summary\": None,\n",
    "                \"raw_output\": None\n",
    "            }\n",
    "    \n",
    "    def process_single_job(self, job_data: Tuple[str, str]) -> dict:\n",
    "        \"\"\"\n",
    "        Process a single job description\n",
    "        \n",
    "        Args:\n",
    "            job_data: Tuple of (job_id, job_description)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Processing result with job_id and summary\n",
    "        \"\"\"\n",
    "        job_id, job_description = job_data\n",
    "        \n",
    "        try:\n",
    "            # Check if job description is empty or NaN\n",
    "            if pd.isna(job_description) or str(job_description).strip() == \"\" or str(job_description).lower() == \"nan\":\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"Job description is empty or missing\",\n",
    "                    \"summary\": None,\n",
    "                    \"job_id\": job_id\n",
    "                }\n",
    "            \n",
    "            # Summarize the job description\n",
    "            summary_result = self.summarize(str(job_description))\n",
    "            \n",
    "            # Add job ID to the summary\n",
    "            if summary_result[\"success\"] and summary_result[\"summary\"]:\n",
    "                summary_result[\"summary\"][\"job_id\"] = job_id\n",
    "            \n",
    "            summary_result[\"job_id\"] = job_id\n",
    "            \n",
    "            return summary_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"summary\": None,\n",
    "                \"job_id\": job_id\n",
    "            }\n",
    "    \n",
    "    def process_batch_concurrent(self, batch_data: List[Tuple[str, str]], max_workers: int = 5) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Process a batch of jobs concurrently\n",
    "        \n",
    "        Args:\n",
    "            batch_data: List of tuples (job_id, job_description)\n",
    "            max_workers: Maximum number of concurrent workers\n",
    "            \n",
    "        Returns:\n",
    "            List of processing results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all jobs in the batch\n",
    "            future_to_job = {executor.submit(self.process_single_job, job_data): job_data[0] \n",
    "                            for job_data in batch_data}\n",
    "            \n",
    "            # Process completed futures\n",
    "            for future in as_completed(future_to_job):\n",
    "                job_id = future_to_job[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    if result[\"success\"]:\n",
    "                        print(f\"✅ Successfully processed job ID: {job_id}\")\n",
    "                    else:\n",
    "                        print(f\"❌ Failed to process job ID: {job_id} - {result['error']}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Exception processing job ID: {job_id} - {str(e)}\")\n",
    "                    results.append({\n",
    "                        \"success\": False,\n",
    "                        \"error\": str(e),\n",
    "                        \"summary\": None,\n",
    "                        \"job_id\": job_id\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def summarize_all_from_parquet(self, parquet_file_path: str, \n",
    "                                  output_file: str = \"jobs_summaries.json\",\n",
    "                                  batch_size: int = 50,\n",
    "                                  max_workers: int = 5,\n",
    "                                  id_column: str = \"id\", \n",
    "                                  jd_column: str = \"JD\",\n",
    "                                  limit: int = None) -> dict:\n",
    "        \"\"\"\n",
    "        Process all job descriptions from a parquet file in batches with concurrent processing\n",
    "        \n",
    "        Args:\n",
    "            parquet_file_path: Path to the parquet file\n",
    "            output_file: Output JSON file path\n",
    "            batch_size: Number of jobs to process in each batch\n",
    "            max_workers: Maximum number of concurrent workers per batch\n",
    "            id_column: Name of the ID column\n",
    "            jd_column: Name of the job description column\n",
    "            limit: Optional limit on total number of jobs to process\n",
    "            \n",
    "        Returns:\n",
    "            dict: Summary of processing results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Read parquet file\n",
    "            print(f\"📖 Reading parquet file: {parquet_file_path}\")\n",
    "            df = pd.read_parquet(parquet_file_path)\n",
    "            \n",
    "            # Validate columns\n",
    "            if id_column not in df.columns or jd_column not in df.columns:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"Required columns not found. Available: {list(df.columns)}\"\n",
    "                }\n",
    "            \n",
    "            # Apply limit if specified\n",
    "            if limit:\n",
    "                df = df.head(limit)\n",
    "            \n",
    "            total_jobs = len(df)\n",
    "            print(f\"📊 Total jobs to process: {total_jobs}\")\n",
    "            print(f\"📦 Batch size: {batch_size}\")\n",
    "            print(f\"👷 Max workers per batch: {max_workers}\")\n",
    "            \n",
    "            # Prepare job data\n",
    "            job_data = [(str(row[id_column]), row[jd_column]) \n",
    "                        for _, row in df.iterrows()]\n",
    "            \n",
    "            # Process in batches\n",
    "            all_summaries = []\n",
    "            successful_summaries = []\n",
    "            failed_jobs = []\n",
    "            \n",
    "            for batch_num in range(0, total_jobs, batch_size):\n",
    "                batch_end = min(batch_num + batch_size, total_jobs)\n",
    "                batch = job_data[batch_num:batch_end]\n",
    "                \n",
    "                print(f\"\\n🔄 Processing batch {batch_num//batch_size + 1} \" +\n",
    "                      f\"(jobs {batch_num + 1} to {batch_end} of {total_jobs})\")\n",
    "                \n",
    "                batch_results = self.process_batch_concurrent(batch, max_workers)\n",
    "                \n",
    "                for result in batch_results:\n",
    "                    all_summaries.append(result)\n",
    "                    if result[\"success\"] and result[\"summary\"]:\n",
    "                        successful_summaries.append(result[\"summary\"])\n",
    "                    else:\n",
    "                        failed_jobs.append({\n",
    "                            \"job_id\": result[\"job_id\"],\n",
    "                            \"error\": result.get(\"error\", \"Unknown error\")\n",
    "                        })\n",
    "                \n",
    "                # Progress update\n",
    "                print(f\"📈 Progress: {len(all_summaries)}/{total_jobs} jobs processed\")\n",
    "            \n",
    "            # Save successful summaries to JSON file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(successful_summaries, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary_stats = {\n",
    "                \"success\": True,\n",
    "                \"total_jobs\": total_jobs,\n",
    "                \"successful\": len(successful_summaries),\n",
    "                \"failed\": len(failed_jobs),\n",
    "                \"output_file\": output_file,\n",
    "                \"elapsed_time\": f\"{elapsed_time:.2f} seconds\",\n",
    "                \"failed_jobs\": failed_jobs[:10] if failed_jobs else []  # Show first 10 failures\n",
    "            }\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"📊 PROCESSING COMPLETE\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"✅ Successful: {summary_stats['successful']}\")\n",
    "            print(f\"❌ Failed: {summary_stats['failed']}\")\n",
    "            print(f\"💾 Output saved to: {output_file}\")\n",
    "            print(f\"⏱️  Total time: {summary_stats['elapsed_time']}\")\n",
    "            \n",
    "            if failed_jobs:\n",
    "                print(f\"\\n⚠️  First few failed jobs:\")\n",
    "                for job in failed_jobs[:5]:\n",
    "                    print(f\"   - Job ID {job['job_id']}: {job['error']}\")\n",
    "            \n",
    "            return summary_stats\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Parquet file not found: {parquet_file_path}\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"Error processing parquet file: {str(e)}\"\n",
    "            }\n",
    "\n",
    "def print_job_summary(summary_result: dict):\n",
    "    \"\"\"Pretty print the job summary result in JSON format\"\"\"\n",
    "    \n",
    "    if not summary_result[\"success\"]:\n",
    "        print(f\"❌ Error for Job ID {summary_result.get('job_id', 'Unknown')}: {summary_result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📋 JOB DESCRIPTION SUMMARY - ID: {summary_result.get('job_id', 'Unknown')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary = summary_result[\"summary\"]\n",
    "    \n",
    "    # Print JSON in a readable format\n",
    "    print(\"📄 JSON OUTPUT:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Add job_id to the summary for complete JSON\n",
    "    complete_summary = {\"job_id\": summary_result.get('job_id', 'Unknown')}\n",
    "    complete_summary.update(summary)\n",
    "    \n",
    "    # Pretty print JSON\n",
    "    print(json.dumps(complete_summary, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4404d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the summarizer\n",
    "    summarizer = JobDescriptionSummarizer()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"JOB DESCRIPTION SUMMARIZER - CONCURRENT BATCH PROCESSING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    parquet_file_path = \"Data_Engineer-20250727.parquet\"  # Your parquet file path\n",
    "    output_file = \"jobs_summaries.json\"  # Output JSON file\n",
    "    batch_size = 50  # Process 50 jobs per batch\n",
    "    max_workers = 5  # Number of concurrent workers per batch\n",
    "    \n",
    "    # Process all jobs from the parquet file\n",
    "    result = summarizer.summarize_all_from_parquet(\n",
    "        parquet_file_path=parquet_file_path,\n",
    "        output_file=output_file,\n",
    "        batch_size=batch_size,\n",
    "        max_workers=max_workers,\n",
    "        id_column=\"job_posting_id\",    # Change if your ID column has different name\n",
    "        jd_column=\"job_summary\",        # Change if your JD column has different name\n",
    "        limit=None  # Set to a number to limit total jobs processed (useful for testing)\n",
    "    )\n",
    "    \n",
    "    # Print final summary\n",
    "    if result[\"success\"]:\n",
    "        print(f\"\\n✅ Successfully processed {result['successful']} job descriptions!\")\n",
    "        print(f\"📄 Results saved to: {result['output_file']}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Error: {result['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"💡 USAGE TIPS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"1. Update 'parquet_file_path' with your actual parquet file path\")\n",
    "    print(\"2. Adjust 'batch_size' to control how many jobs are processed together\")\n",
    "    print(\"3. Adjust 'max_workers' to control concurrent processing (be mindful of API rate limits)\")\n",
    "    print(\"4. Set 'limit' parameter to test with a smaller subset first\")\n",
    "    print(\"5. Update column names if they're different from defaults\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
