{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4037da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20bdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Qdrant imports\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance, \n",
    "    VectorParams, \n",
    "    PointStruct,\n",
    "    CollectionStatus,\n",
    "    OptimizersConfigDiff,\n",
    "    HnswConfigDiff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78988f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobChunkingEmbeddingPipeline:\n",
    "    def __init__(self, \n",
    "                 openai_api_key: str = None,\n",
    "                 qdrant_url: str = None,\n",
    "                 qdrant_port: int = None,\n",
    "                 qdrant_api_key: str = None,\n",
    "                 collection_name: str = \"jobs\",\n",
    "                 embedding_model: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with necessary configurations\n",
    "        \"\"\"\n",
    "        # Use provided values or fall back to environment variables\n",
    "        openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        qdrant_url = qdrant_url or os.getenv(\"QDRANT_URL\", \"localhost\")\n",
    "        qdrant_port = qdrant_port or int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "        qdrant_api_key = qdrant_api_key or os.getenv(\"QDRANT_API_KEY\")\n",
    "        \n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key not found. Please set OPENAI_API_KEY in .env file\")\n",
    "        \n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            openai_api_key=openai_api_key,\n",
    "            model=embedding_model\n",
    "        )\n",
    "        \n",
    "        # Initialize Qdrant client\n",
    "        if qdrant_api_key:\n",
    "            # For Qdrant Cloud\n",
    "            self.client = QdrantClient(\n",
    "                url=qdrant_url,\n",
    "                api_key=qdrant_api_key,\n",
    "                prefer_grpc=True\n",
    "            )\n",
    "            logger.info(f\"Connected to Qdrant Cloud at {qdrant_url}\")\n",
    "        else:\n",
    "            # For local Qdrant\n",
    "            self.client = QdrantClient(host=qdrant_url, port=qdrant_port)\n",
    "            logger.info(f\"Connected to local Qdrant at {qdrant_url}:{qdrant_port}\")\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Initialize collections\n",
    "        self._setup_collections()\n",
    "        \n",
    "    def _setup_collections(self):\n",
    "        \"\"\"Create Qdrant collections if they don't exist\"\"\"\n",
    "        collections = self.client.get_collections().collections\n",
    "        existing_collections = [col.name for col in collections]\n",
    "        \n",
    "        if self.collection_name not in existing_collections:\n",
    "            logger.info(f\"Creating collection: {self.collection_name}\")\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=1536,  # OpenAI text-embedding-3-small dimension\n",
    "                    distance=Distance.COSINE\n",
    "                ),\n",
    "                optimizers_config=OptimizersConfigDiff(\n",
    "                    default_segment_number=2,\n",
    "                    indexing_threshold=10000,\n",
    "                ),\n",
    "                hnsw_config=HnswConfigDiff(\n",
    "                    m=16,\n",
    "                    ef_construct=100\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Create payload indexes for efficient filtering\n",
    "            self._create_payload_indexes()\n",
    "        else:\n",
    "            logger.info(f\"Collection {self.collection_name} already exists\")\n",
    "    \n",
    "    def _create_payload_indexes(self):\n",
    "        \"\"\"Create indexes on frequently filtered fields\"\"\"\n",
    "        index_fields = [\n",
    "            (\"job_id\", \"keyword\"),\n",
    "            (\"chunk_type\", \"keyword\"),\n",
    "            (\"job_title\", \"text\"),\n",
    "            (\"job_level\", \"keyword\"),\n",
    "            (\"work_type\", \"keyword\"),\n",
    "            (\"skills\", \"keyword[]\"),\n",
    "            (\"created_date\", \"datetime\")\n",
    "        ]\n",
    "        \n",
    "        for field_name, field_type in index_fields:\n",
    "            try:\n",
    "                self.client.create_payload_index(\n",
    "                    collection_name=self.collection_name,\n",
    "                    field_name=field_name,\n",
    "                    field_schema=field_type\n",
    "                )\n",
    "                logger.info(f\"Created index for field: {field_name}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Index for {field_name} might already exist: {e}\")\n",
    "    \n",
    "    def create_job_chunks(self, job: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create multiple chunks for a single job\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Extract job information\n",
    "        job_info = job.get(\"Job Information\", {})\n",
    "        job_id = job.get(\"job_id\", \"unknown\")\n",
    "        job_title = job_info.get(\"job_title\") or job_info.get(\"Job Title\", \"Unknown\")\n",
    "        job_level = job_info.get(\"job_level\") or job_info.get(\"Job Level\", \"Unknown\")\n",
    "        work_type = job_info.get(\"work_type\") or job_info.get(\"Work Type\", \"Unknown\")\n",
    "        \n",
    "        # Extract other fields\n",
    "        compensation = job.get(\"Compensation\", {})\n",
    "        overview = job.get(\"Overview\", \"\")\n",
    "        responsibilities = job.get(\"Responsibilities\", [])\n",
    "        requirements = job.get(\"Requirements\", {})\n",
    "        skills = requirements.get(\"Skills\") or requirements.get(\"skills\", [])\n",
    "        experience = requirements.get(\"Experience\") or requirements.get(\"experience\", {})\n",
    "        qualifications = requirements.get(\"Qualifications\") or requirements.get(\"qualifications\", [])\n",
    "        \n",
    "        # 1. Create Full Job Chunk\n",
    "        full_text = f\"\"\"Job Title: {job_title}\n",
    "Job Level: {job_level}\n",
    "Work Type: {work_type}\n",
    "Overview: {overview}\n",
    "Responsibilities: {' '.join(responsibilities) if isinstance(responsibilities, list) else responsibilities}\n",
    "Required Skills: {', '.join(skills) if isinstance(skills, list) else skills}\n",
    "Experience: {experience.get('years_min', 'Not specified')} years minimum, Level: {experience.get('level', 'Not specified')}\n",
    "Qualifications: {' '.join(qualifications) if isinstance(qualifications, list) else qualifications}\"\"\"\n",
    "        \n",
    "        chunks.append({\n",
    "            'id': f\"job_{job_id}_full\",\n",
    "            'text': full_text,\n",
    "            'metadata': {\n",
    "                'chunk_type': 'full_job',\n",
    "                'job_id': job_id,\n",
    "                'job_title': job_title,\n",
    "                'job_level': job_level,\n",
    "                'work_type': work_type,\n",
    "                'skills': skills if isinstance(skills, list) else [],\n",
    "                'years_required': experience.get('years_min') or experience.get('Years_min'),\n",
    "                'experience_level': experience.get('level') or experience.get('Level'),\n",
    "                'created_date': datetime.now().isoformat()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # 2. Create Skills-Focused Chunk\n",
    "        skills_text = f\"\"\"{job_title} position requires the following skills:\n",
    "Technical Skills: {', '.join(skills) if isinstance(skills, list) else skills}\n",
    "Experience Level: {experience.get('level', 'Not specified')}\n",
    "Minimum Years: {experience.get('years_min', 'Not specified')}\n",
    "Key Qualifications: {' '.join(qualifications[:3]) if isinstance(qualifications, list) else qualifications}\"\"\"\n",
    "        \n",
    "        chunks.append({\n",
    "            'id': f\"job_{job_id}_skills\",\n",
    "            'text': skills_text,\n",
    "            'metadata': {\n",
    "                'chunk_type': 'job_skills',\n",
    "                'job_id': job_id,\n",
    "                'job_title': job_title,\n",
    "                'job_level': job_level,\n",
    "                'skills': skills if isinstance(skills, list) else [],\n",
    "                'experience_level': experience.get('level') or experience.get('Level'),\n",
    "                'created_date': datetime.now().isoformat()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # 3. Create Responsibilities Chunk\n",
    "        resp_text = f\"\"\"{job_title} responsibilities include:\n",
    "{' '.join(responsibilities) if isinstance(responsibilities, list) else responsibilities}\n",
    "This role is {job_level} level and offers {work_type} work arrangement.\"\"\"\n",
    "        \n",
    "        chunks.append({\n",
    "            'id': f\"job_{job_id}_responsibilities\",\n",
    "            'text': resp_text,\n",
    "            'metadata': {\n",
    "                'chunk_type': 'job_responsibilities',\n",
    "                'job_id': job_id,\n",
    "                'job_title': job_title,\n",
    "                'job_level': job_level,\n",
    "                'work_type': work_type,\n",
    "                'created_date': datetime.now().isoformat()\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_jobs_batch(self, jobs: List[Dict[str, Any]], batch_size: int = 50):\n",
    "        \"\"\"Process jobs in batches\"\"\"\n",
    "        total_jobs = len(jobs)\n",
    "        logger.info(f\"Processing {total_jobs} jobs in batches of {batch_size}\")\n",
    "        \n",
    "        for i in range(0, total_jobs, batch_size):\n",
    "            batch = jobs[i:i + batch_size]\n",
    "            batch_chunks = []\n",
    "            batch_texts = []\n",
    "            \n",
    "            # Create chunks for all jobs in batch\n",
    "            for job in batch:\n",
    "                chunks = self.create_job_chunks(job)\n",
    "                for chunk in chunks:\n",
    "                    batch_chunks.append(chunk)\n",
    "                    batch_texts.append(chunk['text'])\n",
    "            \n",
    "            # Generate embeddings for all chunks in batch\n",
    "            logger.info(f\"Generating embeddings for {len(batch_texts)} chunks...\")\n",
    "            embeddings = self.embeddings.embed_documents(batch_texts)\n",
    "            \n",
    "            # Prepare points for Qdrant\n",
    "            points = []\n",
    "            for chunk, embedding in zip(batch_chunks, embeddings):\n",
    "                point = PointStruct(\n",
    "                    id=chunk['id'],\n",
    "                    vector=embedding,\n",
    "                    payload={\n",
    "                        'text': chunk['text'],\n",
    "                        **chunk['metadata']\n",
    "                    }\n",
    "                )\n",
    "                points.append(point)\n",
    "            \n",
    "            # Upsert to Qdrant\n",
    "            logger.info(f\"Upserting {len(points)} points to Qdrant...\")\n",
    "            self.client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=points,\n",
    "                wait=True\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Completed batch {i//batch_size + 1}/{(total_jobs + batch_size - 1)//batch_size}\")\n",
    "    \n",
    "    def get_collection_stats(self):\n",
    "        \"\"\"Get statistics about the collection\"\"\"\n",
    "        collection_info = self.client.get_collection(self.collection_name)\n",
    "        return {\n",
    "            'total_points': collection_info.points_count,\n",
    "            'vectors_count': collection_info.vectors_count,\n",
    "            'indexed_vectors': collection_info.indexed_vectors_count,\n",
    "            'status': collection_info.status\n",
    "        }\n",
    "    \n",
    "    def search_jobs(self, query: str, limit: int = 5):\n",
    "        \"\"\"Search for jobs using semantic search\"\"\"\n",
    "        # Generate embedding for query\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        \n",
    "        # Search in Qdrant\n",
    "        search_results = self.client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=limit,\n",
    "            with_payload=True\n",
    "        )\n",
    "        \n",
    "        return search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7140ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load configuration from environment variables\n",
    "    logger.info(\"Loading configuration from environment...\")\n",
    "    \n",
    "    # All configuration is loaded from .env file\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "    QDRANT_URL = os.getenv(\"QDRANT_URL\", \"localhost\")\n",
    "    QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", \"6333\"))\n",
    "    QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")  # Optional, for Qdrant Cloud\n",
    "    COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"jobs\")\n",
    "    EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"50\"))\n",
    "    \n",
    "    # Validate required environment variables\n",
    "    if not OPENAI_API_KEY:\n",
    "        logger.error(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        logger.error(\"Please create a .env file with your API keys\")\n",
    "        return\n",
    "    \n",
    "    # Load jobs data\n",
    "    jobs_file = os.getenv(\"JOBS_FILE\", \"jobs_summaries.json\")\n",
    "    logger.info(f\"Loading jobs data from {jobs_file}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(jobs_file, 'r') as f:\n",
    "            jobs_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Jobs file '{jobs_file}' not found\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"Invalid JSON in '{jobs_file}'\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(jobs_data)} jobs\")\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    try:\n",
    "        pipeline = JobChunkingEmbeddingPipeline(\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            qdrant_url=QDRANT_URL,\n",
    "            qdrant_port=QDRANT_PORT,\n",
    "            qdrant_api_key=QDRANT_API_KEY,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            embedding_model=EMBEDDING_MODEL\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize pipeline: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Process jobs\n",
    "    pipeline.process_jobs_batch(jobs_data, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Get collection statistics\n",
    "    stats = pipeline.get_collection_stats()\n",
    "    logger.info(f\"Collection statistics: {stats}\")\n",
    "    \n",
    "    # Test search functionality if enabled\n",
    "    if os.getenv(\"RUN_SEARCH_TEST\", \"true\").lower() == \"true\":\n",
    "        logger.info(\"\\nTesting search functionality...\")\n",
    "        test_queries = [\n",
    "            \"frontend engineer with React experience\",\n",
    "            \"data engineer with Python and AWS\",\n",
    "            \"senior level remote positions\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            logger.info(f\"\\nSearching for: '{query}'\")\n",
    "            results = pipeline.search_jobs(query, limit=3)\n",
    "            for i, result in enumerate(results):\n",
    "                logger.info(f\"  Result {i+1}: {result.payload.get('job_title')} (Score: {result.score:.3f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JRDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
