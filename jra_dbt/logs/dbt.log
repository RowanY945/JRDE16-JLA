[0m21:02:20.748293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295204FF5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029520BB09B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029520BB0320>]}


============================== 21:02:20.753413 | e65f5548-b7f8-45cd-9c98-b0f02eebbfe6 ==============================
[0m21:02:20.753413 [info ] [MainThread]: Running with dbt=1.10.4
[0m21:02:20.753413 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt debug', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m21:02:20.775582 [info ] [MainThread]: dbt version: 1.10.4
[0m21:02:20.776599 [info ] [MainThread]: python version: 3.12.10
[0m21:02:20.776599 [info ] [MainThread]: python path: C:\Users\George\AppData\Local\Programs\Python\Python312\python.exe
[0m21:02:20.776599 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m21:02:23.494866 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:02:23.494866 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:02:23.494866 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:02:25.832065 [info ] [MainThread]: Using profiles dir at C:\Users\George\.dbt
[0m21:02:25.832065 [info ] [MainThread]: Using profiles.yml file at C:\Users\George\.dbt\profiles.yml
[0m21:02:25.833065 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\dbt_project.yml
[0m21:02:25.833065 [info ] [MainThread]: adapter type: databricks
[0m21:02:25.834028 [info ] [MainThread]: adapter version: 1.10.4
[0m21:02:25.930085 [info ] [MainThread]: Configuration:
[0m21:02:25.931003 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m21:02:25.931863 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m21:02:25.932265 [info ] [MainThread]: Required dependencies:
[0m21:02:25.932265 [debug] [MainThread]: Executing "git --help"
[0m21:02:25.964469 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:02:25.964469 [debug] [MainThread]: STDERR: "b''"
[0m21:02:25.964469 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m21:02:25.965469 [info ] [MainThread]: Connection:
[0m21:02:25.965469 [info ] [MainThread]:   host: https://dbc-22274c33-f88d.cloud.databricks.com
[0m21:02:25.965469 [info ] [MainThread]:   http_path: sql/protocolv1/o/551405405186610/0805-064911-zdmrk3ir
[0m21:02:25.966477 [info ] [MainThread]:   catalog: demo
[0m21:02:25.966477 [info ] [MainThread]:   schema: demo_schema
[0m21:02:25.967520 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m21:02:26.331456 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:02:26.332462 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m21:02:26.332462 [debug] [MainThread]: Using databricks connection "debug"
[0m21:02:26.332462 [debug] [MainThread]: On debug: select 1 as id
[0m21:02:26.332462 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:02:27.434917 [debug] [MainThread]: Databricks adapter: Connection(session-id=0f45453e-a404-404a-80bd-4d1bb600fd93) - Created
[0m21:02:35.805020 [debug] [MainThread]: SQL status: OK in 9.470 seconds
[0m21:02:35.807081 [debug] [MainThread]: Databricks adapter: Cursor(session-id=0f45453e-a404-404a-80bd-4d1bb600fd93, command-id=8faba451-ebd3-4605-a3af-45d3fe5723bc) - Closing
[0m21:02:36.094553 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m21:02:36.096585 [info ] [MainThread]: [32mAll checks passed![0m
[0m21:02:36.099631 [debug] [MainThread]: Command `dbt debug` succeeded at 21:02:36.098645 after 15.49 seconds
[0m21:02:36.100639 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m21:02:36.101628 [debug] [MainThread]: On debug: Close
[0m21:02:36.101628 [debug] [MainThread]: Databricks adapter: Connection(session-id=0f45453e-a404-404a-80bd-4d1bb600fd93) - Closing
[0m21:02:36.608720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029544047530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295239A7320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029520A01730>]}
[0m21:02:36.609706 [debug] [MainThread]: Flushing usage events
[0m21:02:37.624861 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:03:07.606799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE31CA300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE31CAFF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE31C9970>]}


============================== 21:03:07.610836 | 99e2be0c-7e74-44d4-a2c0-e617181bbe5e ==============================
[0m21:03:07.610836 [info ] [MainThread]: Running with dbt=1.10.4
[0m21:03:07.610836 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m21:03:08.328073 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:03:08.329269 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:03:08.329269 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:03:09.002788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FFEEB40B0>]}
[0m21:03:09.073333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FFFE248C0>]}
[0m21:03:09.074330 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m21:03:09.354714 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m21:03:09.355661 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:03:09.356663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F871DECC0>]}
[0m21:03:09.387749 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m21:03:09.388754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE3240F20>]}
[0m21:03:11.796707 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m21:03:11.807731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F887825D0>]}
[0m21:03:11.857583 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m21:03:11.878269 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m21:03:11.899021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8751A6F0>]}
[0m21:03:11.899021 [info ] [MainThread]: Found 5 models, 3 sources, 682 macros
[0m21:03:11.900448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F874A4080>]}
[0m21:03:11.901457 [info ] [MainThread]: 
[0m21:03:11.901457 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:03:11.902480 [info ] [MainThread]: 
[0m21:03:11.902480 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:03:11.902480 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:03:11.907493 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:03:11.907493 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m21:03:11.908519 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m21:03:11.908519 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m21:03:11.908519 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:03:12.150259 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=bcf52d2a-2296-42ae-871f-0690b81d07cc) - Created
[0m21:03:14.500333 [debug] [ThreadPool]: SQL status: OK in 2.590 seconds
[0m21:03:14.501341 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=bcf52d2a-2296-42ae-871f-0690b81d07cc, command-id=5cb5a9f2-c409-4e8a-a807-48b0fd373576) - Closing
[0m21:03:14.502335 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:03:14.502335 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m21:03:14.510443 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m21:03:14.510443 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m21:03:14.510443 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:03:14.869365 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=bbfe1106-13ed-4351-a0ad-7a2b96b8ae7d) - Created
[0m21:03:19.602482 [debug] [ThreadPool]: SQL status: OK in 5.090 seconds
[0m21:03:19.623497 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=bbfe1106-13ed-4351-a0ad-7a2b96b8ae7d, command-id=336f7d55-c1b6-4c5e-a166-baecd4695cf2) - Closing
[0m21:03:19.628508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE6930C20>]}
[0m21:03:19.635498 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_raw
[0m21:03:19.635498 [info ] [Thread-1 (]: 1 of 5 START sql incremental model demo_schema.indeed_raw ...................... [RUN]
[0m21:03:19.636765 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_raw, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:03:19.636765 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_raw'
[0m21:03:19.636765 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_raw
[0m21:03:19.642473 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_raw"
[0m21:03:19.645891 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_raw
[0m21:03:19.673535 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:03:19.681782 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:03:19.681782 [debug] [Thread-1 (]: Safe create: False
[0m21:03:19.694825 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m21:03:19.697913 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m21:03:19.697913 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

      create or replace temporary view `indeed_raw__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)
    
[0m21:03:19.699145 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:03:19.932790 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535) - Created
[0m21:03:26.444424 [debug] [Thread-1 (]: SQL status: OK in 6.750 seconds
[0m21:03:26.445423 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=e900edbc-c767-4639-baf2-9930c30994f4) - Closing
[0m21:03:26.562125 [debug] [Thread-1 (]: No existing relation found
[0m21:03:26.579572 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m21:03:26.579572 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
DESCRIBE TABLE `indeed_raw__dbt_tmp`

  
[0m21:03:27.881914 [debug] [Thread-1 (]: SQL status: OK in 1.300 seconds
[0m21:03:27.882920 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=a789af85-f4a7-4071-94c0-397c6d3a017c) - Closing
[0m21:03:27.913275 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m21:03:27.913875 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m21:03:27.913875 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_raw`
  
  (
    
      job_posting_id string,
    
      url string,
    
      job_title string,
    
      company_name string,
    
      company_url string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      is_remote boolean,
    
      job_posted_date string,
    
      MIN_AMOUNT string,
    
      MAX_AMOUNT string,
    
      timestamp timestamp_ntz,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/indeed_raw'
  
  

  
[0m21:03:37.471952 [debug] [Thread-1 (]: SQL status: OK in 9.560 seconds
[0m21:03:37.474992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=b373e6e8-b686-4938-9bc4-944fecd18006) - Closing
[0m21:03:37.562493 [debug] [Thread-1 (]: Applying tags to relation None
[0m21:03:37.572443 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m21:03:37.574445 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    insert into `demo`.`demo_schema`.`indeed_raw` select * from `indeed_raw__dbt_tmp`
  
[0m21:03:48.870986 [debug] [Thread-1 (]: SQL status: OK in 11.290 seconds
[0m21:03:48.873109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=4da7a6d5-a897-41b5-8047-6dbd41b44405) - Closing
[0m21:03:48.959590 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8870FBF0>]}
[0m21:03:48.961589 [info ] [Thread-1 (]: 1 of 5 OK created sql incremental model demo_schema.indeed_raw ................. [[32mOK[0m in 29.32s]
[0m21:03:48.965657 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_raw
[0m21:03:48.966659 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_cleaned_test
[0m21:03:48.967650 [info ] [Thread-1 (]: 2 of 5 START sql incremental model demo_schema.stg_cleaned_test ................ [RUN]
[0m21:03:48.969658 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_raw, now model.jra_dbt.stg_cleaned_test)
[0m21:03:48.969658 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, name=model.jra_dbt.stg_cleaned_test, idle-time=0.012880563735961914s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_raw
[0m21:03:48.970640 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_cleaned_test
[0m21:03:48.975796 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_cleaned_test"
[0m21:03:48.976746 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_cleaned_test
[0m21:03:48.979036 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:03:48.980037 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:03:48.980037 [debug] [Thread-1 (]: Safe create: False
[0m21:03:48.981036 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m21:03:48.982036 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:48.982036 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

      create or replace temporary view `stg_cleaned_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`stg_test2`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_cleaned_test`
    )
    
[0m21:03:50.441799 [debug] [Thread-1 (]: SQL status: OK in 1.460 seconds
[0m21:03:50.443813 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=618e056d-bb3c-462b-b57c-5899e0f7f912) - Closing
[0m21:03:50.444823 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m21:03:50.454806 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:50.454806 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
  
[0m21:03:50.919313 [debug] [Thread-1 (]: SQL status: OK in 0.460 seconds
[0m21:03:50.923735 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=7a895382-daaf-4255-bdf5-dfb550cb579e) - Closing
[0m21:03:50.929738 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:50.930739 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m21:03:51.327894 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m21:03:51.333001 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=f63d1c8f-5bd9-4ac7-b50d-740e7f706fc5) - Closing
[0m21:03:51.338994 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:51.338994 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
    AND is_nullable = 'NO';
  
[0m21:03:52.103640 [debug] [Thread-1 (]: SQL status: OK in 0.760 seconds
[0m21:03:52.108002 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=d2c5a19a-db76-4cba-b672-41340fa29ca7) - Closing
[0m21:03:52.114948 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:52.114948 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:03:53.681665 [debug] [Thread-1 (]: SQL status: OK in 1.570 seconds
[0m21:03:53.685662 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=e7517f23-5702-44c1-977f-df67d1abd576) - Closing
[0m21:03:53.698649 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:53.698649 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:03:56.036560 [debug] [Thread-1 (]: SQL status: OK in 2.340 seconds
[0m21:03:56.038561 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=659ecda8-7739-4ceb-a539-5be78d32c4da) - Closing
[0m21:03:56.043558 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:56.044561 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m21:03:56.515120 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m21:03:56.520202 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=cd7d0726-730a-468f-bc2f-b1b376f52e81) - Closing
[0m21:03:56.528127 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:56.528127 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m21:03:57.121765 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m21:03:57.124751 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=c7164de3-f4ff-4bde-906d-ed80ff5f3f4c) - Closing
[0m21:03:57.127773 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:57.128757 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
describe extended `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m21:03:58.529499 [debug] [Thread-1 (]: SQL status: OK in 1.400 seconds
[0m21:03:58.531505 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=2b98356c-ae64-4324-83b8-2d8690224686) - Closing
[0m21:03:58.541251 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m21:03:58.548308 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_cleaned_test`
[0m21:03:58.565771 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:58.565771 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_cleaned_test`

  
[0m21:03:58.930128 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m21:03:58.932746 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=e4b8df33-b797-47e5-9011-de032e64982f) - Closing
[0m21:03:58.934362 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:58.935370 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `stg_cleaned_test__dbt_tmp`

  
[0m21:03:59.350231 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m21:03:59.352240 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=bdde5eae-16b4-4e0a-8d05-8a53854e6a9c) - Closing
[0m21:03:59.359737 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m21:03:59.360897 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m21:03:59.361904 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_cleaned_test` as DBT_INTERNAL_DEST
    using
        `stg_cleaned_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m21:04:08.635098 [debug] [Thread-1 (]: SQL status: OK in 9.270 seconds
[0m21:04:08.635098 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=fb5bbabe-6b6d-48b7-9d58-bd592f48bdbc) - Closing
[0m21:04:08.720460 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8870FDA0>]}
[0m21:04:08.720460 [info ] [Thread-1 (]: 2 of 5 OK created sql incremental model demo_schema.stg_cleaned_test ........... [[32mOK[0m in 19.75s]
[0m21:04:08.721508 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_cleaned_test
[0m21:04:08.721508 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m21:04:08.722515 [info ] [Thread-1 (]: 3 of 5 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m21:04:08.722515 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_cleaned_test, now model.jra_dbt.stg_jobs_description_new)
[0m21:04:08.723507 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, name=model.jra_dbt.stg_jobs_description_new, idle-time=0.002054452896118164s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_cleaned_test
[0m21:04:08.723507 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m21:04:08.726514 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m21:04:08.728513 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m21:04:08.731681 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:04:08.731681 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:04:08.732682 [debug] [Thread-1 (]: Safe create: False
[0m21:04:08.732682 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m21:04:08.733609 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:08.734608 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m21:04:09.368040 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m21:04:09.371046 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=ce90fbb2-8163-41de-b4a5-0c1dcb44bb45) - Closing
[0m21:04:09.373056 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m21:04:09.382417 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:09.384515 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m21:04:09.783686 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m21:04:09.785691 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=e097a16a-1ef2-477a-b4f7-e78d1e438a5c) - Closing
[0m21:04:09.788685 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:09.788685 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m21:04:10.145138 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m21:04:10.151151 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=941eeab2-c717-4378-b2b3-16577304a250) - Closing
[0m21:04:10.153231 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:10.154230 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m21:04:10.567731 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m21:04:10.569733 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=0d82f49f-f580-4a66-8d2c-9292946d5577) - Closing
[0m21:04:10.572804 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:10.572804 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:11.422658 [debug] [Thread-1 (]: SQL status: OK in 0.850 seconds
[0m21:04:11.428252 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=9d082360-a710-46a8-98b2-697049205997) - Closing
[0m21:04:11.433813 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:11.433813 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:12.457453 [debug] [Thread-1 (]: SQL status: OK in 1.020 seconds
[0m21:04:12.460017 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=5c4f9bab-7afc-4078-b3c0-b3c5fa9476ed) - Closing
[0m21:04:12.462314 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:12.462314 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m21:04:12.855209 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m21:04:12.856210 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=0a518760-e78c-4e7d-b81d-ad421ade6129) - Closing
[0m21:04:12.858583 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:12.858583 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m21:04:13.354956 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m21:04:13.356978 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=8e0fc8eb-6587-44bf-88c1-286516cb07cf) - Closing
[0m21:04:13.358997 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:13.358997 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m21:04:14.309626 [debug] [Thread-1 (]: SQL status: OK in 0.950 seconds
[0m21:04:14.312729 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=557b5819-5d9f-4063-83f9-ed73aae2f7a1) - Closing
[0m21:04:14.313723 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m21:04:14.314722 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m21:04:14.316715 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:14.316715 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m21:04:15.040781 [debug] [Thread-1 (]: SQL status: OK in 0.720 seconds
[0m21:04:15.042774 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=eb080835-794f-4a1a-a46c-8a6a42449f2d) - Closing
[0m21:04:15.043775 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:15.043775 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m21:04:15.352597 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m21:04:15.355611 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=8c091838-eb87-4059-b1c3-15db9dc5fbd7) - Closing
[0m21:04:15.356598 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m21:04:15.357610 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m21:04:15.357610 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m21:04:19.564326 [debug] [Thread-1 (]: SQL status: OK in 4.210 seconds
[0m21:04:19.566327 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=10e9d055-b9e7-4b44-afaa-adbe253b3338) - Closing
[0m21:04:19.567327 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F88CCBA10>]}
[0m21:04:19.568327 [info ] [Thread-1 (]: 3 of 5 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 10.84s]
[0m21:04:19.570374 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m21:04:19.571327 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_test2
[0m21:04:19.573358 [info ] [Thread-1 (]: 4 of 5 START sql incremental model demo_schema.stg_test2 ....................... [RUN]
[0m21:04:19.575614 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_jobs_description_new, now model.jra_dbt.stg_test2)
[0m21:04:19.576920 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, name=model.jra_dbt.stg_test2, idle-time=0.00959324836730957s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_jobs_description_new
[0m21:04:19.576920 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_test2
[0m21:04:19.592734 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_test2"
[0m21:04:19.597458 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_test2
[0m21:04:19.599523 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:04:19.600616 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:04:19.600616 [debug] [Thread-1 (]: Safe create: False
[0m21:04:19.601614 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m21:04:19.602616 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:19.602616 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

      create or replace temporary view `stg_test2__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`stg_test2`
    )
    
[0m21:04:20.647184 [debug] [Thread-1 (]: SQL status: OK in 1.040 seconds
[0m21:04:20.648221 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=f12f53f0-5bee-475f-8de2-c73bbeb4404c) - Closing
[0m21:04:20.648221 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m21:04:20.651148 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:20.651148 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2'
  
[0m21:04:21.035728 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m21:04:21.042728 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=5b566c0d-4c95-4d8c-b9e2-efc403002263) - Closing
[0m21:04:21.052192 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:21.053137 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m21:04:21.386216 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m21:04:21.393560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=70ef51c2-2478-40b8-ad9f-0a9ea647d41d) - Closing
[0m21:04:21.403082 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:21.405088 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2'
    AND is_nullable = 'NO';
  
[0m21:04:21.849315 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m21:04:21.854315 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=54c12a4f-5d87-458f-bd7e-43dcbcf6e33c) - Closing
[0m21:04:21.861315 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:21.862315 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:22.736706 [debug] [Thread-1 (]: SQL status: OK in 0.870 seconds
[0m21:04:22.743682 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=43fbdc78-0cb0-43cd-80ad-1d106804b73b) - Closing
[0m21:04:22.755055 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:22.756048 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:23.930047 [debug] [Thread-1 (]: SQL status: OK in 1.170 seconds
[0m21:04:23.937132 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=cbb5dde7-bf6d-4993-a069-82b92bae0369) - Closing
[0m21:04:23.945967 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:23.948073 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m21:04:24.317438 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m21:04:24.323995 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=41d09d9c-d8fa-477e-8628-ec3fc9bd2942) - Closing
[0m21:04:24.336999 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:24.339002 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_test2`
  
[0m21:04:24.828320 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m21:04:24.829207 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=d8388254-12ee-471c-8106-7d5c6924b75b) - Closing
[0m21:04:24.831149 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:24.831149 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
describe extended `demo`.`demo_schema`.`stg_test2`
  
[0m21:04:25.454607 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m21:04:25.457624 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=c275489a-6238-4223-8502-5da63cea0fe5) - Closing
[0m21:04:25.458629 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'base_salary': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m21:04:25.458629 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_test2`
[0m21:04:25.461637 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:25.461637 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_test2`

  
[0m21:04:25.874193 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m21:04:25.875180 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=c3b778bf-ad06-4e1a-84a1-fa3e391d5811) - Closing
[0m21:04:25.877179 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:25.877179 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `stg_test2__dbt_tmp`

  
[0m21:04:26.122949 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m21:04:26.124964 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=c4a7f10c-3624-4bf4-b183-9d3bd620ece1) - Closing
[0m21:04:26.125962 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m21:04:26.125962 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m21:04:26.126947 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_test2` as DBT_INTERNAL_DEST
    using
        `stg_test2__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m21:04:32.988841 [debug] [Thread-1 (]: SQL status: OK in 6.860 seconds
[0m21:04:32.990845 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=8887ad0a-1ebf-4792-b429-5ac5c938067b) - Closing
[0m21:04:33.076256 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F88CF7530>]}
[0m21:04:33.077254 [info ] [Thread-1 (]: 4 of 5 OK created sql incremental model demo_schema.stg_test2 .................. [[32mOK[0m in 13.50s]
[0m21:04:33.079704 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_test2
[0m21:04:33.079704 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m21:04:33.080763 [info ] [Thread-1 (]: 5 of 5 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m21:04:33.081998 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_test2, now model.jra_dbt.stg_unified_test)
[0m21:04:33.081998 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, name=model.jra_dbt.stg_unified_test, idle-time=0.005742073059082031s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_test2
[0m21:04:33.083073 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m21:04:33.087403 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m21:04:33.088417 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m21:04:33.090348 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:04:33.091455 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:04:33.091455 [debug] [Thread-1 (]: Safe create: False
[0m21:04:33.092485 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m21:04:33.093482 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:33.093482 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    min_salary,
    max_salary,
    job_posted_date,
    scraped_dts, 
    ingest_dts,
    _rescued_data,
    is_enriched,
    job_source,
    is_active
FROM `demo`.`demo_schema`.`stg_cleaned_test`


    WHERE ingest_dts > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_unified_test`
    )
    
[0m21:04:33.775785 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m21:04:33.778783 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=06fceae5-b5fc-4c68-a4ea-38d052ed2e73) - Closing
[0m21:04:33.779746 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m21:04:33.781724 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:33.781724 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m21:04:34.183976 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m21:04:34.188983 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=8d423062-ec03-428d-b791-45b0bb3fd399) - Closing
[0m21:04:34.191974 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:34.192975 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m21:04:34.626644 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m21:04:34.628670 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=33252f96-3a97-474b-922a-e146c4d095c7) - Closing
[0m21:04:34.633660 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:34.633660 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m21:04:35.279745 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m21:04:35.281860 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=3e0bbd8d-8c55-4f79-b844-385599822f9b) - Closing
[0m21:04:35.284848 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:35.284848 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:36.541878 [debug] [Thread-1 (]: SQL status: OK in 1.260 seconds
[0m21:04:36.542825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=a23a9ada-b887-4a59-a31b-795f6d4c0fbc) - Closing
[0m21:04:36.544832 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:36.544832 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m21:04:38.165081 [debug] [Thread-1 (]: SQL status: OK in 1.620 seconds
[0m21:04:38.168155 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=419812da-4bd8-47e5-96cc-1b94de4c29af) - Closing
[0m21:04:38.172733 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:38.173773 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m21:04:38.648515 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m21:04:38.651481 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=ef6c5588-2fba-49d4-97fa-427bd7b51c83) - Closing
[0m21:04:38.655470 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:38.655470 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m21:04:39.482665 [debug] [Thread-1 (]: SQL status: OK in 0.830 seconds
[0m21:04:39.485668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=7577af6d-1dd2-4613-a49a-819c1318041a) - Closing
[0m21:04:39.487698 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:39.487698 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m21:04:40.268253 [debug] [Thread-1 (]: SQL status: OK in 0.780 seconds
[0m21:04:40.271274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=60b71d93-f789-47db-aa2f-a43e3e089da2) - Closing
[0m21:04:40.272273 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m21:04:40.272273 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m21:04:40.275271 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:40.275271 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m21:04:40.590761 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m21:04:40.591830 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=c44b9c7a-cda2-4ecd-be05-1c7b66ae628f) - Closing
[0m21:04:40.594330 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:40.594330 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m21:04:40.948629 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m21:04:40.950630 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=6522c2b7-15bf-4e54-bdf9-ffa774a97343) - Closing
[0m21:04:40.951630 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m21:04:40.952630 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m21:04:40.952630 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m21:04:46.593860 [debug] [Thread-1 (]: SQL status: OK in 5.640 seconds
[0m21:04:46.597142 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535, command-id=4a634561-5bcb-42e0-878b-1de30a19676a) - Closing
[0m21:04:46.681621 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99e2be0c-7e74-44d4-a2c0-e617181bbe5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F88C989B0>]}
[0m21:04:46.681621 [info ] [Thread-1 (]: 5 of 5 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 13.60s]
[0m21:04:46.683343 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m21:04:46.687420 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=87.0589120388031s, language=None, compute-name=) - Recreating due to idleness
[0m21:04:46.688499 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Reset connection handle
[0m21:04:46.690499 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Reusing connection previously named master
[0m21:04:46.691507 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:04:46.692487 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m21:04:46.693454 [debug] [MainThread]: On list_demo: Close
[0m21:04:46.693454 [debug] [MainThread]: Databricks adapter: Connection(session-id=bcf52d2a-2296-42ae-871f-0690b81d07cc) - Closing
[0m21:04:46.784307 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m21:04:46.785307 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m21:04:46.785307 [debug] [MainThread]: Databricks adapter: Connection(session-id=bbfe1106-13ed-4351-a0ad-7a2b96b8ae7d) - Closing
[0m21:04:46.876021 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m21:04:46.876021 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m21:04:46.876021 [debug] [MainThread]: Databricks adapter: Connection(session-id=6206d371-2f5f-4d93-b65c-8bfe668b6535) - Closing
[0m21:04:48.445012 [info ] [MainThread]: 
[0m21:04:48.445012 [info ] [MainThread]: Finished running 5 incremental models in 0 hours 1 minutes and 36.54 seconds (96.54s).
[0m21:04:48.447172 [debug] [MainThread]: Command end result
[0m21:04:48.468560 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m21:04:48.470701 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m21:04:48.474991 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m21:04:48.474991 [info ] [MainThread]: 
[0m21:04:48.474991 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:04:48.476373 [info ] [MainThread]: 
[0m21:04:48.476373 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m21:04:48.476373 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:04:48.477470 [debug] [MainThread]: Command `dbt run` succeeded at 21:04:48.477470 after 100.97 seconds
[0m21:04:48.477470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FE3DD7FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8762DBE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F88D0A2A0>]}
[0m21:04:48.478476 [debug] [MainThread]: Flushing usage events
[0m21:04:49.709929 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:15:11.096282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027072F82480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027072F828A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027072F832C0>]}


============================== 21:15:11.100512 | 3be49078-231e-40e7-960d-cd7b3296d6ca ==============================
[0m21:15:11.100512 [info ] [MainThread]: Running with dbt=1.10.4
[0m21:15:11.100512 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m21:15:11.847688 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:15:11.847688 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:15:11.847688 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:15:12.486727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3be49078-231e-40e7-960d-cd7b3296d6ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027016108B30>]}
[0m21:15:12.547468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3be49078-231e-40e7-960d-cd7b3296d6ca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270729557C0>]}
[0m21:15:12.547468 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m21:15:12.779707 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m21:15:12.881558 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m21:15:12.882534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '3be49078-231e-40e7-960d-cd7b3296d6ca', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002701867E390>]}
[0m21:15:12.930061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m21:15:12.931069 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_cleaned.sql
[0m21:15:12.931069 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m21:15:13.219078 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.indeed_cleaned' (models\test\indeed_cleaned.sql) depends on a source named 'bronze_layer.indeed_cleaned' which was not found
[0m21:15:13.220069 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:15:13.220069 [debug] [MainThread]: Command `dbt run` failed at 21:15:13.220069 after 2.25 seconds
[0m21:15:13.221049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027072F828A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002701975A8A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002701995F0B0>]}
[0m21:15:13.221049 [debug] [MainThread]: Flushing usage events
[0m21:15:14.191115 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m21:16:06.387389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D951E0440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D951E2F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D951E33B0>]}


============================== 21:16:06.391422 | 40328310-9b24-4da7-b5e9-56474ce13b73 ==============================
[0m21:16:06.391422 [info ] [MainThread]: Running with dbt=1.10.4
[0m21:16:06.391422 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_cleaned', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m21:16:07.110964 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m21:16:07.111962 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m21:16:07.111962 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m21:16:07.768160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D94B51D30>]}
[0m21:16:07.826515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DB7C3CBF0>]}
[0m21:16:07.826515 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m21:16:08.046206 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m21:16:08.122776 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m21:16:08.122776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DB9D43200>]}
[0m21:16:08.171056 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m21:16:08.171056 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_cleaned.sql
[0m21:16:08.171056 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m21:16:08.534637 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m21:16:08.541712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DBA088A70>]}
[0m21:16:08.589779 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m21:16:08.590810 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m21:16:08.623217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DBA057FE0>]}
[0m21:16:08.623217 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m21:16:08.624496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DBA050DA0>]}
[0m21:16:08.625492 [info ] [MainThread]: 
[0m21:16:08.625492 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:16:08.626493 [info ] [MainThread]: 
[0m21:16:08.626493 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:16:08.627523 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m21:16:08.628533 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:16:08.629568 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m21:16:08.629568 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m21:16:08.629568 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m21:16:08.629568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:16:09.015805 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f33077e1-808d-47e1-a3ff-009881825f76) - Created
[0m21:16:09.209507 [debug] [ThreadPool]: SQL status: OK in 0.580 seconds
[0m21:16:09.211432 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f33077e1-808d-47e1-a3ff-009881825f76, command-id=3dd76152-7706-4d94-b590-f8202ef0cc4a) - Closing
[0m21:16:09.219523 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:16:09.220549 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m21:16:09.229522 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m21:16:09.229522 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m21:16:09.229522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:16:09.467054 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=688c48fa-d8c2-4680-8da0-dca5c192154a) - Created
[0m21:16:10.166431 [debug] [ThreadPool]: SQL status: OK in 0.940 seconds
[0m21:16:10.174671 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=688c48fa-d8c2-4680-8da0-dca5c192154a, command-id=0692a0b7-b21d-4171-afdd-96050befa73d) - Closing
[0m21:16:10.177671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DB7F75460>]}
[0m21:16:10.181187 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m21:16:10.181187 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m21:16:10.182411 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m21:16:10.182411 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m21:16:10.182411 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m21:16:10.189428 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m21:16:10.190425 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m21:16:10.215667 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m21:16:10.224148 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m21:16:10.224148 [debug] [Thread-1 (]: Safe create: False
[0m21:16:10.237770 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m21:16:10.247533 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m21:16:10.247533 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_raw`
    
[0m21:16:10.249090 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:16:10.466330 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=29ad1e02-4d21-4201-a510-707d45eeacf5) - Created
[0m21:16:10.940338 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_raw`
    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=ca4cb063-04c9-4195-88bb-26d9684122ed
[0m21:16:11.084988 [debug] [Thread-1 (]: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m21:16:11.088684 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '40328310-9b24-4da7-b5e9-56474ce13b73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D94B06030>]}
[0m21:16:11.088684 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.indeed_cleaned ......... [[31mERROR[0m in 0.90s]
[0m21:16:11.090744 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m21:16:11.090744 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.indeed_cleaned' to be skipped because of status 'error'.  Reason: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql.
[0m21:16:11.094100 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0.9151043891906738s, language=None, compute-name=) - Reusing connection previously named master
[0m21:16:11.094100 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:16:11.094100 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m21:16:11.094100 [debug] [MainThread]: On list_demo: Close
[0m21:16:11.094100 [debug] [MainThread]: Databricks adapter: Connection(session-id=f33077e1-808d-47e1-a3ff-009881825f76) - Closing
[0m21:16:11.204199 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m21:16:11.205204 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m21:16:11.206204 [debug] [MainThread]: Databricks adapter: Connection(session-id=688c48fa-d8c2-4680-8da0-dca5c192154a) - Closing
[0m21:16:11.274560 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m21:16:11.275600 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m21:16:11.276600 [debug] [MainThread]: Databricks adapter: Connection(session-id=29ad1e02-4d21-4201-a510-707d45eeacf5) - Closing
[0m21:16:11.343898 [info ] [MainThread]: 
[0m21:16:11.344901 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.72 seconds (2.72s).
[0m21:16:11.345920 [debug] [MainThread]: Command end result
[0m21:16:11.366419 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m21:16:11.368435 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m21:16:11.373346 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m21:16:11.373346 [info ] [MainThread]: 
[0m21:16:11.374393 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m21:16:11.375363 [info ] [MainThread]: 
[0m21:16:11.376842 [error] [MainThread]: [31mFailure in model indeed_cleaned (models\test\indeed_cleaned.sql)[0m
[0m21:16:11.376842 [error] [MainThread]:   Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m21:16:11.377882 [info ] [MainThread]: 
[0m21:16:11.377882 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\indeed_cleaned.sql
[0m21:16:11.377882 [info ] [MainThread]: 
[0m21:16:11.378930 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m21:16:11.379661 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m21:16:11.379881 [debug] [MainThread]: Command `dbt run` failed at 21:16:11.379881 after 5.12 seconds
[0m21:16:11.380427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D951E0440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D951E33B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024DB9EFE810>]}
[0m21:16:11.380427 [debug] [MainThread]: Flushing usage events
[0m21:16:12.571791 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:28:18.062864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A4ED7EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A536C7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A54BFA70>]}


============================== 22:28:18.067583 | 0707234a-4c30-4cbb-afdd-bf1c2c9ce06b ==============================
[0m22:28:18.067583 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:28:18.068594 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select indeed_raw', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m22:28:18.825217 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:28:18.825217 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:28:18.825217 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:28:19.833033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A2AC4C50>]}
[0m22:28:19.896710 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157C84D5EB0>]}
[0m22:28:19.896710 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:28:20.148459 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:28:20.221720 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:28:20.221720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157CA111760>]}
[0m22:28:20.302439 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:28:20.302439 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:28:20.307995 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:28:20.330981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157C8CAE480>]}
[0m22:28:20.388747 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:28:20.391111 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:28:20.420435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157C9075FA0>]}
[0m22:28:20.420435 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m22:28:20.420435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157CA23FE90>]}
[0m22:28:20.421895 [info ] [MainThread]: 
[0m22:28:20.422901 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:28:20.422901 [info ] [MainThread]: 
[0m22:28:20.423898 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:28:20.423898 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:28:20.424894 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:28:20.424894 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:28:20.424894 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:28:20.424894 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:28:20.425901 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:30:22.624928 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=863e2afb-9f4b-4d91-8059-7fddbf12f463) - Created
[0m22:30:26.104057 [debug] [ThreadPool]: SQL status: OK in 125.680 seconds
[0m22:30:26.108055 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=863e2afb-9f4b-4d91-8059-7fddbf12f463, command-id=f72e7af6-a84a-4e0f-91a7-48ae5664f00a) - Closing
[0m22:30:26.118069 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:30:26.119027 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:30:26.133005 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:30:26.133005 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:30:26.134004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:30:26.508064 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a012d879-96f1-4b59-84d3-a784a53f85ae) - Created
[0m22:30:40.908903 [debug] [ThreadPool]: SQL status: OK in 14.770 seconds
[0m22:30:40.924433 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a012d879-96f1-4b59-84d3-a784a53f85ae, command-id=173ea354-25da-41b8-9415-f83aa72466db) - Closing
[0m22:30:41.076432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A83640E0>]}
[0m22:30:41.085412 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_raw
[0m22:30:41.085412 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_raw ...................... [RUN]
[0m22:30:41.086499 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_raw, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:30:41.087523 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_raw'
[0m22:30:41.087523 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_raw
[0m22:30:41.098417 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_raw"
[0m22:30:41.099432 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_raw
[0m22:30:41.126475 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:30:41.133474 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:30:41.134424 [debug] [Thread-1 (]: Safe create: False
[0m22:30:41.147824 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m22:30:41.148990 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:30:41.148990 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

      create or replace temporary view `indeed_raw__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_raw`
    )
    
[0m22:30:41.148990 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:31:41.426466 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234) - Created
[0m22:32:50.971253 [debug] [Thread-1 (]: SQL status: OK in 129.820 seconds
[0m22:32:50.973264 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=4c7c623d-260c-4367-977c-0182fee7b298) - Closing
[0m22:32:51.054115 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:32:51.071578 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:51.071578 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_raw'
  
[0m22:32:51.621091 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m22:32:51.622036 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=fa5a35bb-7b0a-42b3-a755-5567f4969a2a) - Closing
[0m22:32:51.628094 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:51.628094 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_raw';
  
[0m22:32:52.488652 [debug] [Thread-1 (]: SQL status: OK in 0.860 seconds
[0m22:32:52.489653 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=727c146d-d050-4362-8fdd-875a82f38662) - Closing
[0m22:32:52.494684 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:52.494684 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_raw'
    AND is_nullable = 'NO';
  
[0m22:32:54.332810 [debug] [Thread-1 (]: SQL status: OK in 1.840 seconds
[0m22:32:54.334852 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=2ba9a588-f178-4ac2-815a-69ce4939e999) - Closing
[0m22:32:54.341178 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:54.342246 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_raw' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_raw' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:32:56.816541 [debug] [Thread-1 (]: SQL status: OK in 2.470 seconds
[0m22:32:56.818581 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=bce8e0da-150b-4571-bd2d-15014814b723) - Closing
[0m22:32:56.822614 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:56.823607 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_raw'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_raw'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:32:59.316711 [debug] [Thread-1 (]: SQL status: OK in 2.490 seconds
[0m22:32:59.319718 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=593086b7-a8c1-4b91-8162-bd2215e65270) - Closing
[0m22:32:59.327778 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:59.327778 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_raw';
  
[0m22:32:59.687698 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m22:32:59.691692 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=1148298c-4d21-4d27-9690-5caaf59240fa) - Closing
[0m22:32:59.697749 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:32:59.697749 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_raw`
  
[0m22:33:00.783409 [debug] [Thread-1 (]: SQL status: OK in 1.080 seconds
[0m22:33:00.789375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=10ab41d5-b2b7-43f6-b384-472b7b6830f6) - Closing
[0m22:33:00.792420 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:33:00.793413 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
describe extended `demo`.`demo_schema`.`indeed_raw`
  
[0m22:33:02.521364 [debug] [Thread-1 (]: SQL status: OK in 1.730 seconds
[0m22:33:02.528116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=8f3c6d07-8f3e-4d1b-b68d-e813462af3ae) - Closing
[0m22:33:02.533163 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'url': '', 'job_title': '', 'company_name': '', 'company_url': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'is_remote': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:33:02.545161 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_raw`
[0m22:33:02.569798 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:33:02.569798 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_raw`

  
[0m22:33:03.028627 [debug] [Thread-1 (]: SQL status: OK in 0.460 seconds
[0m22:33:03.031619 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=dc6201df-0246-4b33-a2aa-e9ae408a669f) - Closing
[0m22:33:03.033622 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:33:03.033622 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
DESCRIBE TABLE `indeed_raw__dbt_tmp`

  
[0m22:33:03.585840 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m22:33:03.587851 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234, command-id=0b190266-dc83-4d66-bc23-f8813ddefce6) - Closing
[0m22:33:03.601689 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m22:33:03.602652 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:33:03.603625 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_raw` as DBT_INTERNAL_DEST
    using
        `indeed_raw__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:33:04.717710 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_raw` as DBT_INTERNAL_DEST
    using
        `indeed_raw__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_MERGE_UNRESOLVED_EXPRESSION] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$throwIfNotResolved$3(ResolveDeltaMergeInto.scala:57)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.throwIfNotResolved(ResolveDeltaMergeInto.scala:50)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1(ResolveDeltaMergeInto.scala:71)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1$adapted(ResolveDeltaMergeInto.scala:71)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveOrFail(ResolveDeltaMergeInto.scala:71)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$6(ResolveDeltaMergeInto.scala:189)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveClause$1(ResolveDeltaMergeInto.scala:174)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$26(ResolveDeltaMergeInto.scala:369)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveReferencesAndSchema(ResolveDeltaMergeInto.scala:368)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:1029)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:758)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$throwIfNotResolved$3(ResolveDeltaMergeInto.scala:57)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.throwIfNotResolved(ResolveDeltaMergeInto.scala:50)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1(ResolveDeltaMergeInto.scala:71)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1$adapted(ResolveDeltaMergeInto.scala:71)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveOrFail(ResolveDeltaMergeInto.scala:71)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$6(ResolveDeltaMergeInto.scala:189)
		at scala.collection.immutable.List.flatMap(List.scala:366)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveClause$1(ResolveDeltaMergeInto.scala:174)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$26(ResolveDeltaMergeInto.scala:369)
		at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at scala.collection.TraversableLike.map(TraversableLike.scala:286)
		at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
		at scala.collection.AbstractTraversable.map(Traversable.scala:108)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveReferencesAndSchema(ResolveDeltaMergeInto.scala:368)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:1029)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
		at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
		at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
		at scala.collection.immutable.List.foldLeft(List.scala:91)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=d85eaf71-aa3c-4c5a-bc32-0a77b77ca668
[0m22:33:04.737632 [debug] [Thread-1 (]: Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql
[0m22:33:04.738634 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0707234a-4c30-4cbb-afdd-bf1c2c9ce06b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157CA7BF7A0>]}
[0m22:33:04.740044 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.indeed_raw ............. [[31mERROR[0m in 143.65s]
[0m22:33:04.740044 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_raw
[0m22:33:04.741329 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.indeed_raw' to be skipped because of status 'error'.  Reason: Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql.
[0m22:33:04.742332 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=143.66489434242249s, language=None, compute-name=) - Recreating due to idleness
[0m22:33:04.742332 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Reset connection handle
[0m22:33:04.743548 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Reusing connection previously named master
[0m22:33:04.743548 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:33:04.743548 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:33:04.743548 [debug] [MainThread]: On list_demo: Close
[0m22:33:04.744559 [debug] [MainThread]: Databricks adapter: Connection(session-id=863e2afb-9f4b-4d91-8059-7fddbf12f463) - Closing
[0m22:33:05.155224 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:33:05.157221 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:33:05.157221 [debug] [MainThread]: Databricks adapter: Connection(session-id=a012d879-96f1-4b59-84d3-a784a53f85ae) - Closing
[0m22:33:05.225611 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_raw' was properly closed.
[0m22:33:05.227609 [debug] [MainThread]: On model.jra_dbt.indeed_raw: Close
[0m22:33:05.228609 [debug] [MainThread]: Databricks adapter: Connection(session-id=af267db0-12af-4dd9-bfc5-3e0ae0935234) - Closing
[0m22:33:06.142815 [info ] [MainThread]: 
[0m22:33:06.144343 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 4 minutes and 45.72 seconds (285.72s).
[0m22:33:06.146915 [debug] [MainThread]: Command end result
[0m22:33:06.174820 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:33:06.177142 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:33:06.180732 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:33:06.181757 [info ] [MainThread]: 
[0m22:33:06.181757 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:33:06.182705 [info ] [MainThread]: 
[0m22:33:06.183680 [error] [MainThread]: [31mFailure in model indeed_raw (models\test\indeed_raw.sql)[0m
[0m22:33:06.183680 [error] [MainThread]:   Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql
[0m22:33:06.184665 [info ] [MainThread]: 
[0m22:33:06.185169 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\indeed_raw.sql
[0m22:33:06.185602 [info ] [MainThread]: 
[0m22:33:06.185602 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:33:06.186656 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:33:06.187632 [debug] [MainThread]: Command `dbt run` failed at 22:33:06.187632 after 288.26 seconds
[0m22:33:06.187632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157CA111F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157C8952EA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000157A57CF6B0>]}
[0m22:33:06.187632 [debug] [MainThread]: Flushing usage events
[0m22:33:07.212407 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:36:50.846031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186F0EF1430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186F0EF3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186F0EF29C0>]}


============================== 22:36:50.850070 | f732b85b-70c8-4498-bb8a-fd10eae5ab42 ==============================
[0m22:36:50.850070 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:36:50.851067 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m22:36:51.578755 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:36:51.578755 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:36:51.578755 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:36:52.224306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186940E7950>]}
[0m22:36:52.282422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186F402B170>]}
[0m22:36:52.282422 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:36:52.513663 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:36:52.569707 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:36:52.571741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018697642180>]}
[0m22:36:52.621952 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:36:52.621952 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:36:52.626979 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:36:52.646500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018697727E90>]}
[0m22:36:52.696628 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:36:52.698659 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:36:52.725969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018696588380>]}
[0m22:36:52.725969 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m22:36:52.726979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186964E8770>]}
[0m22:36:52.728073 [info ] [MainThread]: 
[0m22:36:52.728073 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:36:52.728073 [info ] [MainThread]: 
[0m22:36:52.729096 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:36:52.729096 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:36:52.730095 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:36:52.730095 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:36:52.730095 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:36:52.731108 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:36:52.731108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:36:53.167101 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ff164046-ad48-4b99-b391-99aba5992d6e) - Created
[0m22:36:53.885273 [debug] [ThreadPool]: SQL status: OK in 1.150 seconds
[0m22:36:53.889659 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ff164046-ad48-4b99-b391-99aba5992d6e, command-id=e08d40ae-bd13-41cb-bd0f-2e3e0fed6ee5) - Closing
[0m22:36:53.908226 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:36:53.908226 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:36:53.926424 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:36:53.926424 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:36:53.926424 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:36:54.169992 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=4ba46bda-f505-483a-a01f-c6ad03444b7e) - Created
[0m22:36:54.968729 [debug] [ThreadPool]: SQL status: OK in 1.040 seconds
[0m22:36:54.981659 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4ba46bda-f505-483a-a01f-c6ad03444b7e, command-id=3812fbc4-b271-46bc-a474-21f67ea27f1d) - Closing
[0m22:36:54.986748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018697780260>]}
[0m22:36:54.992067 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m22:36:54.993065 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m22:36:54.993065 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:36:54.993065 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m22:36:54.994073 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m22:36:55.002127 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m22:36:55.003076 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m22:36:55.028587 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:36:55.035586 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:36:55.036632 [debug] [Thread-1 (]: Safe create: False
[0m22:36:55.049136 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:36:55.050162 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:36:55.050162 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_raw`
    
[0m22:36:55.051148 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:36:55.300442 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=422dcb19-8c08-4472-ade2-cb4c97963058) - Created
[0m22:36:55.677750 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_raw`
    
: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=dba0e193-87a5-440a-b011-0045967572d9
[0m22:36:55.685745 [debug] [Thread-1 (]: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m22:36:55.690898 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f732b85b-70c8-4498-bb8a-fd10eae5ab42', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001869783D280>]}
[0m22:36:55.690898 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.indeed_cleaned ......... [[31mERROR[0m in 0.69s]
[0m22:36:55.692905 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m22:36:55.693905 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.indeed_cleaned' to be skipped because of status 'error'.  Reason: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql.
[0m22:36:55.695919 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0.7091703414916992s, language=None, compute-name=) - Reusing connection previously named master
[0m22:36:55.695919 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:36:55.696901 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:36:55.696901 [debug] [MainThread]: On list_demo: Close
[0m22:36:55.697997 [debug] [MainThread]: Databricks adapter: Connection(session-id=ff164046-ad48-4b99-b391-99aba5992d6e) - Closing
[0m22:36:55.761658 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:36:55.762172 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:36:55.762172 [debug] [MainThread]: Databricks adapter: Connection(session-id=4ba46bda-f505-483a-a01f-c6ad03444b7e) - Closing
[0m22:36:55.844747 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m22:36:55.845741 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m22:36:55.846739 [debug] [MainThread]: Databricks adapter: Connection(session-id=422dcb19-8c08-4472-ade2-cb4c97963058) - Closing
[0m22:36:55.967545 [info ] [MainThread]: 
[0m22:36:55.968472 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.24 seconds (3.24s).
[0m22:36:55.970970 [debug] [MainThread]: Command end result
[0m22:36:55.999963 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:36:56.000952 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:36:56.005943 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:36:56.005943 [info ] [MainThread]: 
[0m22:36:56.007146 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:36:56.008223 [info ] [MainThread]: 
[0m22:36:56.008223 [error] [MainThread]: [31mFailure in model indeed_cleaned (models\test\indeed_cleaned.sql)[0m
[0m22:36:56.008223 [error] [MainThread]:   Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `job_industries` cannot be resolved. Did you mean one of the following? [`job_title`, `ingest_dts`, `job_function`, `job_location`, `job_summary`]. SQLSTATE: 42703; line 14 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m22:36:56.009693 [info ] [MainThread]: 
[0m22:36:56.010661 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\indeed_cleaned.sql
[0m22:36:56.010661 [info ] [MainThread]: 
[0m22:36:56.010661 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:36:56.012153 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:36:56.012153 [debug] [MainThread]: Command `dbt run` failed at 22:36:56.012153 after 5.28 seconds
[0m22:36:56.013161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000186F10B95E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018697727E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018696318050>]}
[0m22:36:56.013161 [debug] [MainThread]: Flushing usage events
[0m22:36:57.009831 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:48:55.739476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C884E6BA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C88EDFCE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C88EDFEC0>]}


============================== 22:48:55.744514 | 9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb ==============================
[0m22:48:55.744514 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:48:55.744514 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select indeed_raw', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m22:48:56.463443 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:48:56.463443 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:48:56.463443 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:48:57.112343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAC222D20>]}
[0m22:48:57.176389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAB685550>]}
[0m22:48:57.177404 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:48:57.407285 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:48:57.465772 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:48:57.466769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAD71ED20>]}
[0m22:48:57.516821 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:48:57.517822 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:48:57.522830 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:48:57.542665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAD807BC0>]}
[0m22:48:57.596201 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:48:57.598196 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:48:57.616366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAC6E6BA0>]}
[0m22:48:57.616366 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m22:48:57.617365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024CAC3D3BC0>]}
[0m22:48:57.618380 [info ] [MainThread]: 
[0m22:48:57.618380 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:48:57.618380 [info ] [MainThread]: 
[0m22:48:57.619364 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:48:57.619364 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:48:57.620617 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:48:57.620617 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:48:57.620617 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:48:57.620617 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:48:57.621630 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:48:57.929062 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=b5fa4ecb-50a3-446f-b991-c15acbc83e19) - Created
[0m22:48:58.129707 [debug] [ThreadPool]: SQL status: OK in 0.510 seconds
[0m22:48:58.130706 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b5fa4ecb-50a3-446f-b991-c15acbc83e19, command-id=17a26873-c31d-4f03-aab0-7fe2c1fc5634) - Closing
[0m22:48:58.135713 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:48:58.136709 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:48:58.147709 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:48:58.147709 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:48:58.147709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:48:58.329064 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f2bad8ee-3b7c-4a3c-ba83-d8968d07b51d) - Created
[0m22:48:59.030256 [debug] [ThreadPool]: SQL status: OK in 0.880 seconds
[0m22:48:59.039794 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f2bad8ee-3b7c-4a3c-ba83-d8968d07b51d, command-id=e9393a19-0755-4d53-add6-046ab0df2565) - Closing
[0m22:48:59.043756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C87AE1250>]}
[0m22:48:59.049249 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_raw
[0m22:48:59.049249 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_raw ...................... [RUN]
[0m22:48:59.050444 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_raw, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:48:59.051472 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_raw'
[0m22:48:59.051472 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_raw
[0m22:48:59.061619 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_raw"
[0m22:48:59.062596 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_raw
[0m22:48:59.087206 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:48:59.094284 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:48:59.094284 [debug] [Thread-1 (]: Safe create: False
[0m22:48:59.106996 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m22:48:59.108006 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:48:59.108968 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

      create or replace temporary view `indeed_raw__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_raw`
    )
    
[0m22:48:59.108968 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:48:59.312748 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d) - Created
[0m22:49:00.166629 [debug] [Thread-1 (]: SQL status: OK in 1.060 seconds
[0m22:49:00.166629 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=7d57aa4c-8111-431f-8bbc-51c42e0e77f0) - Closing
[0m22:49:00.167628 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:49:00.177630 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:00.178867 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_raw'
  
[0m22:49:00.509590 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m22:49:00.511739 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=c0c30223-88e6-4bd0-bc7e-7cb8d2217f44) - Closing
[0m22:49:00.515802 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:00.516802 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_raw';
  
[0m22:49:00.859212 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m22:49:00.866273 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=e9e826fd-d761-47e3-9afc-b4277ffed040) - Closing
[0m22:49:00.888946 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:00.890994 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_raw'
    AND is_nullable = 'NO';
  
[0m22:49:01.319957 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m22:49:01.325947 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=1261fd28-8950-4808-92a2-1ee0006da902) - Closing
[0m22:49:01.352129 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:01.352129 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_raw' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_raw' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:49:02.257090 [debug] [Thread-1 (]: SQL status: OK in 0.900 seconds
[0m22:49:02.259100 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=c488f56f-3573-442c-a159-10c5d0b7b3f2) - Closing
[0m22:49:02.264090 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:02.264090 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_raw'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_raw'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:49:03.709640 [debug] [Thread-1 (]: SQL status: OK in 1.440 seconds
[0m22:49:03.711748 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=54438e56-2feb-4b67-8ed3-35611d342e3e) - Closing
[0m22:49:03.715755 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:03.716755 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_raw';
  
[0m22:49:04.145798 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m22:49:04.147791 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=01b04e7f-2136-4009-b10f-2abc464fb9f7) - Closing
[0m22:49:04.152308 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:04.152308 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_raw`
  
[0m22:49:04.590111 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m22:49:04.593108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=5a5d6000-4c66-4aef-adc8-d712be8672a5) - Closing
[0m22:49:04.596101 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:04.596101 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
describe extended `demo`.`demo_schema`.`indeed_raw`
  
[0m22:49:05.197406 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m22:49:05.205513 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=3337effc-4885-4c98-9c3d-be5476c3fa64) - Closing
[0m22:49:05.209798 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'url': '', 'job_title': '', 'company_name': '', 'company_url': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'is_remote': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:49:05.248485 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_raw`
[0m22:49:05.315116 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:05.315116 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_raw`

  
[0m22:49:05.601491 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:49:05.602505 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=983e4729-5ce1-4c53-80e8-d9a1e3f0f7f7) - Closing
[0m22:49:05.604579 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:05.604579 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */

    
DESCRIBE TABLE `indeed_raw__dbt_tmp`

  
[0m22:49:05.923060 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m22:49:05.925418 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d, command-id=bdfa0412-f3c4-4b1c-baa5-50f7910306f7) - Closing
[0m22:49:05.933937 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_raw"
[0m22:49:05.934940 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_raw"
[0m22:49:05.934940 [debug] [Thread-1 (]: On model.jra_dbt.indeed_raw: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_raw` as DBT_INTERNAL_DEST
    using
        `indeed_raw__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:49:06.830354 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_raw"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_raw` as DBT_INTERNAL_DEST
    using
        `indeed_raw__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_MERGE_UNRESOLVED_EXPRESSION] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$throwIfNotResolved$3(ResolveDeltaMergeInto.scala:57)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.throwIfNotResolved(ResolveDeltaMergeInto.scala:50)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1(ResolveDeltaMergeInto.scala:71)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1$adapted(ResolveDeltaMergeInto.scala:71)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveOrFail(ResolveDeltaMergeInto.scala:71)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$6(ResolveDeltaMergeInto.scala:189)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveClause$1(ResolveDeltaMergeInto.scala:174)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$26(ResolveDeltaMergeInto.scala:369)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveReferencesAndSchema(ResolveDeltaMergeInto.scala:368)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:1029)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:321)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:267)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:758)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$throwIfNotResolved$3(ResolveDeltaMergeInto.scala:57)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.throwIfNotResolved(ResolveDeltaMergeInto.scala:50)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1(ResolveDeltaMergeInto.scala:71)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveOrFail$1$adapted(ResolveDeltaMergeInto.scala:71)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveOrFail(ResolveDeltaMergeInto.scala:71)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$6(ResolveDeltaMergeInto.scala:189)
		at scala.collection.immutable.List.flatMap(List.scala:366)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveClause$1(ResolveDeltaMergeInto.scala:174)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.$anonfun$resolveReferencesAndSchema$26(ResolveDeltaMergeInto.scala:369)
		at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at scala.collection.TraversableLike.map(TraversableLike.scala:286)
		at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
		at scala.collection.AbstractTraversable.map(Traversable.scala:108)
		at com.databricks.sql.transaction.tahoe.ResolveDeltaMergeInto$.resolveReferencesAndSchema(ResolveDeltaMergeInto.scala:368)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:1029)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
		at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
		at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
		at scala.collection.immutable.List.foldLeft(List.scala:91)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:498)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:491)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:397)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:491)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:416)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:478)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:478)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:294)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:668)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:151)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:668)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:661)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:658)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:658)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:288)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:287)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=470bd109-ef56-42a0-8842-9f3f9149972c
[0m22:49:06.834872 [debug] [Thread-1 (]: Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql
[0m22:49:06.836873 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a4a7d8d-7ea9-476f-b9b7-b9427ad8bceb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C884B6090>]}
[0m22:49:06.837874 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.indeed_raw ............. [[31mERROR[0m in 7.79s]
[0m22:49:06.837874 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_raw
[0m22:49:06.838873 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.indeed_raw' to be skipped because of status 'error'.  Reason: Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql.
[0m22:49:06.839869 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.7961132526397705s, language=None, compute-name=) - Reusing connection previously named master
[0m22:49:06.839869 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:49:06.840886 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:49:06.840886 [debug] [MainThread]: On list_demo: Close
[0m22:49:06.840886 [debug] [MainThread]: Databricks adapter: Connection(session-id=b5fa4ecb-50a3-446f-b991-c15acbc83e19) - Closing
[0m22:49:06.929470 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:49:06.929990 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:49:06.929990 [debug] [MainThread]: Databricks adapter: Connection(session-id=f2bad8ee-3b7c-4a3c-ba83-d8968d07b51d) - Closing
[0m22:49:06.993100 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_raw' was properly closed.
[0m22:49:06.993100 [debug] [MainThread]: On model.jra_dbt.indeed_raw: Close
[0m22:49:06.994080 [debug] [MainThread]: Databricks adapter: Connection(session-id=20a9c616-9f65-46e1-8366-3db576d5ae4d) - Closing
[0m22:49:07.546844 [info ] [MainThread]: 
[0m22:49:07.547837 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 9.93 seconds (9.93s).
[0m22:49:07.549023 [debug] [MainThread]: Command end result
[0m22:49:07.586220 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:49:07.588219 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:49:07.592239 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:49:07.592239 [info ] [MainThread]: 
[0m22:49:07.593235 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m22:49:07.593235 [info ] [MainThread]: 
[0m22:49:07.594237 [error] [MainThread]: [31mFailure in model indeed_raw (models\test\indeed_raw.sql)[0m
[0m22:49:07.596389 [error] [MainThread]:   Database Error in model indeed_raw (models\test\indeed_raw.sql)
  [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve company_url in UPDATE clause given columns DBT_INTERNAL_SOURCE.url, DBT_INTERNAL_SOURCE.job_posting_id, DBT_INTERNAL_SOURCE.job_title, DBT_INTERNAL_SOURCE.company_name, DBT_INTERNAL_SOURCE.job_location, DBT_INTERNAL_SOURCE.job_summary, DBT_INTERNAL_SOURCE.job_seniority_level, DBT_INTERNAL_SOURCE.job_function, DBT_INTERNAL_SOURCE.job_employment_type, DBT_INTERNAL_SOURCE.job_industries, DBT_INTERNAL_SOURCE.job_posted_date, DBT_INTERNAL_SOURCE.MIN_AMOUNT, DBT_INTERNAL_SOURCE.MAX_AMOUNT, DBT_INTERNAL_SOURCE.timestamp, DBT_INTERNAL_SOURCE._rescued_data, DBT_INTERNAL_SOURCE.ingest_dts, DBT_INTERNAL_SOURCE.source_file, DBT_INTERNAL_SOURCE.year, DBT_INTERNAL_SOURCE.month, DBT_INTERNAL_SOURCE.day.; line 15 pos 4
  compiled code at target\run\jra_dbt\models\test\indeed_raw.sql
[0m22:49:07.597386 [info ] [MainThread]: 
[0m22:49:07.598272 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\indeed_raw.sql
[0m22:49:07.599238 [info ] [MainThread]: 
[0m22:49:07.599238 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m22:49:07.600235 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:49:07.601335 [debug] [MainThread]: Command `dbt run` failed at 22:49:07.601335 after 11.97 seconds
[0m22:49:07.603457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C88FD0800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C88FD2A80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C88FD32C0>]}
[0m22:49:07.604457 [debug] [MainThread]: Flushing usage events
[0m22:49:08.834682 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:49.480047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E21A685D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E21AB1E8A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E21A527080>]}


============================== 22:49:49.484574 | c8d8edfb-c1a3-47a5-b162-b3ce70f250a1 ==============================
[0m22:49:49.484574 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:49:49.484574 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_test', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m22:49:50.204452 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:49:50.204452 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:49:50.204452 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:49:50.849867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23E06E180>]}
[0m22:49:50.911532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E21AF6FEF0>]}
[0m22:49:50.911532 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:49:51.149973 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:49:51.211187 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:49:51.211694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23E189820>]}
[0m22:49:51.260492 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m22:49:51.260492 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_test.sql
[0m22:49:51.261004 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_raw.sql
[0m22:49:51.493815 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:49:51.502819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F8CB890>]}
[0m22:49:51.553425 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:49:51.555424 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:49:51.573860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F877EF0>]}
[0m22:49:51.574859 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m22:49:51.574859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F8CD340>]}
[0m22:49:51.575852 [info ] [MainThread]: 
[0m22:49:51.575852 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:49:51.576853 [info ] [MainThread]: 
[0m22:49:51.576853 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:51.576853 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:49:51.577852 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:51.577852 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:49:51.578850 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:49:51.578850 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:49:51.578850 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:51.869666 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=8681defa-b1c3-4fa0-a037-41e624a2754a) - Created
[0m22:49:52.034231 [debug] [ThreadPool]: SQL status: OK in 0.460 seconds
[0m22:49:52.035228 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8681defa-b1c3-4fa0-a037-41e624a2754a, command-id=3bc22247-6761-4d67-a0a0-86cd4166e113) - Closing
[0m22:49:52.040597 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:52.040597 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:49:52.049921 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:49:52.050898 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:49:52.050898 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:52.223607 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ec9ee4a8-7c5e-4789-9b90-cc3eb1664ce0) - Created
[0m22:49:52.837542 [debug] [ThreadPool]: SQL status: OK in 0.790 seconds
[0m22:49:52.841571 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ec9ee4a8-7c5e-4789-9b90-cc3eb1664ce0, command-id=0bc8d91c-2307-46b4-b097-2c9ff3aa9b77) - Closing
[0m22:49:52.843563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F8CD2E0>]}
[0m22:49:52.846597 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m22:49:52.847601 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m22:49:52.847601 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:52.847601 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_test'
[0m22:49:52.848877 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m22:49:52.854260 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m22:49:52.855253 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m22:49:52.881072 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:49:52.889281 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:49:52.890280 [debug] [Thread-1 (]: Safe create: False
[0m22:49:52.904383 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m22:49:52.904383 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:49:52.905388 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)
    
[0m22:49:52.905388 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:49:53.123428 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836) - Created
[0m22:49:53.672972 [debug] [Thread-1 (]: SQL status: OK in 0.770 seconds
[0m22:49:53.672972 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836, command-id=c624e896-da53-4e68-b338-f8aa6d918726) - Closing
[0m22:49:53.674123 [debug] [Thread-1 (]: No existing relation found
[0m22:49:53.681422 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:49:53.681422 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m22:49:53.982047 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m22:49:53.984076 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836, command-id=733e266f-c0f4-442e-b114-93dccca57d97) - Closing
[0m22:49:54.014932 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m22:49:54.015944 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:49:54.015944 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_test`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      MIN_AMOUNT string,
    
      MAX_AMOUNT string,
    
      timestamp timestamp_ntz,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/indeed_test'
  
  

  
[0m22:49:57.547870 [debug] [Thread-1 (]: SQL status: OK in 3.530 seconds
[0m22:49:57.547870 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836, command-id=0c1eb8ff-27ad-480a-a466-527ee638ca23) - Closing
[0m22:49:57.551802 [debug] [Thread-1 (]: Applying tags to relation None
[0m22:49:57.551802 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:49:57.551802 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    insert into `demo`.`demo_schema`.`indeed_test` select * from `indeed_test__dbt_tmp`
  
[0m22:50:06.135831 [debug] [Thread-1 (]: SQL status: OK in 8.580 seconds
[0m22:50:06.136845 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836, command-id=ec0f1672-f1cc-48cb-a241-6cc33a542ce6) - Closing
[0m22:50:06.222179 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8d8edfb-c1a3-47a5-b162-b3ce70f250a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23E158440>]}
[0m22:50:06.224236 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 13.37s]
[0m22:50:06.227173 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m22:50:06.229195 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=13.385631799697876s, language=None, compute-name=) - Reusing connection previously named master
[0m22:50:06.230357 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:50:06.231440 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:50:06.231440 [debug] [MainThread]: On list_demo: Close
[0m22:50:06.232455 [debug] [MainThread]: Databricks adapter: Connection(session-id=8681defa-b1c3-4fa0-a037-41e624a2754a) - Closing
[0m22:50:06.299203 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:50:06.299203 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:50:06.300195 [debug] [MainThread]: Databricks adapter: Connection(session-id=ec9ee4a8-7c5e-4789-9b90-cc3eb1664ce0) - Closing
[0m22:50:06.376695 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_test' was properly closed.
[0m22:50:06.376695 [debug] [MainThread]: On model.jra_dbt.indeed_test: Close
[0m22:50:06.376695 [debug] [MainThread]: Databricks adapter: Connection(session-id=bf88d578-0ed1-42c4-b9fb-a7d5e0be6836) - Closing
[0m22:50:06.832168 [info ] [MainThread]: 
[0m22:50:06.832168 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 15.26 seconds (15.26s).
[0m22:50:06.833177 [debug] [MainThread]: Command end result
[0m22:50:06.853648 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:50:06.855316 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:50:06.859661 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:50:06.859661 [info ] [MainThread]: 
[0m22:50:06.860228 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:50:06.860785 [info ] [MainThread]: 
[0m22:50:06.861301 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:50:06.861824 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:50:06.862355 [debug] [MainThread]: Command `dbt run` succeeded at 22:50:06.862355 after 17.48 seconds
[0m22:50:06.862919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E21A53AC00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F64EA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E23F64E330>]}
[0m22:50:06.862919 [debug] [MainThread]: Flushing usage events
[0m22:50:07.815705 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:50:30.210940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52C7C0740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52C7C3470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52C7C07D0>]}


============================== 22:50:30.213935 | 8b8a249b-655c-4808-9baa-81c19e649ede ==============================
[0m22:50:30.213935 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:50:30.214935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_cleaned', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m22:50:30.948036 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:50:30.948036 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:50:30.949485 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:50:31.584081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b8a249b-655c-4808-9baa-81c19e649ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52C195760>]}
[0m22:50:31.641779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b8a249b-655c-4808-9baa-81c19e649ede', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A54FBDA3F0>]}
[0m22:50:31.641779 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:50:31.869972 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:50:31.958319 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:50:31.958720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8b8a249b-655c-4808-9baa-81c19e649ede', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A551312C00>]}
[0m22:50:32.008082 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:50:32.008082 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m22:50:32.198895 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.indeed_cleaned' (models\test\indeed_cleaned.sql) depends on a source named 'bronze_layer.indeed_test' which was not found
[0m22:50:32.198895 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:50:32.199879 [debug] [MainThread]: Command `dbt run` failed at 22:50:32.199879 after 2.10 seconds
[0m22:50:32.200860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52DD81760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A54FFED970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A52A6738C0>]}
[0m22:50:32.200860 [debug] [MainThread]: Flushing usage events
[0m22:50:33.208849 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:51:53.922691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441A0F38C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024419EA2720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024419B35430>]}


============================== 22:51:53.926899 | de8ab70f-27f3-4e70-9073-157caed2edba ==============================
[0m22:51:53.926899 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:51:53.927911 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m22:51:54.634632 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:51:54.635627 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:51:54.635627 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:51:55.276911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443D7FF440>]}
[0m22:51:55.334115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441B7228D0>]}
[0m22:51:55.334115 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:51:55.555251 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:51:55.622031 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:51:55.623033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443B878290>]}
[0m22:51:55.669792 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:51:55.670791 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m22:51:55.670791 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m22:51:56.039959 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:51:56.047010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443F03F2C0>]}
[0m22:51:56.095420 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:51:56.096620 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:51:56.126522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443EFF7470>]}
[0m22:51:56.126522 [info ] [MainThread]: Found 6 models, 4 sources, 682 macros
[0m22:51:56.127522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443F003C50>]}
[0m22:51:56.128536 [info ] [MainThread]: 
[0m22:51:56.128536 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:51:56.129524 [info ] [MainThread]: 
[0m22:51:56.129524 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:51:56.130522 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:51:56.130522 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:51:56.131524 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:51:56.131524 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:51:56.131524 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:51:56.131524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:56.418195 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=52fb5734-25c8-4866-b5cf-6f8942073069) - Created
[0m22:51:56.645361 [debug] [ThreadPool]: SQL status: OK in 0.510 seconds
[0m22:51:56.647429 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=52fb5734-25c8-4866-b5cf-6f8942073069, command-id=e8013280-6299-46f6-ae95-3ddea62d1012) - Closing
[0m22:51:56.659929 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:51:56.660767 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:51:56.670772 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:51:56.670772 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:51:56.670772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:51:56.923221 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=fbaab3fb-469b-4304-b2ba-19202eab4279) - Created
[0m22:51:57.471348 [debug] [ThreadPool]: SQL status: OK in 0.800 seconds
[0m22:51:57.476340 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=fbaab3fb-469b-4304-b2ba-19202eab4279, command-id=636f2036-d9a7-4dba-9de5-4faad094bd84) - Closing
[0m22:51:57.478338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441D2F2420>]}
[0m22:51:57.481031 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m22:51:57.481031 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m22:51:57.482030 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:51:57.482030 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m22:51:57.483031 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m22:51:57.489047 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m22:51:57.490031 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m22:51:57.515847 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:51:57.523916 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:51:57.524912 [debug] [Thread-1 (]: Safe create: False
[0m22:51:57.537838 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:51:57.537838 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:51:57.537838 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`
    
[0m22:51:57.538838 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:51:57.822046 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4) - Created
[0m22:51:58.383703 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m22:51:58.385219 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4, command-id=bb54dac2-46ad-4ab1-961e-3dc7ad882344) - Closing
[0m22:51:58.385219 [debug] [Thread-1 (]: No existing relation found
[0m22:51:58.393215 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:51:58.395034 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m22:51:58.772708 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m22:51:58.774725 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4, command-id=3da0b48b-ab6b-4d2a-9d80-6e0a9977b3ef) - Closing
[0m22:51:58.805131 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:51:58.805131 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:51:58.806080 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary string,
    
      max_salary string,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
[0m22:52:00.191674 [debug] [Thread-1 (]: SQL status: OK in 1.390 seconds
[0m22:52:00.195681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4, command-id=c42e7c4a-4120-4404-8f13-dd69ce49677a) - Closing
[0m22:52:00.201343 [debug] [Thread-1 (]: Applying tags to relation None
[0m22:52:00.202333 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:52:00.202333 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    insert into `demo`.`demo_schema`.`indeed_cleaned` select * from `indeed_cleaned__dbt_tmp`
  
[0m22:52:04.117476 [debug] [Thread-1 (]: SQL status: OK in 3.920 seconds
[0m22:52:04.118407 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4, command-id=202a706b-95be-45ef-bdc6-60b4b56ea1b6) - Closing
[0m22:52:04.131809 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de8ab70f-27f3-4e70-9073-157caed2edba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441764FFB0>]}
[0m22:52:04.132836 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.65s]
[0m22:52:04.132836 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m22:52:04.134803 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.65646505355835s, language=None, compute-name=) - Reusing connection previously named master
[0m22:52:04.134803 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:52:04.134803 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:52:04.134803 [debug] [MainThread]: On list_demo: Close
[0m22:52:04.135948 [debug] [MainThread]: Databricks adapter: Connection(session-id=52fb5734-25c8-4866-b5cf-6f8942073069) - Closing
[0m22:52:04.208642 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:52:04.208642 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:52:04.208642 [debug] [MainThread]: Databricks adapter: Connection(session-id=fbaab3fb-469b-4304-b2ba-19202eab4279) - Closing
[0m22:52:04.275582 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m22:52:04.276673 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m22:52:04.277673 [debug] [MainThread]: Databricks adapter: Connection(session-id=b8893300-9044-4d1d-9235-b33658e7c4f4) - Closing
[0m22:52:04.541695 [info ] [MainThread]: 
[0m22:52:04.543112 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.41 seconds (8.41s).
[0m22:52:04.545160 [debug] [MainThread]: Command end result
[0m22:52:04.580601 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:52:04.582620 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:52:04.586615 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:52:04.587788 [info ] [MainThread]: 
[0m22:52:04.587788 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:52:04.587788 [info ] [MainThread]: 
[0m22:52:04.588787 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:52:04.590131 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:52:04.591339 [debug] [MainThread]: Command `dbt run` succeeded at 22:52:04.591339 after 10.77 seconds
[0m22:52:04.591339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441A3E7440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002443ED07EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002441A4AE930>]}
[0m22:52:04.591877 [debug] [MainThread]: Flushing usage events
[0m22:52:05.596549 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:55:38.430475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2DAD3D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2DBAC890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2DBAC8C0>]}


============================== 22:55:38.433488 | c99bc9c9-2409-491f-b2ba-8023fa8540d3 ==============================
[0m22:55:38.433488 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:55:38.434643 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_jobs_description_new', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m22:55:39.155809 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:55:39.155809 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:55:39.155809 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:55:39.799495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c99bc9c9-2409-491f-b2ba-8023fa8540d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC4EC741D0>]}
[0m22:55:39.856468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c99bc9c9-2409-491f-b2ba-8023fa8540d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC50A51640>]}
[0m22:55:39.857411 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:55:40.086894 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:55:40.172995 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:55:40.173994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'c99bc9c9-2409-491f-b2ba-8023fa8540d3', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC511C7560>]}
[0m22:55:40.223668 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:55:40.223668 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m22:55:40.224664 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m22:55:40.506795 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.stg_jobs_description_new' (models\test\stg_jobs_description_new.sql) depends on a source named 'silver_layer.indeed_cleaed' which was not found
[0m22:55:40.508085 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:55:40.509210 [debug] [MainThread]: Command `dbt run` failed at 22:55:40.508085 after 2.18 seconds
[0m22:55:40.509210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC2CAC4A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC52445820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EC524548F0>]}
[0m22:55:40.509210 [debug] [MainThread]: Flushing usage events
[0m22:55:41.514747 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:57:05.671765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F8A7353D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F8A735580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F8A735520>]}


============================== 22:57:05.675901 | c7cb0d55-8a73-4f2e-bec0-3c891b97e91f ==============================
[0m22:57:05.675901 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:57:05.676897 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_jobs_description_new', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m22:57:06.406258 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:57:06.407290 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:57:06.407290 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:57:07.071091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c7cb0d55-8a73-4f2e-bec0-3c891b97e91f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FAFD3F080>]}
[0m22:57:07.127938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c7cb0d55-8a73-4f2e-bec0-3c891b97e91f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FAF5D1D30>]}
[0m22:57:07.128940 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:57:07.350687 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:57:07.409310 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:57:07.409310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'c7cb0d55-8a73-4f2e-bec0-3c891b97e91f', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FB0059A60>]}
[0m22:57:07.455918 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:57:07.455918 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m22:57:07.455918 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m22:57:07.742364 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.stg_jobs_description_new' (models\test\stg_jobs_description_new.sql) depends on a source named 'silver_layer.indeed_cleaed' which was not found
[0m22:57:07.743313 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:57:07.743313 [debug] [MainThread]: Command `dbt run` failed at 22:57:07.743313 after 2.18 seconds
[0m22:57:07.743313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F8DE01220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FB14C0D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FB168CE00>]}
[0m22:57:07.743313 [debug] [MainThread]: Flushing usage events
[0m22:57:08.759120 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:57:28.384071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3D039C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3C3FF320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3C3FF3B0>]}


============================== 22:57:28.388071 | ed5cada6-a975-42b8-9a5b-a8f7b366a3fc ==============================
[0m22:57:28.388071 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:57:28.389072 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select stg_jobs_description_new', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m22:57:29.115525 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:57:29.116519 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:57:29.116519 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:57:29.753661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B5FECC8F0>]}
[0m22:57:29.812409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B5F911640>]}
[0m22:57:29.812409 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:57:30.042372 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:57:30.111820 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:57:30.112656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B60711D00>]}
[0m22:57:30.161189 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:57:30.161189 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m22:57:30.161189 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m22:57:30.524907 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:57:30.534905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B619B1400>]}
[0m22:57:30.583489 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:57:30.584482 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:57:30.617669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3CF3DC70>]}
[0m22:57:30.617669 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m22:57:30.619147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B619E1160>]}
[0m22:57:30.621620 [info ] [MainThread]: 
[0m22:57:30.622634 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:57:30.623627 [info ] [MainThread]: 
[0m22:57:30.623627 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:57:30.623627 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:57:30.624626 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:57:30.625643 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:57:30.625643 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:57:30.625643 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:57:30.625643 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:57:31.043007 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=270efd59-1d7d-43dc-927f-9d8948e60d53) - Created
[0m22:57:31.306791 [debug] [ThreadPool]: SQL status: OK in 0.680 seconds
[0m22:57:31.309797 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=270efd59-1d7d-43dc-927f-9d8948e60d53, command-id=6db4bbab-973b-4bb8-98a0-4ea3a3c8fb79) - Closing
[0m22:57:31.324186 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:57:31.324186 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:57:31.336185 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:57:31.336185 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:57:31.336185 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:57:31.540068 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d9bbf932-676c-4ca1-a607-ebedaee1d972) - Created
[0m22:57:32.099122 [debug] [ThreadPool]: SQL status: OK in 0.760 seconds
[0m22:57:32.105236 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d9bbf932-676c-4ca1-a607-ebedaee1d972, command-id=6b538c44-62d0-4124-a3f2-9a4088d85014) - Closing
[0m22:57:32.108432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B619BF800>]}
[0m22:57:32.113417 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m22:57:32.114425 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m22:57:32.114425 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_jobs_description_new, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:57:32.114425 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_jobs_description_new'
[0m22:57:32.115425 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m22:57:32.123457 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m22:57:32.124481 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m22:57:32.149508 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:57:32.155794 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:57:32.155794 [debug] [Thread-1 (]: Safe create: False
[0m22:57:32.169151 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m22:57:32.170134 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:32.170134 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`

Union all
SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`indeed_cleaned`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m22:57:32.170134 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:57:32.395467 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e) - Created
[0m22:57:33.109087 [debug] [Thread-1 (]: SQL status: OK in 0.940 seconds
[0m22:57:33.111124 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=b0a95de2-8323-4084-bdfd-d25e2c2e17b3) - Closing
[0m22:57:33.111124 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:57:33.121179 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:33.122127 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m22:57:33.409414 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m22:57:33.414411 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=c3906e6a-6276-4b55-8e6a-b9bade7dad32) - Closing
[0m22:57:33.419700 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:33.420740 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m22:57:33.702429 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:57:33.706404 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=2a2d7b4a-4b17-478e-bebf-4a1b82f056ab) - Closing
[0m22:57:33.714432 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:33.715454 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m22:57:34.186653 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m22:57:34.188765 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=f9830e99-5c77-4e93-8cad-509c5af11be0) - Closing
[0m22:57:34.194101 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:34.194101 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:57:34.803190 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m22:57:34.806377 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=31cc9cf0-1f1c-401f-8752-33cd20be70c0) - Closing
[0m22:57:34.816664 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:34.816664 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:57:35.682033 [debug] [Thread-1 (]: SQL status: OK in 0.870 seconds
[0m22:57:35.686053 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=8474535b-0f3d-4421-8c30-14f380f91c3a) - Closing
[0m22:57:35.696043 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:35.696043 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m22:57:36.047797 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m22:57:36.051797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=38444e17-e426-40a4-8be3-672d2c195771) - Closing
[0m22:57:36.059380 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:36.059380 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m22:57:36.614859 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m22:57:36.617849 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=0d1db596-0edb-4a50-a42f-71f67be7f893) - Closing
[0m22:57:36.622857 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:36.622857 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m22:57:37.159915 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m22:57:37.161958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=b6234b59-42f5-4ba8-83d3-6fb69e4c6c2b) - Closing
[0m22:57:37.161958 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m22:57:37.168236 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m22:57:37.190413 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:37.190413 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m22:57:37.447771 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:57:37.450760 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=66a74514-5f2f-454e-85eb-d982561dc550) - Closing
[0m22:57:37.452767 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:37.453767 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m22:57:37.737626 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:57:37.742907 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=826ee546-3c34-47b7-9d29-f1abba7d188f) - Closing
[0m22:57:37.757293 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m22:57:37.758312 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:57:37.758312 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:57:48.905690 [debug] [Thread-1 (]: SQL status: OK in 11.150 seconds
[0m22:57:48.907689 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e, command-id=f6933a76-238a-4fdc-b8ff-ee9fe83a9b96) - Closing
[0m22:57:48.988588 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ed5cada6-a975-42b8-9a5b-a8f7b366a3fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B607ABA40>]}
[0m22:57:48.989091 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 16.87s]
[0m22:57:48.991284 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m22:57:48.995160 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=16.885732650756836s, language=None, compute-name=) - Reusing connection previously named master
[0m22:57:48.996171 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:57:48.997151 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:57:48.998152 [debug] [MainThread]: On list_demo: Close
[0m22:57:48.998152 [debug] [MainThread]: Databricks adapter: Connection(session-id=270efd59-1d7d-43dc-927f-9d8948e60d53) - Closing
[0m22:57:49.075792 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:57:49.076843 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:57:49.076843 [debug] [MainThread]: Databricks adapter: Connection(session-id=d9bbf932-676c-4ca1-a607-ebedaee1d972) - Closing
[0m22:57:49.149666 [debug] [MainThread]: Connection 'model.jra_dbt.stg_jobs_description_new' was properly closed.
[0m22:57:49.151681 [debug] [MainThread]: On model.jra_dbt.stg_jobs_description_new: Close
[0m22:57:49.152674 [debug] [MainThread]: Databricks adapter: Connection(session-id=e0fd2d8c-7413-4d8a-8620-b53fe6ae346e) - Closing
[0m22:57:49.458429 [info ] [MainThread]: 
[0m22:57:49.459415 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 18.83 seconds (18.83s).
[0m22:57:49.460501 [debug] [MainThread]: Command end result
[0m22:57:49.486565 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:57:49.488490 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:57:49.493296 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:57:49.493296 [info ] [MainThread]: 
[0m22:57:49.493296 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:57:49.494315 [info ] [MainThread]: 
[0m22:57:49.494315 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:57:49.495378 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:57:49.495662 [debug] [MainThread]: Command `dbt run` succeeded at 22:57:49.495662 after 21.23 seconds
[0m22:57:49.495662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3CD87C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3CCCFFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B3CCCE630>]}
[0m22:57:49.496831 [debug] [MainThread]: Flushing usage events
[0m22:57:50.461200 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:58:38.740087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB4C4D760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB4C4C7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB4C4DA30>]}


============================== 22:58:38.744742 | e8b78898-b18b-4cb7-afe8-a4584620f0fb ==============================
[0m22:58:38.744742 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:58:38.744742 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m22:58:39.453978 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:58:39.453978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:58:39.453978 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:58:40.107153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB1CB8D70>]}
[0m22:58:40.164719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB5D65160>]}
[0m22:58:40.164719 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:58:40.392452 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:58:40.457739 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:58:40.457739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED92EE8D0>]}
[0m22:58:40.505154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:58:40.506153 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:58:40.510154 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:58:40.529640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED93DBF80>]}
[0m22:58:40.579400 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:58:40.581676 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:58:40.606581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED82B6DE0>]}
[0m22:58:40.606581 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m22:58:40.607439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED941F620>]}
[0m22:58:40.610450 [info ] [MainThread]: 
[0m22:58:40.610450 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:58:40.611481 [info ] [MainThread]: 
[0m22:58:40.611481 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:58:40.611481 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:58:40.618241 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:58:40.618241 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:58:40.618241 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:58:40.618241 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:58:40.619227 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:58:40.849480 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=c75afc7b-3c09-49b0-9013-a2f2e0ef3003) - Created
[0m22:58:41.429917 [debug] [ThreadPool]: SQL status: OK in 0.810 seconds
[0m22:58:41.432983 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c75afc7b-3c09-49b0-9013-a2f2e0ef3003, command-id=3f8641cc-81db-4c00-bed4-cea57afd425b) - Closing
[0m22:58:41.436923 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:58:41.436923 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:58:41.448970 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:58:41.448970 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:58:41.448970 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:58:41.671237 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=fdbe41e0-b6ed-429b-815f-b3e9908713b9) - Created
[0m22:58:42.148367 [debug] [ThreadPool]: SQL status: OK in 0.700 seconds
[0m22:58:42.158607 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=fdbe41e0-b6ed-429b-815f-b3e9908713b9, command-id=827b8f88-fdd1-4c15-9012-77ad4d51d4b0) - Closing
[0m22:58:42.165532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED7FADAF0>]}
[0m22:58:42.173214 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m22:58:42.174211 [info ] [Thread-1 (]: 1 of 6 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m22:58:42.176098 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:58:42.176098 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m22:58:42.177083 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m22:58:42.187129 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m22:58:42.188120 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m22:58:42.214163 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:58:42.221373 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:58:42.222373 [debug] [Thread-1 (]: Safe create: False
[0m22:58:42.234551 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:58:42.235592 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:42.235592 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m22:58:42.236560 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:58:42.431099 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a) - Created
[0m22:58:42.856901 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m22:58:42.859902 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=9db04320-6e35-4b8c-800f-bd4f2f1ba800) - Closing
[0m22:58:42.860894 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:58:42.871283 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:42.871283 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m22:58:43.137582 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:58:43.142586 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=b3b73b69-7b48-46e1-9414-81e5b4d6bb3a) - Closing
[0m22:58:43.152796 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:43.152796 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:58:43.442459 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m22:58:43.448369 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=fbab99ad-6c63-48c9-8d77-27bd0d0a6d40) - Closing
[0m22:58:43.458356 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:43.459356 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m22:58:43.779809 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m22:58:43.784052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=1f974cd6-8f95-4f7e-8910-98ab4c54dd00) - Closing
[0m22:58:43.789965 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:43.790983 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:58:44.464273 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m22:58:44.469722 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=fffce63a-97f5-4938-901e-148eb588e8a4) - Closing
[0m22:58:44.479812 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:44.480812 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:58:45.411175 [debug] [Thread-1 (]: SQL status: OK in 0.930 seconds
[0m22:58:45.415602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=c018b792-ec98-4d0f-950d-606f203f5258) - Closing
[0m22:58:45.423266 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:45.423266 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:58:45.721913 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m22:58:45.725418 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=07cffd48-aa00-4de8-8c92-eb400e78d64f) - Closing
[0m22:58:45.733408 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:45.734406 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:58:46.193645 [debug] [Thread-1 (]: SQL status: OK in 0.460 seconds
[0m22:58:46.196576 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=e3825d80-735f-49e2-b519-2b3db9b8d22a) - Closing
[0m22:58:46.202570 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:46.202570 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:58:46.677158 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m22:58:46.681777 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=a77b612d-2ca0-4d09-a104-3f00c4d2e44f) - Closing
[0m22:58:46.684781 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:58:46.694785 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m22:58:46.714406 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:46.714406 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m22:58:46.986877 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:58:46.988877 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=4a67b57b-ddf8-4656-a927-a86a03dbde2f) - Closing
[0m22:58:46.990886 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:46.990886 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m22:58:47.269671 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:58:47.274692 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=a04c96ec-8fbf-454f-a96a-9580235be13e) - Closing
[0m22:58:47.288352 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:58:47.289598 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:58:47.289598 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:58:50.851491 [debug] [Thread-1 (]: SQL status: OK in 3.560 seconds
[0m22:58:50.852526 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=c413abc4-5b0c-4dab-8ef9-9451264f30f8) - Closing
[0m22:58:50.866742 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB1C8FE90>]}
[0m22:58:50.866742 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 8.69s]
[0m22:58:50.868824 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m22:58:50.868824 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m22:58:50.868824 [info ] [Thread-1 (]: 2 of 6 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m22:58:50.872091 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_cleaned, now model.jra_dbt.indeed_test)
[0m22:58:50.873085 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, name=model.jra_dbt.indeed_test, idle-time=0.008286476135253906s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_cleaned
[0m22:58:50.874085 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m22:58:50.878078 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m22:58:50.879079 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m22:58:50.880340 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:58:50.881341 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:58:50.881341 [debug] [Thread-1 (]: Safe create: False
[0m22:58:50.882504 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m22:58:50.883524 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:50.883524 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_test`
    )
    
[0m22:58:51.537768 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m22:58:51.540294 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=947056a8-e492-48ea-9a53-b8c9a0b723d8) - Closing
[0m22:58:51.541344 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:58:51.545336 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:51.545336 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test'
  
[0m22:58:51.805771 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:58:51.807770 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=2f67aca3-f09a-42b6-b833-7fc15431ef82) - Closing
[0m22:58:51.808851 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:51.809850 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m22:58:52.145333 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m22:58:52.149342 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=ad820519-8c2e-4584-be82-dad51f3cad2c) - Closing
[0m22:58:52.151341 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:52.151341 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test'
    AND is_nullable = 'NO';
  
[0m22:58:52.509728 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m22:58:52.513797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=012b1578-a6cf-4607-bc71-00460e9e1030) - Closing
[0m22:58:52.517795 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:52.518803 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:58:53.140131 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m22:58:53.145496 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=1ef2ec48-7eb6-44ce-b8e6-ea7dc2d0f254) - Closing
[0m22:58:53.150475 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:53.151480 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:58:53.905483 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m22:58:53.910488 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=967b4bff-4c56-4c5a-85cb-ff3c0190990a) - Closing
[0m22:58:53.916474 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:53.917462 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m22:58:54.160679 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m22:58:54.164885 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=9b00e78d-2ff7-4315-be0b-c4dbb08f2261) - Closing
[0m22:58:54.169944 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:54.169944 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_test`
  
[0m22:58:54.485136 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m22:58:54.490507 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=fc9e6951-5113-49ce-bebf-7f1f43954293) - Closing
[0m22:58:54.492505 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:54.493505 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
describe extended `demo`.`demo_schema`.`indeed_test`
  
[0m22:58:55.085393 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m22:58:55.092540 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=44665a55-d085-443e-968d-20c9ae65ff1c) - Closing
[0m22:58:55.094535 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:58:55.095531 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_test`
[0m22:58:55.100537 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:55.100537 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_test`

  
[0m22:58:55.379814 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:58:55.384527 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=621e57b5-3fc0-43a9-9890-07b5781d2342) - Closing
[0m22:58:55.387816 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:55.387816 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m22:58:55.629709 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m22:58:55.635217 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=16c9618f-8354-4184-a77c-f6ccf60a0162) - Closing
[0m22:58:55.637172 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m22:58:55.638195 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m22:58:55.639162 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_test` as DBT_INTERNAL_DEST
    using
        `indeed_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:00.050854 [debug] [Thread-1 (]: SQL status: OK in 4.410 seconds
[0m22:59:00.052844 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=d952f573-412a-46f7-8237-a75387803387) - Closing
[0m22:59:00.053937 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED99CFE30>]}
[0m22:59:00.053937 [info ] [Thread-1 (]: 2 of 6 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 9.18s]
[0m22:59:00.056024 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m22:59:00.056024 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_cleaned_test
[0m22:59:00.056024 [info ] [Thread-1 (]: 3 of 6 START sql incremental model demo_schema.stg_cleaned_test ................ [RUN]
[0m22:59:00.056960 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_test, now model.jra_dbt.stg_cleaned_test)
[0m22:59:00.056960 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, name=model.jra_dbt.stg_cleaned_test, idle-time=0.0030236244201660156s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_test
[0m22:59:00.056960 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_cleaned_test
[0m22:59:00.059973 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_cleaned_test"
[0m22:59:00.061019 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_cleaned_test
[0m22:59:00.062973 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:59:00.062973 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:59:00.062973 [debug] [Thread-1 (]: Safe create: False
[0m22:59:00.063973 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m22:59:00.065023 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:00.065023 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

      create or replace temporary view `stg_cleaned_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`stg_test2`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_cleaned_test`
    )
    
[0m22:59:00.585547 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m22:59:00.588735 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=0a67180c-4e47-4c75-9ed9-2a70969097fa) - Closing
[0m22:59:00.589805 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:59:00.591821 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:00.592828 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
  
[0m22:59:00.844913 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m22:59:00.849909 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=0a8646a4-2221-415f-a068-6b6e3c60bab8) - Closing
[0m22:59:00.853886 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:00.853886 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m22:59:01.119715 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:59:01.121708 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=6372a032-cc1d-4eec-a747-f413f5a260a6) - Closing
[0m22:59:01.125675 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:01.125675 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
    AND is_nullable = 'NO';
  
[0m22:59:01.439769 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m22:59:01.445559 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=5109b805-e999-469f-9811-221f8f5713a6) - Closing
[0m22:59:01.449563 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:01.449563 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:01.964372 [debug] [Thread-1 (]: SQL status: OK in 0.510 seconds
[0m22:59:01.968183 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=7123ce74-5a99-4a46-9fe4-0fcd86af3930) - Closing
[0m22:59:02.020970 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:02.021974 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:02.800387 [debug] [Thread-1 (]: SQL status: OK in 0.780 seconds
[0m22:59:02.801392 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=a5ea243d-8225-40b7-8537-94e0a6a1b1bb) - Closing
[0m22:59:02.803388 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:02.803388 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m22:59:03.079559 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:59:03.082547 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=e558b359-e396-456b-a550-e091d190bf65) - Closing
[0m22:59:03.084632 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:03.085633 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m22:59:03.395767 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m22:59:03.401578 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=cf3cd56e-2fc8-46e3-b36b-38c2a6cdbbdc) - Closing
[0m22:59:03.406569 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:03.407570 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
describe extended `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m22:59:03.821404 [debug] [Thread-1 (]: SQL status: OK in 0.410 seconds
[0m22:59:03.828330 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=51f88d77-5ea3-4786-956f-f88b62ecde70) - Closing
[0m22:59:03.829329 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:59:03.830327 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_cleaned_test`
[0m22:59:03.835621 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:03.837606 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_cleaned_test`

  
[0m22:59:04.095397 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:59:04.096800 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=5c4d2b66-4ef5-4c50-8269-f9c352271dee) - Closing
[0m22:59:04.099367 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:04.099367 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `stg_cleaned_test__dbt_tmp`

  
[0m22:59:04.306323 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m22:59:04.308304 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=a5d65a5f-b1ef-48eb-96d9-f648d59b8d4f) - Closing
[0m22:59:04.309303 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m22:59:04.310303 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m22:59:04.311421 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_cleaned_test` as DBT_INTERNAL_DEST
    using
        `stg_cleaned_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:08.087102 [debug] [Thread-1 (]: SQL status: OK in 3.780 seconds
[0m22:59:08.088103 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=efca1eca-0bd9-4ed8-aff6-2db13b53d2ae) - Closing
[0m22:59:08.089103 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED71A3BF0>]}
[0m22:59:08.090102 [info ] [Thread-1 (]: 3 of 6 OK created sql incremental model demo_schema.stg_cleaned_test ........... [[32mOK[0m in 8.03s]
[0m22:59:08.090102 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_cleaned_test
[0m22:59:08.090102 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m22:59:08.091534 [info ] [Thread-1 (]: 4 of 6 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m22:59:08.091534 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_cleaned_test, now model.jra_dbt.stg_jobs_description_new)
[0m22:59:08.091534 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, name=model.jra_dbt.stg_jobs_description_new, idle-time=0.002431631088256836s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_cleaned_test
[0m22:59:08.092564 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m22:59:08.095569 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.095569 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m22:59:08.097569 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:59:08.098558 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:59:08.098558 [debug] [Thread-1 (]: Safe create: False
[0m22:59:08.099556 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.099556 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.100779 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`

Union all
SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`indeed_cleaned`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m22:59:08.452244 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m22:59:08.453242 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=09b9e60f-a0b1-4666-8df0-c89c9ae2029b) - Closing
[0m22:59:08.454245 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:59:08.456644 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.465437 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m22:59:08.711903 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m22:59:08.716922 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=993f08e7-68cd-483f-995e-ffc5a56237fe) - Closing
[0m22:59:08.720926 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.721894 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m22:59:08.988551 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:59:08.992575 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=25b770cf-c3d8-4cf3-9889-4ac111f4c125) - Closing
[0m22:59:08.996604 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:08.996604 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m22:59:09.262064 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:59:09.264157 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=d686e5a7-2627-4b30-8d95-f976641567fb) - Closing
[0m22:59:09.268232 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:09.269234 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:09.776228 [debug] [Thread-1 (]: SQL status: OK in 0.510 seconds
[0m22:59:09.780196 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=4dd223bd-8efe-4aff-a0ed-42b7cf07a8f8) - Closing
[0m22:59:09.784217 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:09.784217 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:10.601932 [debug] [Thread-1 (]: SQL status: OK in 0.820 seconds
[0m22:59:10.603976 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=f4feeb1f-8a3b-4ade-8085-d085f9be97ba) - Closing
[0m22:59:10.605034 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:10.606360 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m22:59:10.866747 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:59:10.870019 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=1582a5e7-8434-44ad-8be9-f5180739e1b4) - Closing
[0m22:59:10.873137 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:10.873137 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m22:59:11.251212 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m22:59:11.253260 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=7b78dbe1-0ba6-4f16-91ae-085eac0a3136) - Closing
[0m22:59:11.257210 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:11.258198 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m22:59:11.626381 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m22:59:11.630852 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=9be92703-feba-49cd-b30d-6642915e75d7) - Closing
[0m22:59:11.631854 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m22:59:11.632844 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m22:59:11.636173 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:11.636173 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m22:59:11.876001 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m22:59:11.879991 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=dbb0e8ca-a046-41c9-a311-09e9137dfd42) - Closing
[0m22:59:11.883991 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:11.883991 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m22:59:12.161836 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:59:12.164876 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=e16dce0f-1f08-40b1-a359-9f60493463c2) - Closing
[0m22:59:12.165831 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m22:59:12.166801 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m22:59:12.168846 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:17.432830 [debug] [Thread-1 (]: SQL status: OK in 5.260 seconds
[0m22:59:17.433825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=dc29f0c5-094a-418d-94af-170335079c71) - Closing
[0m22:59:17.508875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED95789B0>]}
[0m22:59:17.510816 [info ] [Thread-1 (]: 4 of 6 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 9.42s]
[0m22:59:17.513818 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m22:59:17.514820 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_test2
[0m22:59:17.516826 [info ] [Thread-1 (]: 5 of 6 START sql incremental model demo_schema.stg_test2 ....................... [RUN]
[0m22:59:17.518815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_jobs_description_new, now model.jra_dbt.stg_test2)
[0m22:59:17.519819 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, name=model.jra_dbt.stg_test2, idle-time=0.011957168579101562s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_jobs_description_new
[0m22:59:17.520862 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_test2
[0m22:59:17.526543 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_test2"
[0m22:59:17.527542 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_test2
[0m22:59:17.529752 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:59:17.529752 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:59:17.529752 [debug] [Thread-1 (]: Safe create: False
[0m22:59:17.530788 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m22:59:17.531773 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:17.531773 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

      create or replace temporary view `stg_test2__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`stg_test2`
    )
    
[0m22:59:18.275668 [debug] [Thread-1 (]: SQL status: OK in 0.740 seconds
[0m22:59:18.275668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=64d77355-ed98-41d2-bfd7-046aaf81d8b0) - Closing
[0m22:59:18.277205 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:59:18.278246 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:18.278246 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2'
  
[0m22:59:18.485547 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m22:59:18.486546 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=52dad353-a1a4-466f-95ac-c7282350ff7c) - Closing
[0m22:59:18.489052 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:18.489052 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m22:59:18.692809 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m22:59:18.696891 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=4e2314c5-a7b1-47db-b859-57eee3e0a9fb) - Closing
[0m22:59:18.699810 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:18.699810 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2'
    AND is_nullable = 'NO';
  
[0m22:59:19.066436 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m22:59:19.071599 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=9cf2fe93-0d80-4b4b-9a0a-befed9980aee) - Closing
[0m22:59:19.075668 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:19.077683 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:19.679212 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m22:59:19.682247 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=75fc6b77-e141-4af2-9ff1-50a489b2fbcb) - Closing
[0m22:59:19.688228 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:19.689271 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:20.662286 [debug] [Thread-1 (]: SQL status: OK in 0.970 seconds
[0m22:59:20.664454 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=5e98e9b8-4675-4fa6-8488-80513ec1d74a) - Closing
[0m22:59:20.665469 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:20.666462 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m22:59:20.953953 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m22:59:20.954958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=095fcfb7-9dab-407f-8136-4e661cab8bcc) - Closing
[0m22:59:20.957293 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:20.957293 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_test2`
  
[0m22:59:21.223380 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:59:21.227380 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=d3a195d2-8af9-4b69-b933-475c36380bd3) - Closing
[0m22:59:21.230386 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:21.230386 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
describe extended `demo`.`demo_schema`.`stg_test2`
  
[0m22:59:21.677921 [debug] [Thread-1 (]: SQL status: OK in 0.450 seconds
[0m22:59:21.683638 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=7ec4567b-00ea-4b4a-8b11-321c01ebfbec) - Closing
[0m22:59:21.686664 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'base_salary': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:59:21.687636 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_test2`
[0m22:59:21.692639 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:21.692639 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_test2`

  
[0m22:59:21.946914 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m22:59:21.953911 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=a71681fd-03cd-44ae-a2c4-ce69ff2441b1) - Closing
[0m22:59:21.960421 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:21.960421 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `stg_test2__dbt_tmp`

  
[0m22:59:22.188033 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m22:59:22.191942 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=f49305f3-4fe9-4682-a79f-55dd6b9ccbb1) - Closing
[0m22:59:22.195125 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m22:59:22.196062 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m22:59:22.196062 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_test2` as DBT_INTERNAL_DEST
    using
        `stg_test2__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:26.431031 [debug] [Thread-1 (]: SQL status: OK in 4.230 seconds
[0m22:59:26.433027 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=f3ed5609-bf6b-4cb8-80d7-db708b59d990) - Closing
[0m22:59:26.434027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED952C290>]}
[0m22:59:26.435019 [info ] [Thread-1 (]: 5 of 6 OK created sql incremental model demo_schema.stg_test2 .................. [[32mOK[0m in 8.92s]
[0m22:59:26.436987 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_test2
[0m22:59:26.436987 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m22:59:26.437956 [info ] [Thread-1 (]: 6 of 6 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m22:59:26.439014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_test2, now model.jra_dbt.stg_unified_test)
[0m22:59:26.440047 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, name=model.jra_dbt.stg_unified_test, idle-time=0.006020069122314453s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_test2
[0m22:59:26.440047 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m22:59:26.448019 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m22:59:26.448965 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m22:59:26.450268 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:59:26.451351 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:59:26.451351 [debug] [Thread-1 (]: Safe create: False
[0m22:59:26.452354 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m22:59:26.452354 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:26.452354 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    min_salary,
    max_salary,
    job_posted_date,
    scraped_dts, 
    ingest_dts,
    _rescued_data,
    is_enriched,
    job_source,
    is_active
FROM `demo`.`demo_schema`.`stg_cleaned_test`


    WHERE ingest_dts > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_unified_test`
    )
    
[0m22:59:26.986422 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m22:59:26.987426 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=beb0ecb8-0725-4dea-90a7-79d4d1b917b4) - Closing
[0m22:59:26.987426 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:59:26.989361 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:26.989361 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m22:59:27.286217 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m22:59:27.289230 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=642f36d3-df37-4219-99b8-627f23881436) - Closing
[0m22:59:27.292163 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:27.293170 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m22:59:27.526082 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m22:59:27.528083 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=cce1f41b-7c53-4d65-b618-be92546256c3) - Closing
[0m22:59:27.530090 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:27.530090 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m22:59:27.821285 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m22:59:27.823329 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=6b5e6070-9f72-4767-8a5d-a084cde044df) - Closing
[0m22:59:27.826358 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:27.826358 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:28.371188 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m22:59:28.377196 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=ed129d33-24ff-45de-90c2-b0fff1646485) - Closing
[0m22:59:28.384210 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:28.385196 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:29.060069 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m22:59:29.063136 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=1d150244-92e8-49cf-82af-d2e5ac448b4a) - Closing
[0m22:59:29.065420 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:29.065420 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m22:59:29.291132 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m22:59:29.294116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=e0db7565-a015-440a-a9a9-58ea4ccf396a) - Closing
[0m22:59:29.297151 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:29.298127 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m22:59:29.592941 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m22:59:29.595929 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=062e88d0-94c3-4663-b263-572092c1686b) - Closing
[0m22:59:29.597972 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:29.597972 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m22:59:29.989721 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m22:59:29.993063 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=2920427a-c2f7-4bbd-ab87-64f831afe3be) - Closing
[0m22:59:29.994118 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m22:59:29.994118 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m22:59:29.996109 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:29.996109 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m22:59:30.271174 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m22:59:30.274822 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=5ef5a24c-b607-4b65-941e-1b220e8787b6) - Closing
[0m22:59:30.277822 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:30.278818 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m22:59:30.534927 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m22:59:30.537921 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=b0183e28-763a-4306-a3cd-8bda849b4ecc) - Closing
[0m22:59:30.538922 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m22:59:30.539923 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m22:59:30.539923 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:35.201013 [debug] [Thread-1 (]: SQL status: OK in 4.660 seconds
[0m22:59:35.202002 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a, command-id=19b2e44c-148a-4f93-9e4f-f3028e5b6026) - Closing
[0m22:59:35.204000 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8b78898-b18b-4cb7-afe8-a4584620f0fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB5D90350>]}
[0m22:59:35.204000 [info ] [Thread-1 (]: 6 of 6 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 8.76s]
[0m22:59:35.204948 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m22:59:35.208090 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=53.040531158447266s, language=None, compute-name=) - Reusing connection previously named master
[0m22:59:35.209080 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:59:35.209080 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:59:35.210079 [debug] [MainThread]: On list_demo: Close
[0m22:59:35.211078 [debug] [MainThread]: Databricks adapter: Connection(session-id=c75afc7b-3c09-49b0-9013-a2f2e0ef3003) - Closing
[0m22:59:35.280517 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:59:35.281507 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:59:35.282528 [debug] [MainThread]: Databricks adapter: Connection(session-id=fdbe41e0-b6ed-429b-815f-b3e9908713b9) - Closing
[0m22:59:35.355663 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m22:59:35.357661 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m22:59:35.358668 [debug] [MainThread]: Databricks adapter: Connection(session-id=0df51e72-1d52-4ba4-8000-1213775f5a5a) - Closing
[0m22:59:37.132878 [info ] [MainThread]: 
[0m22:59:37.134825 [info ] [MainThread]: Finished running 6 incremental models in 0 hours 0 minutes and 56.52 seconds (56.52s).
[0m22:59:37.140886 [debug] [MainThread]: Command end result
[0m22:59:37.162432 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:59:37.164486 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:59:37.169384 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:59:37.169384 [info ] [MainThread]: 
[0m22:59:37.170416 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:59:37.170416 [info ] [MainThread]: 
[0m22:59:37.170416 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m22:59:37.171736 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:59:37.171736 [debug] [MainThread]: Command `dbt run` succeeded at 22:59:37.171736 after 58.54 seconds
[0m22:59:37.171736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB45F1730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CEB4A4E360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CED99F6E70>]}
[0m22:59:37.172753 [debug] [MainThread]: Flushing usage events
[0m22:59:38.480995 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:12:46.460591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AE184AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AD837E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AE32DDC0>]}


============================== 23:12:46.464683 | 9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7 ==============================
[0m23:12:46.464683 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:12:46.465605 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m23:12:47.205265 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:12:47.205265 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:12:47.205265 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:12:47.843360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AB374FB0>]}
[0m23:12:47.902057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6ADB9E930>]}
[0m23:12:47.902057 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:12:48.126281 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:12:48.197938 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:12:48.198921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AF426D20>]}
[0m23:12:48.246337 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:12:48.246337 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m23:12:48.464613 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:12:48.473582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2D33A10>]}
[0m23:12:48.526528 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:12:48.528530 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:12:48.552016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2CD7F50>]}
[0m23:12:48.552016 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:12:48.553013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D16ABCB0>]}
[0m23:12:48.554191 [info ] [MainThread]: 
[0m23:12:48.554566 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:12:48.554566 [info ] [MainThread]: 
[0m23:12:48.554566 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:48.555727 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:12:48.613327 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:48.613327 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:12:48.614319 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:12:48.614319 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:12:48.615318 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:12:49.094633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=8eb100aa-53de-4a36-85d6-76abe8d60246) - Created
[0m23:12:49.784267 [debug] [ThreadPool]: SQL status: OK in 1.170 seconds
[0m23:12:49.785254 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8eb100aa-53de-4a36-85d6-76abe8d60246, command-id=ff3d85cd-5bbe-40ed-88c0-56b9fbe964a3) - Closing
[0m23:12:49.786753 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:49.786753 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:12:49.796100 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:12:49.796100 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:12:49.796100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:12:49.967296 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=727b0267-21ee-45a6-9f61-15a590f4c1d0) - Created
[0m23:12:50.513845 [debug] [ThreadPool]: SQL status: OK in 0.720 seconds
[0m23:12:50.527783 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=727b0267-21ee-45a6-9f61-15a590f4c1d0, command-id=9681fcca-fc14-4d99-9aee-69504467af5c) - Closing
[0m23:12:50.535096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2D37E00>]}
[0m23:12:50.544610 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:12:50.545573 [info ] [Thread-1 (]: 1 of 6 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:12:50.546586 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:50.546586 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:12:50.546586 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:12:50.554136 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:12:50.556066 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:12:50.581607 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:12:50.588566 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:12:50.588566 [debug] [Thread-1 (]: Safe create: False
[0m23:12:50.600605 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:12:50.601603 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:50.601603 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    MIN_AMOUNT AS min_salary, 
    MAX_AMOUNT AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:12:50.602606 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:12:50.840589 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0) - Created
[0m23:12:51.227339 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m23:12:51.230360 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=cb18e4b5-05b7-44ba-b089-1aca23e818d4) - Closing
[0m23:12:51.230360 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:12:51.239353 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:51.240468 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:12:51.587609 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m23:12:51.591601 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=ff84256b-7a6f-4a1d-b696-d3a5c3444071) - Closing
[0m23:12:51.598602 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:51.599545 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:12:51.849292 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:12:51.852668 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=ee1e004f-8815-4b5f-b649-272cab929b4e) - Closing
[0m23:12:51.862371 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:51.862371 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:12:52.146028 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:12:52.150033 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=629cba04-f4ea-4200-8f39-8002e7d26a18) - Closing
[0m23:12:52.157653 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:52.157653 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:12:52.695786 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m23:12:52.698722 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=a4b1a384-c0c7-49bd-b233-c5df205c4507) - Closing
[0m23:12:52.708370 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:52.709368 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:12:53.438119 [debug] [Thread-1 (]: SQL status: OK in 0.730 seconds
[0m23:12:53.441116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=281147ed-8198-4510-a941-b5930bcf8006) - Closing
[0m23:12:53.451921 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:53.451921 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:12:53.654644 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:12:53.657446 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=71e558ca-a64a-41ae-946e-c9fa6fe1c338) - Closing
[0m23:12:53.660451 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:53.660451 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:12:54.019972 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m23:12:54.022966 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=35ab1d15-4a58-483f-8ad7-e7756b393a66) - Closing
[0m23:12:54.026963 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:54.026963 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:12:54.429118 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m23:12:54.433989 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c91ab3b7-09d4-4af2-acfb-c0f67d209312) - Closing
[0m23:12:54.435979 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:12:54.443978 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:12:54.473552 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:54.473552 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:12:54.743434 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:12:54.747428 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=9372becc-a383-41a7-b169-97b34eccec1d) - Closing
[0m23:12:54.750426 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:54.751507 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:12:55.024660 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:12:55.025659 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=cd874b56-5b74-43a0-a7fb-e793ba14ec9d) - Closing
[0m23:12:55.034662 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:12:55.035663 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:12:55.036661 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:12:58.455095 [debug] [Thread-1 (]: SQL status: OK in 3.420 seconds
[0m23:12:58.456096 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=64597520-0c4a-492b-a360-eae8f0eeb7a3) - Closing
[0m23:12:58.478259 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6AD786000>]}
[0m23:12:58.479294 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 7.93s]
[0m23:12:58.480353 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:12:58.481665 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m23:12:58.481665 [info ] [Thread-1 (]: 2 of 6 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m23:12:58.482682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_cleaned, now model.jra_dbt.indeed_test)
[0m23:12:58.482682 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, name=model.jra_dbt.indeed_test, idle-time=0.006535768508911133s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_cleaned
[0m23:12:58.483721 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m23:12:58.488682 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m23:12:58.489679 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m23:12:58.490679 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:12:58.491678 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:12:58.491678 [debug] [Thread-1 (]: Safe create: False
[0m23:12:58.492678 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m23:12:58.492678 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:12:58.493679 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_test`
    )
    
[0m23:12:58.979177 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m23:12:58.980184 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=aac3022a-bb46-4a35-8033-ac8eb08f45e8) - Closing
[0m23:12:58.980184 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:12:58.982192 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:12:58.983192 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test'
  
[0m23:12:59.172526 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m23:12:59.174215 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=716e3e78-7204-46c6-b529-6f00418b23b1) - Closing
[0m23:12:59.175846 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:12:59.175846 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m23:12:59.359756 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m23:12:59.362770 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=962b5524-f88e-4e2f-9bb9-ddd3c6441d76) - Closing
[0m23:12:59.363783 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:12:59.363783 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test'
    AND is_nullable = 'NO';
  
[0m23:12:59.637230 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:12:59.640166 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=f1044238-f91a-41b4-a5c7-dd94aea6b37a) - Closing
[0m23:12:59.643163 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:12:59.644165 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:00.193484 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m23:13:00.197503 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=132dc3f3-ab44-4b3f-8eaf-a4062816c6da) - Closing
[0m23:13:00.201635 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:00.201635 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:00.876259 [debug] [Thread-1 (]: SQL status: OK in 0.670 seconds
[0m23:13:00.880456 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=502d709d-58f5-44f8-9353-d7e91408f641) - Closing
[0m23:13:00.885032 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:00.887052 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m23:13:01.096464 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:01.098928 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=fce450a1-8832-4aef-9248-aa34b72f7718) - Closing
[0m23:13:01.101296 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:01.101296 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_test`
  
[0m23:13:01.353368 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:13:01.355071 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=7f878cfd-cbff-412d-bc08-391e0d4a3539) - Closing
[0m23:13:01.356796 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:01.356796 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
describe extended `demo`.`demo_schema`.`indeed_test`
  
[0m23:13:01.881438 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m23:13:01.889086 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=92626ee7-a51d-48b0-ae47-ab437352ead4) - Closing
[0m23:13:01.891083 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:13:01.893083 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_test`
[0m23:13:01.896531 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:01.896531 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_test`

  
[0m23:13:02.157265 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:02.162259 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=086cf28d-f8f3-4a08-a459-9e8ad15ba27d) - Closing
[0m23:13:02.166255 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:02.167257 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m23:13:02.364135 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:13:02.365189 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=9cd5a6ac-7acb-4730-9fe3-3cff4cc0edf2) - Closing
[0m23:13:02.366187 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m23:13:02.368138 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:13:02.368138 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_test` as DBT_INTERNAL_DEST
    using
        `indeed_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:13:05.565257 [debug] [Thread-1 (]: SQL status: OK in 3.200 seconds
[0m23:13:05.567252 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=190b2e00-4935-4fb3-8093-52b92e0426ca) - Closing
[0m23:13:05.568251 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2D89EE0>]}
[0m23:13:05.569250 [info ] [Thread-1 (]: 2 of 6 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 7.09s]
[0m23:13:05.570250 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m23:13:05.570250 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_cleaned_test
[0m23:13:05.571249 [info ] [Thread-1 (]: 3 of 6 START sql incremental model demo_schema.stg_cleaned_test ................ [RUN]
[0m23:13:05.572328 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_test, now model.jra_dbt.stg_cleaned_test)
[0m23:13:05.573318 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, name=model.jra_dbt.stg_cleaned_test, idle-time=0.005067110061645508s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_test
[0m23:13:05.573318 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_cleaned_test
[0m23:13:05.579627 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_cleaned_test"
[0m23:13:05.580576 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_cleaned_test
[0m23:13:05.587737 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:13:05.588672 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:13:05.588672 [debug] [Thread-1 (]: Safe create: False
[0m23:13:05.589763 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m23:13:05.590734 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:05.590734 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

      create or replace temporary view `stg_cleaned_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`stg_test2`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_cleaned_test`
    )
    
[0m23:13:05.923588 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m23:13:05.926587 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=393086d2-386e-439d-b15d-075db1ea8d6f) - Closing
[0m23:13:05.927533 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:13:05.930725 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:05.930725 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
  
[0m23:13:06.170993 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:13:06.174349 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=2f7f7d60-8473-4877-946d-2d95caa4970c) - Closing
[0m23:13:06.178925 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:06.179936 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m23:13:06.428787 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:13:06.429807 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c22c5f6f-7226-4f79-8cc0-662f78483290) - Closing
[0m23:13:06.431805 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:06.431805 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test'
    AND is_nullable = 'NO';
  
[0m23:13:06.691735 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:06.694934 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=0b7c5f32-376b-4b41-bc86-55b3c79a06e7) - Closing
[0m23:13:06.699322 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:06.700313 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:07.180884 [debug] [Thread-1 (]: SQL status: OK in 0.480 seconds
[0m23:13:07.183877 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=246d5d5e-ffd4-4425-8707-03aeee624677) - Closing
[0m23:13:07.185875 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:07.186875 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_cleaned_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_cleaned_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:07.817513 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m23:13:07.822504 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c81493ad-195d-4910-9cf7-bb434c1737f7) - Closing
[0m23:13:07.824498 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:07.824498 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_cleaned_test';
  
[0m23:13:08.033925 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:08.037921 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=4b9fc3bf-0e66-475b-8240-7caeed34e614) - Closing
[0m23:13:08.042921 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:08.042921 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m23:13:08.320781 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:13:08.326878 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=1785ddff-3cac-497d-9cd4-61baac7f79cd) - Closing
[0m23:13:08.332875 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:08.333814 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
describe extended `demo`.`demo_schema`.`stg_cleaned_test`
  
[0m23:13:08.698577 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m23:13:08.705664 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=ef285467-6894-4ef4-a853-6c15e7118cce) - Closing
[0m23:13:08.707855 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:13:08.707855 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_cleaned_test`
[0m23:13:08.711561 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:08.712574 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_cleaned_test`

  
[0m23:13:08.961758 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:13:08.967813 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=f9548946-2f36-4901-8070-1dbdfa5b4faa) - Closing
[0m23:13:08.972879 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:08.972879 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

    
DESCRIBE TABLE `stg_cleaned_test__dbt_tmp`

  
[0m23:13:09.214943 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:13:09.219935 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=6912782e-3825-4be9-a4f6-8a1aa6fc57f4) - Closing
[0m23:13:09.222880 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m23:13:09.224206 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m23:13:09.225269 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_cleaned_test` as DBT_INTERNAL_DEST
    using
        `stg_cleaned_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:13:12.229975 [debug] [Thread-1 (]: SQL status: OK in 3.000 seconds
[0m23:13:12.230975 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=87b99540-d08f-4ea8-b643-3450ca85b2e3) - Closing
[0m23:13:12.231974 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2DC0320>]}
[0m23:13:12.232982 [info ] [Thread-1 (]: 3 of 6 OK created sql incremental model demo_schema.stg_cleaned_test ........... [[32mOK[0m in 6.66s]
[0m23:13:12.234001 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_cleaned_test
[0m23:13:12.234001 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m23:13:12.234001 [info ] [Thread-1 (]: 4 of 6 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m23:13:12.235021 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_cleaned_test, now model.jra_dbt.stg_jobs_description_new)
[0m23:13:12.235021 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, name=model.jra_dbt.stg_jobs_description_new, idle-time=0.0030472278594970703s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_cleaned_test
[0m23:13:12.235021 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m23:13:12.242085 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.243164 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m23:13:12.245254 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:13:12.247191 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:13:12.247191 [debug] [Thread-1 (]: Safe create: False
[0m23:13:12.248741 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.248741 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.249769 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`indeed_cleaned`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m23:13:12.553136 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m23:13:12.556141 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=b6d91572-d483-47cb-98e5-052745ba0c99) - Closing
[0m23:13:12.557141 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:13:12.561138 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.562146 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m23:13:12.774632 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:12.778636 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=8151102c-e93e-4eff-9d90-556b809f58ff) - Closing
[0m23:13:12.781642 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.782589 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m23:13:12.961803 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m23:13:12.964815 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=027d4fe6-2f02-4d38-9246-7718a88c214c) - Closing
[0m23:13:12.967818 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:12.968809 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m23:13:13.227757 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:13.231143 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c0906a00-2c64-4c02-ab91-41d346641b14) - Closing
[0m23:13:13.234230 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:13.234230 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:13.650537 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m23:13:13.655602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=260fff2a-0cb3-4b1c-a583-93c0bbedc809) - Closing
[0m23:13:13.660182 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:13.661469 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:14.274039 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m23:13:14.275974 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=2dfb4b3d-4eca-4008-beb6-52f902606dce) - Closing
[0m23:13:14.277094 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:14.278142 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m23:13:14.495329 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:13:14.498346 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=59de5a54-c46e-4e53-a84d-49773284b2eb) - Closing
[0m23:13:14.500355 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:14.501332 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m23:13:14.729881 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m23:13:14.730899 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=6675cb83-539a-4c16-b0af-6bd899ede014) - Closing
[0m23:13:14.733883 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:14.733883 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m23:13:15.059879 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m23:13:15.061885 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=92b9b433-ebb6-419e-8695-f36f149b921a) - Closing
[0m23:13:15.062932 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m23:13:15.063952 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m23:13:15.065942 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:15.065942 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m23:13:15.302728 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:13:15.307873 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=2f1c51a4-ae09-4cc0-bdeb-ef740dc257ae) - Closing
[0m23:13:15.309886 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:15.310852 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m23:13:15.508554 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:13:15.511617 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=dbea1824-1aa3-44b1-9512-c7aae0c9057f) - Closing
[0m23:13:15.513620 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m23:13:15.514566 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m23:13:15.515911 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:13:21.176162 [debug] [Thread-1 (]: SQL status: OK in 5.660 seconds
[0m23:13:21.180172 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=33870798-2c6a-4382-8547-9fdfb6ea3216) - Closing
[0m23:13:21.239633 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2DCB440>]}
[0m23:13:21.241716 [info ] [Thread-1 (]: 4 of 6 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 9.00s]
[0m23:13:21.243667 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m23:13:21.244653 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_test2
[0m23:13:21.245765 [info ] [Thread-1 (]: 5 of 6 START sql incremental model demo_schema.stg_test2 ....................... [RUN]
[0m23:13:21.246706 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_jobs_description_new, now model.jra_dbt.stg_test2)
[0m23:13:21.247697 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, name=model.jra_dbt.stg_test2, idle-time=0.00806427001953125s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_jobs_description_new
[0m23:13:21.248718 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_test2
[0m23:13:21.252710 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_test2"
[0m23:13:21.253707 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_test2
[0m23:13:21.256039 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:13:21.257186 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:13:21.258091 [debug] [Thread-1 (]: Safe create: False
[0m23:13:21.259103 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m23:13:21.260053 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:21.260053 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

      create or replace temporary view `stg_test2__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`stg_test2`
    )
    
[0m23:13:21.843933 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m23:13:21.844935 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=ca4794b7-6def-48f4-a612-9f78d29e3512) - Closing
[0m23:13:21.845938 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:13:21.847932 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:21.848968 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2'
  
[0m23:13:22.058772 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:22.060816 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=67eb1dd3-127a-4698-b2d9-916b1f758bfd) - Closing
[0m23:13:22.062823 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:22.062823 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m23:13:22.255391 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m23:13:22.257427 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c8be194a-537d-475c-beed-020e0ef427e2) - Closing
[0m23:13:22.259398 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:22.259398 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2'
    AND is_nullable = 'NO';
  
[0m23:13:22.531480 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:13:22.535543 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=9614e1e4-851b-4fdb-b1aa-d1809a640992) - Closing
[0m23:13:22.538536 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:22.538536 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:23.117056 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m23:13:23.118056 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=91592818-a6d8-47db-bd96-33ee81a94432) - Closing
[0m23:13:23.121048 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:23.121982 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:23.807596 [debug] [Thread-1 (]: SQL status: OK in 0.690 seconds
[0m23:13:23.810109 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=f4ec5b39-bf2b-4d8b-bf0b-d5f55d0994fd) - Closing
[0m23:13:23.814242 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:23.815245 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m23:13:24.026817 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:24.030808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=2ace9aa3-cd8f-4f6e-a8dd-9c88962b4738) - Closing
[0m23:13:24.032782 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:24.033794 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_test2`
  
[0m23:13:24.289446 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:24.291445 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=38bdfdc3-dbe9-4003-8cd2-e0e7e7178707) - Closing
[0m23:13:24.292445 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:24.293444 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
describe extended `demo`.`demo_schema`.`stg_test2`
  
[0m23:13:24.673715 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m23:13:24.678272 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=d3a4208c-807a-4579-bab0-1a688a599f47) - Closing
[0m23:13:24.679344 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'base_salary': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:13:24.680359 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_test2`
[0m23:13:24.683291 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:24.684290 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_test2`

  
[0m23:13:25.036403 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m23:13:25.039388 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=66287c7d-7c1a-4db0-b1f2-b31c339cc11f) - Closing
[0m23:13:25.041400 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:25.041400 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `stg_test2__dbt_tmp`

  
[0m23:13:25.274455 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m23:13:25.281630 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=39ede435-db57-4e5c-bc49-4d80b632c956) - Closing
[0m23:13:25.283204 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m23:13:25.285293 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m23:13:25.285293 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_test2` as DBT_INTERNAL_DEST
    using
        `stg_test2__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:13:29.801790 [debug] [Thread-1 (]: SQL status: OK in 4.520 seconds
[0m23:13:29.801790 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=d3104e79-3e90-4965-a756-32d18f497a91) - Closing
[0m23:13:29.803859 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2CB73B0>]}
[0m23:13:29.804168 [info ] [Thread-1 (]: 5 of 6 OK created sql incremental model demo_schema.stg_test2 .................. [[32mOK[0m in 8.56s]
[0m23:13:29.804962 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_test2
[0m23:13:29.805473 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:13:29.805473 [info ] [Thread-1 (]: 6 of 6 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:13:29.806482 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_test2, now model.jra_dbt.stg_unified_test)
[0m23:13:29.806482 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, name=model.jra_dbt.stg_unified_test, idle-time=0.0036809444427490234s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_test2
[0m23:13:29.806482 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:13:29.809799 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:13:29.810805 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:13:29.813792 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:13:29.814792 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:13:29.814792 [debug] [Thread-1 (]: Safe create: False
[0m23:13:29.816791 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:13:29.817793 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:29.818794 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    min_salary,
    max_salary,
    job_posted_date,
    scraped_dts, 
    ingest_dts,
    _rescued_data,
    is_enriched,
    job_source,
    is_active
FROM `demo`.`demo_schema`.`stg_cleaned_test`


    WHERE ingest_dts > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_unified_test`
    )
    
[0m23:13:30.142290 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m23:13:30.143367 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=f64f4e19-242c-40e3-98af-64e5869c3c3a) - Closing
[0m23:13:30.143367 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:13:30.146374 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:30.146374 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:13:30.405823 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:30.407676 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=c8e7b88a-d60e-4bab-ba04-18db4649deec) - Closing
[0m23:13:30.410684 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:30.410684 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:13:30.649393 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:13:30.650393 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=12a79af8-1000-4490-9db8-ae96dd598a32) - Closing
[0m23:13:30.652447 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:30.652447 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:13:30.909046 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:13:30.910590 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=900f249e-7ed3-4c39-b60f-e36b2ad9d154) - Closing
[0m23:13:30.911590 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:30.912599 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:31.365569 [debug] [Thread-1 (]: SQL status: OK in 0.450 seconds
[0m23:13:31.367576 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=dfc72efe-1315-46a5-b7a3-a32585bc4daa) - Closing
[0m23:13:31.368583 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:31.369576 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:13:32.002614 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m23:13:32.003638 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=b2bd65d3-a6e2-4000-a4f1-e013d38f31b4) - Closing
[0m23:13:32.005637 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:32.005637 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:13:32.279160 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:13:32.280167 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=a8786ec4-53a1-48c9-a907-984c9ba90b56) - Closing
[0m23:13:32.282408 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:32.282408 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:13:32.549898 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:13:32.553211 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=f929907e-1d01-410e-a28b-5feb0f1b9450) - Closing
[0m23:13:32.556154 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:32.557189 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:13:32.906316 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m23:13:32.912051 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=abf97daf-9670-4dbb-9a8a-e574cbb62728) - Closing
[0m23:13:32.913122 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:13:32.914142 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:13:32.918455 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:32.919581 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:13:33.232633 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m23:13:33.237636 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=4aa2efd7-9504-4d3f-9d2b-3c45c98d2f78) - Closing
[0m23:13:33.242335 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:33.242335 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:13:33.451315 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:13:33.452211 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=b160f776-3ba5-4145-bdc2-627ddde3c575) - Closing
[0m23:13:33.453255 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:13:33.454252 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:13:33.454252 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:13:36.667596 [debug] [Thread-1 (]: SQL status: OK in 3.210 seconds
[0m23:13:36.670584 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0, command-id=71835c1a-aa69-4375-b3d3-5c97c26fe8a6) - Closing
[0m23:13:36.672577 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e052825-ac3f-4ebb-8a6e-ce36f40cd2f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2E6F080>]}
[0m23:13:36.673590 [info ] [Thread-1 (]: 6 of 6 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 6.87s]
[0m23:13:36.675591 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:13:36.679581 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=46.144484519958496s, language=None, compute-name=) - Reusing connection previously named master
[0m23:13:36.681547 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:13:36.681547 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:13:36.681547 [debug] [MainThread]: On list_demo: Close
[0m23:13:36.682531 [debug] [MainThread]: Databricks adapter: Connection(session-id=8eb100aa-53de-4a36-85d6-76abe8d60246) - Closing
[0m23:13:36.747428 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:13:36.747428 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:13:36.748507 [debug] [MainThread]: Databricks adapter: Connection(session-id=727b0267-21ee-45a6-9f61-15a590f4c1d0) - Closing
[0m23:13:36.807309 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:13:36.807309 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:13:36.807309 [debug] [MainThread]: Databricks adapter: Connection(session-id=58427034-cc91-4e48-a001-29b0fdcf19d0) - Closing
[0m23:13:38.248067 [info ] [MainThread]: 
[0m23:13:38.248067 [info ] [MainThread]: Finished running 6 incremental models in 0 hours 0 minutes and 49.69 seconds (49.69s).
[0m23:13:38.250154 [debug] [MainThread]: Command end result
[0m23:13:38.270292 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:13:38.271457 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:13:38.277058 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:13:38.277058 [info ] [MainThread]: 
[0m23:13:38.277058 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:13:38.278058 [info ] [MainThread]: 
[0m23:13:38.278058 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=6
[0m23:13:38.279060 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:13:38.279060 [debug] [MainThread]: Command `dbt run` succeeded at 23:13:38.279060 after 51.94 seconds
[0m23:13:38.280068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6ADBF6720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D2D07FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A6D18E9340>]}
[0m23:13:38.280068 [debug] [MainThread]: Flushing usage events
[0m23:13:39.695490 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:16:52.187733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200C9B2DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200C9B3860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200C9B3D40>]}


============================== 23:16:52.191438 | 1505a11a-ea09-4828-afd8-033566d88828 ==============================
[0m23:16:52.191438 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:16:52.192054 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run -selectt stg_unified_test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m23:16:52.930296 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:16:52.931299 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:16:52.931299 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:16:53.607142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200CA4E990>]}
[0m23:16:53.665106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000120302F80E0>]}
[0m23:16:53.666045 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:16:53.887058 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:16:53.956207 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:16:53.956207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001203064E660>]}
[0m23:16:54.003737 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:16:54.003737 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:16:54.008780 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:16:54.027344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012031714B30>]}
[0m23:16:54.077207 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:16:54.078217 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:16:54.109372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001203175E690>]}
[0m23:16:54.110395 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:16:54.110395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012030120A40>]}
[0m23:16:54.110395 [warn ] [MainThread]: The selection criterion 'electt' does not match any enabled nodes
[0m23:16:54.112371 [info ] [MainThread]: 
[0m23:16:54.112371 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:16:54.113359 [info ] [MainThread]: 
[0m23:16:54.113359 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:16:54.114351 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:16:54.115359 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:16:54.115359 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:16:54.115359 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:16:54.116349 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:16:54.116349 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:16:54.341854 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=b53ee423-9f0d-43f1-98f6-2de6be3cb7b9) - Created
[0m23:16:54.481403 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m23:16:54.483405 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b53ee423-9f0d-43f1-98f6-2de6be3cb7b9, command-id=c3f2c7a8-a4d5-4504-bbdb-93310b59b930) - Closing
[0m23:16:54.498657 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:16:54.499639 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:16:54.508638 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:16:54.509641 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:16:54.509641 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:16:54.724707 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=26fc1328-08c6-413d-90c2-6da4b72c8f99) - Created
[0m23:16:55.259951 [debug] [ThreadPool]: SQL status: OK in 0.750 seconds
[0m23:16:55.265441 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=26fc1328-08c6-413d-90c2-6da4b72c8f99, command-id=637029d1-2a12-4e3a-8f4c-183604e9ca37) - Closing
[0m23:16:55.267794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000120302F88F0>]}
[0m23:16:55.270792 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:16:55.270792 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:16:55.271791 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:16:55.271791 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:16:55.272791 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:16:55.282792 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:16:55.283790 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:16:55.309497 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:16:55.317614 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:16:55.317614 [debug] [Thread-1 (]: Safe create: False
[0m23:16:55.332016 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:16:55.333035 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:55.333035 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    min_salary,
    max_salary,
    job_posted_date,
    scraped_dts, 
    ingest_dts,
    _rescued_data,
    is_enriched,
    job_source,
    is_active
FROM `demo`.`demo_schema`.`stg_cleaned_test`


    WHERE ingest_dts > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_unified_test`
    )
    
[0m23:16:55.333035 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:16:55.525144 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b) - Created
[0m23:16:55.829285 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m23:16:55.831289 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=9cd48700-cd34-4340-be00-5c818380b146) - Closing
[0m23:16:55.832627 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:16:55.846329 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:55.846329 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:16:56.063766 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:16:56.065222 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=e2905f08-afd0-48d6-bb4f-a87f42b575ce) - Closing
[0m23:16:56.069265 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:56.069265 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:16:56.289724 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:16:56.294728 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=3a82b65e-d60b-4db1-a6d8-b23ea3250187) - Closing
[0m23:16:56.303334 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:56.304325 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:16:56.571718 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:16:56.576678 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=9f278025-7871-414e-9413-0a7ba7a65164) - Closing
[0m23:16:56.584206 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:56.585204 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:16:57.196573 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m23:16:57.199575 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=4253873e-a187-40bf-90b1-98c31646f118) - Closing
[0m23:16:57.205593 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:57.206678 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:16:57.934577 [debug] [Thread-1 (]: SQL status: OK in 0.730 seconds
[0m23:16:57.936659 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=02def055-cb0b-4307-9bb5-92409fc8f9b9) - Closing
[0m23:16:57.943838 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:57.944834 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:16:58.144963 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:16:58.148573 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=69f6678a-3436-484d-9c49-ae9d8d424187) - Closing
[0m23:16:58.155089 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:58.155089 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:16:58.422611 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:16:58.427213 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=ca949e9f-8b2b-4849-8afb-173b8632af4d) - Closing
[0m23:16:58.432313 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:58.432313 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:16:58.770709 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m23:16:58.773772 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=02379dc9-c2d6-4c11-9ac2-d08f8634b96b) - Closing
[0m23:16:58.774805 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:16:58.780784 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:16:58.807909 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:58.807909 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:16:59.074251 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:16:59.077626 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=7c53c59b-1c39-4939-91ad-eba2c46a1804) - Closing
[0m23:16:59.080745 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:59.081685 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:16:59.304118 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:16:59.309108 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=7e9770bc-e49a-44a5-a99e-9aae8ca5c971) - Closing
[0m23:16:59.318164 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:16:59.319113 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:16:59.320104 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:17:02.466472 [debug] [Thread-1 (]: SQL status: OK in 3.150 seconds
[0m23:17:02.466472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b, command-id=817da8db-2536-410f-8156-9c5f9f856ee4) - Closing
[0m23:17:02.480306 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1505a11a-ea09-4828-afd8-033566d88828', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000120318D33B0>]}
[0m23:17:02.480306 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 7.21s]
[0m23:17:02.481306 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:17:02.482307 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.214512586593628s, language=None, compute-name=) - Reusing connection previously named master
[0m23:17:02.482307 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:17:02.482307 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:17:02.483320 [debug] [MainThread]: On list_demo: Close
[0m23:17:02.483320 [debug] [MainThread]: Databricks adapter: Connection(session-id=b53ee423-9f0d-43f1-98f6-2de6be3cb7b9) - Closing
[0m23:17:02.567003 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:17:02.567003 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:17:02.567003 [debug] [MainThread]: Databricks adapter: Connection(session-id=26fc1328-08c6-413d-90c2-6da4b72c8f99) - Closing
[0m23:17:02.634330 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:17:02.635331 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:17:02.635331 [debug] [MainThread]: Databricks adapter: Connection(session-id=ee2c0580-b7c9-419c-a456-5cb23ae3b35b) - Closing
[0m23:17:02.839965 [info ] [MainThread]: 
[0m23:17:02.840964 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.73 seconds (8.73s).
[0m23:17:02.842989 [debug] [MainThread]: Command end result
[0m23:17:02.865646 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:17:02.867647 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:17:02.871649 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:17:02.871649 [info ] [MainThread]: 
[0m23:17:02.872713 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:17:02.872713 [info ] [MainThread]: 
[0m23:17:02.873845 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:17:02.873845 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:17:02.875859 [debug] [MainThread]: Command `dbt run` succeeded at 23:17:02.874858 after 10.80 seconds
[0m23:17:02.875859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001202FEC76B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200C394230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001200DFE2F60>]}
[0m23:17:02.875859 [debug] [MainThread]: Flushing usage events
[0m23:17:03.877634 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:17:45.826550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252BFCEBC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252C02D10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252C02D2BD0>]}


============================== 23:17:45.830695 | 802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8 ==============================
[0m23:17:45.830695 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:17:45.830695 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run -selectt stg_unified_test', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m23:17:46.558554 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:17:46.559563 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:17:46.559563 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:17:47.206145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E3933770>]}
[0m23:17:47.267368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252C02086B0>]}
[0m23:17:47.268368 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:17:47.493398 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:17:47.562417 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:17:47.563395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E3E61B20>]}
[0m23:17:47.610238 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:17:47.610238 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\stg_unified_test.sql
[0m23:17:47.826460 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:17:47.839495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E51C3C20>]}
[0m23:17:47.890476 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:17:47.891460 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:17:47.924792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E5148410>]}
[0m23:17:47.925989 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:17:47.925989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E5162CC0>]}
[0m23:17:47.926971 [warn ] [MainThread]: The selection criterion 'electt' does not match any enabled nodes
[0m23:17:47.927968 [info ] [MainThread]: 
[0m23:17:47.927968 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:17:47.928963 [info ] [MainThread]: 
[0m23:17:47.928963 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:17:47.928963 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:17:47.929970 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:17:47.929970 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:17:47.929970 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:17:47.931148 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:17:47.931148 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:17:48.204619 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=70d468d8-607b-4233-94d2-dc138d26a8e1) - Created
[0m23:17:48.348823 [debug] [ThreadPool]: SQL status: OK in 0.420 seconds
[0m23:17:48.350808 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=70d468d8-607b-4233-94d2-dc138d26a8e1, command-id=f0353e48-64fb-427c-90cd-da8db3c1dfa3) - Closing
[0m23:17:48.359827 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:17:48.359827 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:17:48.368278 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:17:48.368278 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:17:48.368278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:17:48.565537 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=43b144aa-bc78-4ef8-9dee-2fb214dc5409) - Created
[0m23:17:48.918427 [debug] [ThreadPool]: SQL status: OK in 0.550 seconds
[0m23:17:48.922448 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=43b144aa-bc78-4ef8-9dee-2fb214dc5409, command-id=d23dfa38-c947-4a96-8911-60d330669a8a) - Closing
[0m23:17:48.924447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252BD9E71A0>]}
[0m23:17:48.927448 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:17:48.927448 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:17:48.928641 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:17:48.928641 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:17:48.929641 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:17:48.936941 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:17:48.937939 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:17:48.963491 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:17:48.970492 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:17:48.970492 [debug] [Thread-1 (]: Safe create: False
[0m23:17:48.985506 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:17:48.986502 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:48.986502 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m23:17:48.986502 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:17:49.220597 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c) - Created
[0m23:17:49.740657 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m23:17:49.742658 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=dc5f8c9f-a980-48e3-9e4d-5bd3a2e763d0) - Closing
[0m23:17:49.742658 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:17:49.754584 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:49.754584 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:17:50.320154 [debug] [Thread-1 (]: SQL status: OK in 0.560 seconds
[0m23:17:50.324950 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=71d99650-5590-413c-a713-e7571f52747b) - Closing
[0m23:17:50.331259 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:50.332266 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:17:50.595532 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:17:50.597609 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=8cf7a72b-ecf6-4511-99e0-680896bd949e) - Closing
[0m23:17:50.602968 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:50.602968 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:17:50.886047 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:17:50.889194 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=03e5b746-404d-42c4-b7d3-f06e1e279e8a) - Closing
[0m23:17:50.895181 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:50.895181 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:17:51.443722 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m23:17:51.447333 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=c792b3ca-186f-46e4-b04f-3e7616125c47) - Closing
[0m23:17:51.457648 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:51.458689 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:17:52.185019 [debug] [Thread-1 (]: SQL status: OK in 0.720 seconds
[0m23:17:52.190100 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=09920921-93dd-4073-a76c-666525e8d282) - Closing
[0m23:17:52.195120 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:52.195120 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:17:52.438857 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:17:52.441857 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=e6475180-d1b6-45f9-8445-4a2d3c404f43) - Closing
[0m23:17:52.444855 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:52.446359 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:17:52.727109 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:17:52.732449 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=05cb2aae-0003-407d-a6e9-17aa42532170) - Closing
[0m23:17:52.741521 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:52.742517 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:17:53.138955 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m23:17:53.142985 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=a3253e4d-91f6-470c-817d-85885093758a) - Closing
[0m23:17:53.143983 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:17:53.152916 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:17:53.173970 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:53.173970 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:17:53.400812 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m23:17:53.404358 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=77648eb6-12d8-4c35-8b96-a269ba6f63ce) - Closing
[0m23:17:53.406355 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:53.406355 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:17:53.607854 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:17:53.611493 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c, command-id=92328eb5-6fcb-45de-8a9c-dded77b76e40) - Closing
[0m23:17:53.620008 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:17:53.620008 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:17:53.620949 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:17:57.449704 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419020 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6729 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d627d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b90 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a584 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef602 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=57da8533-2776-4f65-baaa-29d96ee32c85
[0m23:17:57.456703 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:17:57.457711 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '802dfbe2-d311-4c7a-bfc8-4a607ea0a2e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E522E1E0>]}
[0m23:17:57.458704 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 8.53s]
[0m23:17:57.458704 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:17:57.459702 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m23:17:57.460704 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=8.53525710105896s, language=None, compute-name=) - Reusing connection previously named master
[0m23:17:57.460704 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:17:57.460704 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:17:57.460704 [debug] [MainThread]: On list_demo: Close
[0m23:17:57.461863 [debug] [MainThread]: Databricks adapter: Connection(session-id=70d468d8-607b-4233-94d2-dc138d26a8e1) - Closing
[0m23:17:57.543880 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:17:57.543880 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:17:57.544914 [debug] [MainThread]: Databricks adapter: Connection(session-id=43b144aa-bc78-4ef8-9dee-2fb214dc5409) - Closing
[0m23:17:57.620550 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:17:57.621539 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:17:57.621539 [debug] [MainThread]: Databricks adapter: Connection(session-id=d9b6a9e0-707b-4144-8844-9082f0416c9c) - Closing
[0m23:17:57.973146 [info ] [MainThread]: 
[0m23:17:57.974186 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.04 seconds (10.04s).
[0m23:17:57.976247 [debug] [MainThread]: Command end result
[0m23:17:57.999471 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:17:58.001483 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:17:58.005673 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:17:58.005673 [info ] [MainThread]: 
[0m23:17:58.007026 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:17:58.007026 [info ] [MainThread]: 
[0m23:17:58.007026 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m23:17:58.008037 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:17:58.008037 [info ] [MainThread]: 
[0m23:17:58.009035 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m23:17:58.009035 [info ] [MainThread]: 
[0m23:17:58.010037 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:17:58.010037 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:17:58.011037 [debug] [MainThread]: Command `dbt run` failed at 23:17:58.011037 after 12.30 seconds
[0m23:17:58.011037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E3B08E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E3A2FE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000252E3A2EC30>]}
[0m23:17:58.011037 [debug] [MainThread]: Flushing usage events
[0m23:17:58.972397 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:18:16.358984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0F9338620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0F90DEA80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0F90DF9B0>]}


============================== 23:18:16.363517 | 529d4294-775b-4740-a352-4cd5208c6f36 ==============================
[0m23:18:16.363517 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:18:16.364516 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run -select stg_unified_test', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m23:18:17.096960 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:18:17.097959 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:18:17.097959 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:18:17.737351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0F65F4F50>]}
[0m23:18:17.795065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09C3AEC90>]}
[0m23:18:17.796065 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:18:18.029441 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:18:18.098378 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:18:18.099349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09DD1F350>]}
[0m23:18:18.146373 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:18:18.146373 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:18:18.151929 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:18:18.170965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09C9C9670>]}
[0m23:18:18.221763 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:18:18.223772 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:18:18.247589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09DCB4FE0>]}
[0m23:18:18.247589 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:18:18.248745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09DE5E900>]}
[0m23:18:18.248745 [warn ] [MainThread]: The selection criterion 'elect' does not match any enabled nodes
[0m23:18:18.249796 [info ] [MainThread]: 
[0m23:18:18.249796 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:18:18.251302 [info ] [MainThread]: 
[0m23:18:18.251302 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:18:18.251302 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:18:18.252824 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:18:18.252824 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:18:18.252824 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:18:18.252824 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:18:18.253834 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:18.671152 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=5442b8cd-3591-439a-b297-e9741a6eea03) - Created
[0m23:18:18.914396 [debug] [ThreadPool]: SQL status: OK in 0.660 seconds
[0m23:18:18.916391 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5442b8cd-3591-439a-b297-e9741a6eea03, command-id=196ddbb3-4043-470e-8fdc-3ad7efb0c267) - Closing
[0m23:18:18.924803 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:18:18.925844 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:18:18.935322 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:18:18.935322 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:18:18.935322 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:18:19.312506 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=96185e30-9940-4408-9b0f-030751e1e7a0) - Created
[0m23:18:19.781386 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m23:18:19.787397 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=96185e30-9940-4408-9b0f-030751e1e7a0, command-id=57a4aabd-d9ec-4071-8857-071955c65040) - Closing
[0m23:18:19.789408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0FC11DA00>]}
[0m23:18:19.792502 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:18:19.792502 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:18:19.793502 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:18:19.793502 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:18:19.793502 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:18:19.803511 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:18:19.804508 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:18:19.830819 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:18:19.837937 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:18:19.837937 [debug] [Thread-1 (]: Safe create: False
[0m23:18:19.852057 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:18:19.852057 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:19.853181 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m23:18:19.853181 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:18:20.198967 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6) - Created
[0m23:18:20.531125 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m23:18:20.532116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=5210f281-982d-4d7c-8338-423a11adf413) - Closing
[0m23:18:20.532116 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:18:20.541381 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:20.541381 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:18:20.793853 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:18:20.796110 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=b3a4b994-6ff9-4ee3-bce3-b1c37c0241a8) - Closing
[0m23:18:20.801245 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:20.801245 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:18:21.078291 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:18:21.080309 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=03a8ec56-24f2-4e3e-a8b6-44bb62c1337e) - Closing
[0m23:18:21.085298 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:21.085298 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:18:21.359459 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:18:21.364214 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=5887998b-4cf8-4dc1-b218-d6f5bdac1906) - Closing
[0m23:18:21.375205 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:21.375205 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:18:21.875772 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m23:18:21.880953 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=fc1c3488-a6f6-4e80-a4cb-b87c75812046) - Closing
[0m23:18:21.887041 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:21.888049 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:18:22.503480 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m23:18:22.505461 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=a2bf0af0-3ff6-42d6-8d09-0b319dd7a6f8) - Closing
[0m23:18:22.509477 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:22.510885 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:18:22.820801 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m23:18:22.821899 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=d2cea6fb-965c-4a66-a86d-151b887e86c3) - Closing
[0m23:18:22.826323 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:22.826323 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:18:23.122609 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m23:18:23.128154 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=791c4476-9b3b-41e6-afe1-e689c57ffd62) - Closing
[0m23:18:23.134137 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:23.135139 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:18:23.565858 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m23:18:23.567781 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=0ffb296a-0539-476b-84e6-1e7a08ae88ee) - Closing
[0m23:18:23.568802 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:18:23.575104 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:18:23.597802 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:23.597802 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:18:23.890707 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:18:23.893146 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=f7c916df-c924-482a-a9b9-b16618e46677) - Closing
[0m23:18:23.894147 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:23.895147 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:18:24.193282 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m23:18:24.196033 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6, command-id=fcb5ed92-1f1d-4530-be5d-755d9626c590) - Closing
[0m23:18:24.203866 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:18:24.205800 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:18:24.206871 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:18:29.122087 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419020 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6729 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d627d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b90 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a584 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef602 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=4846886d-f3af-4030-9a7e-1ad2ab082e9f
[0m23:18:29.128609 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:18:29.131134 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '529d4294-775b-4740-a352-4cd5208c6f36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09DF647A0>]}
[0m23:18:29.132119 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 9.34s]
[0m23:18:29.132119 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:18:29.133469 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m23:18:29.134622 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=9.344213485717773s, language=None, compute-name=) - Reusing connection previously named master
[0m23:18:29.134622 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:18:29.135779 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:18:29.135779 [debug] [MainThread]: On list_demo: Close
[0m23:18:29.135779 [debug] [MainThread]: Databricks adapter: Connection(session-id=5442b8cd-3591-439a-b297-e9741a6eea03) - Closing
[0m23:18:29.301669 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:18:29.302675 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:18:29.303675 [debug] [MainThread]: Databricks adapter: Connection(session-id=96185e30-9940-4408-9b0f-030751e1e7a0) - Closing
[0m23:18:29.480221 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:18:29.480221 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:18:29.480221 [debug] [MainThread]: Databricks adapter: Connection(session-id=64cccdc5-455b-4b9c-8e36-09d2848629d6) - Closing
[0m23:18:29.762484 [info ] [MainThread]: 
[0m23:18:29.763905 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 11.51 seconds (11.51s).
[0m23:18:29.768154 [debug] [MainThread]: Command end result
[0m23:18:29.846953 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:18:29.849917 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:18:29.854921 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:18:29.854921 [info ] [MainThread]: 
[0m23:18:29.854921 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:18:29.855921 [info ] [MainThread]: 
[0m23:18:29.855921 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m23:18:29.855921 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:18:29.856933 [info ] [MainThread]: 
[0m23:18:29.856933 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m23:18:29.856933 [info ] [MainThread]: 
[0m23:18:29.857938 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:18:29.858928 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:18:29.859929 [debug] [MainThread]: Command `dbt run` failed at 23:18:29.859929 after 13.62 seconds
[0m23:18:29.859929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09C8BACF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A09C9C9C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0F9112AB0>]}
[0m23:18:29.861404 [debug] [MainThread]: Flushing usage events
[0m23:18:31.062580 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:21:53.109872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D266038D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D26603AFC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D265652180>]}


============================== 23:21:53.113361 | d58951ed-db70-4642-b65f-9ee02b029873 ==============================
[0m23:21:53.113361 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:21:53.114361 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m23:21:53.841360 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:21:53.842796 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:21:53.842796 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:21:54.501775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D265BA4350>]}
[0m23:21:54.565940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D26930B4D0>]}
[0m23:21:54.565940 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:21:54.797581 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:21:54.861467 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:21:54.861467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D20B64B350>]}
[0m23:21:54.908037 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:21:54.908037 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m23:21:55.133403 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:21:55.146549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D20B9E3EF0>]}
[0m23:21:55.200709 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:21:55.202803 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:21:55.221259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D20B964230>]}
[0m23:21:55.221259 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:21:55.222251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D20A353710>]}
[0m23:21:55.224290 [info ] [MainThread]: 
[0m23:21:55.224290 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:21:55.225283 [info ] [MainThread]: 
[0m23:21:55.225283 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:21:55.225283 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:21:55.226291 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:21:55.226291 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:21:55.226291 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:21:55.227767 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:21:55.227767 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:21:55.509155 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d8f358f4-7d1a-462f-b029-64ea8578949f) - Created
[0m23:21:55.668213 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m23:21:55.670228 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d8f358f4-7d1a-462f-b029-64ea8578949f, command-id=60f65628-0cba-4a08-a217-ea9756e5eccf) - Closing
[0m23:21:55.677307 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:21:55.678317 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:21:55.686281 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:21:55.686281 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:21:55.686281 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:21:55.913110 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=72a61446-c63c-451c-bb79-71660eb1f102) - Created
[0m23:21:56.602016 [debug] [ThreadPool]: SQL status: OK in 0.920 seconds
[0m23:21:56.616049 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=72a61446-c63c-451c-bb79-71660eb1f102, command-id=60f29c31-62c3-4e31-a5d4-2e57bfca9e03) - Closing
[0m23:21:56.621568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2639472F0>]}
[0m23:21:56.628754 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:21:56.629926 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:21:56.631045 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:21:56.631045 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:21:56.631045 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:21:56.639027 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:21:56.640026 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:21:56.668818 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:21:56.676776 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:21:56.676776 [debug] [Thread-1 (]: Safe create: False
[0m23:21:56.690274 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:21:56.690274 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:56.691537 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    cast(nullif(MIN_AMOUNT, '') AS DOUBLE) as min_salary, 
    cast(nullif(MAX_AMOUNT, '') AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:21:56.691537 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:21:56.892687 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4) - Created
[0m23:21:57.287212 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m23:21:57.290634 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=c8ab838a-0796-48f8-bb4d-0e55cfd33ec7) - Closing
[0m23:21:57.291633 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:21:57.302718 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:57.302718 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:21:57.621060 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m23:21:57.626051 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=4bb9f72b-f50a-4dc3-bdac-f71cf410a500) - Closing
[0m23:21:57.632412 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:57.632412 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:21:57.883189 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:21:57.885107 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=9aaba0e7-1338-4a71-82f8-7f64cc0f5a6e) - Closing
[0m23:21:57.891106 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:57.892106 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:21:58.169862 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:21:58.171862 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=dc113bdb-20ad-4060-8541-22c9cfe4617e) - Closing
[0m23:21:58.178919 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:58.179892 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:21:58.667932 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m23:21:58.673945 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=6a354392-d53b-462b-ad37-3a900f68587c) - Closing
[0m23:21:58.678594 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:58.678594 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:21:59.362827 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m23:21:59.366615 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=6d85a6d5-102e-4634-b601-5adc861660d7) - Closing
[0m23:21:59.374595 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:59.375639 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:21:59.573412 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:21:59.576808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=71527529-e05c-4018-8e71-0bf3a271090b) - Closing
[0m23:21:59.581171 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:59.581171 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:21:59.846342 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:21:59.849341 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=2386f79e-624c-41f7-ae25-53b603a4cc86) - Closing
[0m23:21:59.853340 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:21:59.853340 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:22:00.251347 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m23:22:00.255334 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=cf046bbe-93c7-4e7b-a66d-76f5c0ad1a8e) - Closing
[0m23:22:00.256407 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:22:00.263360 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:22:00.290353 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:22:00.291350 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:22:00.514414 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:22:00.519402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=a0170a25-2590-4edd-af32-761b80fc1848) - Closing
[0m23:22:00.521907 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:22:00.521907 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:22:00.719703 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:22:00.726814 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=4e6b7e0e-48af-484d-9aa0-af76784d6010) - Closing
[0m23:22:00.741179 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:22:00.742185 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:22:00.743152 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:22:03.544765 [debug] [Thread-1 (]: SQL status: OK in 2.800 seconds
[0m23:22:03.547799 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4, command-id=47fbd5fb-a3e4-4833-bea6-e9a12f9a0e83) - Closing
[0m23:22:03.565098 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd58951ed-db70-4642-b65f-9ee02b029873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D20B76B8C0>]}
[0m23:22:03.567080 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.93s]
[0m23:22:03.570161 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:22:03.573268 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.9506988525390625s, language=None, compute-name=) - Reusing connection previously named master
[0m23:22:03.574251 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:22:03.575265 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:22:03.576249 [debug] [MainThread]: On list_demo: Close
[0m23:22:03.576249 [debug] [MainThread]: Databricks adapter: Connection(session-id=d8f358f4-7d1a-462f-b029-64ea8578949f) - Closing
[0m23:22:03.644671 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:22:03.645766 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:22:03.646752 [debug] [MainThread]: Databricks adapter: Connection(session-id=72a61446-c63c-451c-bb79-71660eb1f102) - Closing
[0m23:22:03.736008 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:22:03.737001 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:22:03.738002 [debug] [MainThread]: Databricks adapter: Connection(session-id=e2c0c808-cb47-4c8e-b06a-e612f8e896b4) - Closing
[0m23:22:03.991689 [info ] [MainThread]: 
[0m23:22:03.993622 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.77 seconds (8.77s).
[0m23:22:03.997128 [debug] [MainThread]: Command end result
[0m23:22:04.023276 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:22:04.026217 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:22:04.029592 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:22:04.030598 [info ] [MainThread]: 
[0m23:22:04.030598 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:22:04.031602 [info ] [MainThread]: 
[0m23:22:04.031602 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:22:04.032620 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:22:04.033665 [debug] [MainThread]: Command `dbt run` succeeded at 23:22:04.032620 after 11.03 seconds
[0m23:22:04.033665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2665B69C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2664DE480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D2664DE630>]}
[0m23:22:04.033665 [debug] [MainThread]: Flushing usage events
[0m23:22:05.058357 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:22:12.614760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B0C45580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B1561D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B12712E0>]}


============================== 23:22:12.618751 | 71b57018-1d4b-4322-a8b9-0dcb0d1712c9 ==============================
[0m23:22:12.618751 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:22:12.619740 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run -select stg_unified_test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m23:22:13.353128 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:22:13.353128 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:22:13.353128 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:22:14.000095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B0C459D0>]}
[0m23:22:14.056458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B4309430>]}
[0m23:22:14.057456 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:22:14.289251 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:22:14.361105 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:22:14.362041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D4DEE570>]}
[0m23:22:14.408115 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:22:14.408115 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:22:14.413106 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:22:14.433163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D4C69280>]}
[0m23:22:14.482325 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:22:14.484573 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:22:14.513365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D4D82F00>]}
[0m23:22:14.513365 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:22:14.514372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D5EBAF30>]}
[0m23:22:14.514372 [warn ] [MainThread]: The selection criterion 'elect' does not match any enabled nodes
[0m23:22:14.515402 [info ] [MainThread]: 
[0m23:22:14.516533 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:22:14.516533 [info ] [MainThread]: 
[0m23:22:14.516533 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:22:14.517533 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:22:14.518573 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:22:14.518573 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:22:14.518573 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:22:14.518573 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:22:14.519563 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:22:14.771799 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=8cf44aac-d3b0-4e76-b62c-ec8e8c5355ad) - Created
[0m23:22:14.907751 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m23:22:14.911812 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8cf44aac-d3b0-4e76-b62c-ec8e8c5355ad, command-id=306c5408-54ca-478d-a1a8-825c85385c76) - Closing
[0m23:22:14.922975 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:22:14.924022 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:22:14.935563 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:22:14.935563 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:22:14.936565 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:22:15.134179 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3f240e42-a862-4dff-9331-e4658da2a5fa) - Created
[0m23:22:15.548983 [debug] [ThreadPool]: SQL status: OK in 0.610 seconds
[0m23:22:15.558781 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3f240e42-a862-4dff-9331-e4658da2a5fa, command-id=7c7c0641-a7af-4086-9139-01bdcfd759cf) - Closing
[0m23:22:15.564802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B0CB1C40>]}
[0m23:22:15.570264 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:22:15.570264 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:22:15.572261 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:22:15.572261 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:22:15.572261 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:22:15.585570 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:22:15.585570 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:22:15.611485 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:22:15.617756 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:22:15.618805 [debug] [Thread-1 (]: Safe create: False
[0m23:22:15.631542 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:22:15.631542 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:15.632481 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m23:22:15.632481 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:22:15.828335 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=a0510b3a-919b-45af-8d19-552575e8edb6) - Created
[0m23:22:16.277630 [debug] [Thread-1 (]: SQL status: OK in 0.640 seconds
[0m23:22:16.277630 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=3b675702-558b-489f-962f-571c058379e1) - Closing
[0m23:22:16.278710 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:22:16.286678 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:16.286678 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:22:16.490866 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:22:16.494423 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=36337e6c-52e9-4610-b886-c653d35765e2) - Closing
[0m23:22:16.505625 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:16.505625 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:22:16.730143 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:22:16.735073 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=17f09896-3d56-422a-a057-414ed9cef62a) - Closing
[0m23:22:16.741519 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:16.741519 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:22:16.995557 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:22:16.998548 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=446277bd-3956-415e-abac-89ec8f8e84e0) - Closing
[0m23:22:17.006543 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:17.006543 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:22:17.405008 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m23:22:17.407080 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=3853565f-0a1d-456c-902e-fbbcc8dcf00c) - Closing
[0m23:22:17.412634 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:17.419289 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:22:18.035143 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m23:22:18.038608 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=32730e32-4a7e-4a81-aae8-952400b2460b) - Closing
[0m23:22:18.049030 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:18.050033 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:22:18.289478 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:22:18.292477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=33d14d22-58a8-4259-a3dd-1975735fc71f) - Closing
[0m23:22:18.295472 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:18.296444 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:22:18.603160 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m23:22:18.607213 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=c2298cb9-2f53-4f51-970c-ef3161a7f671) - Closing
[0m23:22:18.615231 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:18.615231 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:22:19.067305 [debug] [Thread-1 (]: SQL status: OK in 0.450 seconds
[0m23:22:19.069316 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=603088bc-d347-4c96-bf3e-0eea2bd5ac68) - Closing
[0m23:22:19.070242 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:22:19.076767 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:22:19.099837 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:19.099837 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:22:19.324173 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:22:19.328983 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=79af15b6-b7b4-4526-bc03-7773c490f6cd) - Closing
[0m23:22:19.332006 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:19.332006 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:22:19.541767 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:22:19.546179 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a0510b3a-919b-45af-8d19-552575e8edb6, command-id=2187ab1b-87a9-4085-a0d0-dc4f410b53f7) - Closing
[0m23:22:19.554413 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:22:19.555414 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:22:19.555414 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:22:23.133557 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419020 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6729 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d627d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b90 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a584 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef602 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=6c467d9c-418c-418d-827d-b7b2812d0a24
[0m23:22:23.139626 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:22:23.142652 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '71b57018-1d4b-4322-a8b9-0dcb0d1712c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D60036B0>]}
[0m23:22:23.142652 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 7.57s]
[0m23:22:23.143626 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:22:23.144627 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m23:22:23.145641 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.580838918685913s, language=None, compute-name=) - Reusing connection previously named master
[0m23:22:23.145641 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:22:23.146642 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:22:23.146642 [debug] [MainThread]: On list_demo: Close
[0m23:22:23.146642 [debug] [MainThread]: Databricks adapter: Connection(session-id=8cf44aac-d3b0-4e76-b62c-ec8e8c5355ad) - Closing
[0m23:22:23.204998 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:22:23.206032 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:22:23.206032 [debug] [MainThread]: Databricks adapter: Connection(session-id=3f240e42-a862-4dff-9331-e4658da2a5fa) - Closing
[0m23:22:23.263336 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:22:23.263336 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:22:23.263336 [debug] [MainThread]: Databricks adapter: Connection(session-id=a0510b3a-919b-45af-8d19-552575e8edb6) - Closing
[0m23:22:23.467010 [info ] [MainThread]: 
[0m23:22:23.467010 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.95 seconds (8.95s).
[0m23:22:23.468507 [debug] [MainThread]: Command end result
[0m23:22:23.487766 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:22:23.489765 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:22:23.494763 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:22:23.494763 [info ] [MainThread]: 
[0m23:22:23.494763 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:22:23.496075 [info ] [MainThread]: 
[0m23:22:23.496075 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m23:22:23.497087 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:22:23.497087 [info ] [MainThread]: 
[0m23:22:23.497087 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m23:22:23.498086 [info ] [MainThread]: 
[0m23:22:23.498086 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:22:23.499087 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:22:23.499087 [debug] [MainThread]: Command `dbt run` failed at 23:22:23.499087 after 11.00 seconds
[0m23:22:23.500090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207B151E9C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D4393710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000207D4A6D850>]}
[0m23:22:23.500090 [debug] [MainThread]: Flushing usage events
[0m23:22:24.459622 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:24:04.567671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027687FB1400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276882A7560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002768854DEB0>]}


============================== 23:24:04.572504 | 13eb691f-1091-4bdf-ab4e-1470361fe89e ==============================
[0m23:24:04.572504 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:24:04.573454 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m23:24:05.299867 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:24:05.300884 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:24:05.300884 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:24:05.961332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027685674E60>]}
[0m23:24:06.019906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276882A7560>]}
[0m23:24:06.020994 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:24:06.262318 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:24:06.321931 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:24:06.321931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276AB8F7CE0>]}
[0m23:24:06.370102 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:24:06.370617 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:24:06.375123 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:24:06.396267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276ACDA4B00>]}
[0m23:24:06.447231 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:24:06.449816 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:24:06.472317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276ACD9FE90>]}
[0m23:24:06.472317 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:24:06.472317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276ACDB0DA0>]}
[0m23:24:06.473355 [info ] [MainThread]: 
[0m23:24:06.473355 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:24:06.473355 [info ] [MainThread]: 
[0m23:24:06.474874 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:24:06.474874 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:24:06.475955 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:24:06.475955 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:24:06.475955 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:24:06.476901 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:24:06.476901 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:24:06.754140 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3602ae72-3491-4549-be78-22fb2542ccc3) - Created
[0m23:24:07.042859 [debug] [ThreadPool]: SQL status: OK in 0.570 seconds
[0m23:24:07.045865 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3602ae72-3491-4549-be78-22fb2542ccc3, command-id=64c97d6b-8d05-4fef-a05e-967532ad3433) - Closing
[0m23:24:07.057320 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:24:07.058444 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:24:07.072120 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:24:07.073120 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:24:07.073120 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:24:07.284345 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d5891568-fc0c-4df6-bfa9-8a1affac15ca) - Created
[0m23:24:07.676690 [debug] [ThreadPool]: SQL status: OK in 0.600 seconds
[0m23:24:07.681207 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d5891568-fc0c-4df6-bfa9-8a1affac15ca, command-id=2f756eb4-c90d-4fc4-9f07-ef2a0d93e0ae) - Closing
[0m23:24:07.684024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276ABCCA2A0>]}
[0m23:24:07.686062 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:24:07.687097 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:24:07.687097 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:24:07.688101 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:24:07.688101 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:24:07.698623 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:24:07.698623 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:24:07.725961 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:24:07.732563 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:24:07.732563 [debug] [Thread-1 (]: Safe create: False
[0m23:24:07.745382 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:24:07.745382 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:07.746377 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    cast(nullif(MIN_AMOUNT, '') AS DOUBLE) as min_salary, 
    cast(nullif(MAX_AMOUNT, '') AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:24:07.746377 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:24:07.988699 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514) - Created
[0m23:24:08.362196 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m23:24:08.363183 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=9dbde8bf-8f96-4f09-98f8-5c17ed7eb426) - Closing
[0m23:24:08.363183 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:24:08.372120 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:08.373342 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:24:08.593925 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:24:08.598974 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=90223a20-4417-4458-ab35-81dcf3dc4c0f) - Closing
[0m23:24:08.602585 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:08.602585 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:24:08.826485 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:24:08.827430 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=0b1f4159-770c-4041-bced-def65d908699) - Closing
[0m23:24:08.833116 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:08.833116 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:24:09.089037 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:24:09.094047 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=a83dea35-e227-424d-a9f8-807565450b3c) - Closing
[0m23:24:09.106977 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:09.106977 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:24:09.609684 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m23:24:09.614051 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=bbf7f17e-04f8-41c3-bdc7-60fb360413f4) - Closing
[0m23:24:09.621148 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:09.622102 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:24:10.277768 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m23:24:10.280808 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=35a44e95-c8d7-492f-8403-624d4bdb3234) - Closing
[0m23:24:10.287175 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:10.287175 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:24:10.526948 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:24:10.530192 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=fc86cefc-2a50-4384-9683-928f8f96e3f7) - Closing
[0m23:24:10.536381 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:10.536381 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:24:10.808417 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:24:10.812424 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=65bf52b6-a1e9-4661-b3dc-fd45e7e1da42) - Closing
[0m23:24:10.818350 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:10.818350 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:24:11.139528 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m23:24:11.146518 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=e5d751d9-05c3-4647-b4a0-2aac26b79206) - Closing
[0m23:24:11.150516 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:24:11.165476 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:24:11.186466 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:11.186466 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:24:11.409658 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:24:11.413373 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=7d32f6ec-1006-4fe2-b99f-d40baf34a371) - Closing
[0m23:24:11.416400 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:11.416400 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:24:11.629989 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:24:11.632018 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=e60d3382-f047-4109-86c3-18ef8ff87396) - Closing
[0m23:24:11.639083 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:24:11.640138 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:24:11.641408 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:24:14.420260 [debug] [Thread-1 (]: SQL status: OK in 2.780 seconds
[0m23:24:14.422263 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514, command-id=b5476b7f-d207-41a4-900e-a9404e400bd5) - Closing
[0m23:24:14.445854 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13eb691f-1091-4bdf-ab4e-1470361fe89e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276ACF2FB30>]}
[0m23:24:14.447929 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.76s]
[0m23:24:14.449889 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:24:14.453392 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.769367456436157s, language=None, compute-name=) - Reusing connection previously named master
[0m23:24:14.455384 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:24:14.456381 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:24:14.456381 [debug] [MainThread]: On list_demo: Close
[0m23:24:14.457374 [debug] [MainThread]: Databricks adapter: Connection(session-id=3602ae72-3491-4549-be78-22fb2542ccc3) - Closing
[0m23:24:14.538022 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:24:14.540032 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:24:14.540032 [debug] [MainThread]: Databricks adapter: Connection(session-id=d5891568-fc0c-4df6-bfa9-8a1affac15ca) - Closing
[0m23:24:14.598381 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:24:14.599504 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:24:14.602035 [debug] [MainThread]: Databricks adapter: Connection(session-id=4b66d0ef-bd7c-45df-b376-55f60c711514) - Closing
[0m23:24:14.864128 [info ] [MainThread]: 
[0m23:24:14.866128 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.39 seconds (8.39s).
[0m23:24:14.870125 [debug] [MainThread]: Command end result
[0m23:24:14.896643 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:24:14.897665 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:24:14.902119 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:24:14.902119 [info ] [MainThread]: 
[0m23:24:14.903066 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:24:14.904252 [info ] [MainThread]: 
[0m23:24:14.904252 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:24:14.905261 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:24:14.906731 [debug] [MainThread]: Command `dbt run` succeeded at 23:24:14.906731 after 10.46 seconds
[0m23:24:14.907739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276879CE330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002768854DEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276AAB88470>]}
[0m23:24:14.907739 [debug] [MainThread]: Flushing usage events
[0m23:24:15.931234 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:25:44.661417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5531E2F90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5531E34D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5531E0FE0>]}


============================== 23:25:44.665419 | bffa654f-7a43-466c-b5f4-a1c49dafc261 ==============================
[0m23:25:44.665419 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:25:44.666429 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_cleaned', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m23:25:45.377065 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:25:45.377065 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:25:45.378334 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:25:46.011595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5765D11F0>]}
[0m23:25:46.073640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5763AF3B0>]}
[0m23:25:46.074197 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:25:46.295723 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:25:46.373252 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:25:46.373252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B575F52000>]}
[0m23:25:46.420248 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:25:46.420248 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m23:25:46.636805 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:25:46.646922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5780C3B90>]}
[0m23:25:46.695768 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:25:46.697761 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:25:46.733190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5780440E0>]}
[0m23:25:46.733190 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:25:46.734654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B576C97920>]}
[0m23:25:46.735670 [info ] [MainThread]: 
[0m23:25:46.736667 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:25:46.736667 [info ] [MainThread]: 
[0m23:25:46.737660 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:25:46.737660 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:25:46.738653 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:25:46.738653 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:25:46.739657 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:25:46.739657 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:25:46.739657 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:25:47.041828 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=6ae5bba8-bb08-4846-aa2a-ce601fe6658b) - Created
[0m23:25:47.173754 [debug] [ThreadPool]: SQL status: OK in 0.430 seconds
[0m23:25:47.176830 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=6ae5bba8-bb08-4846-aa2a-ce601fe6658b, command-id=c6557569-e275-4249-a116-7d98b4c203b2) - Closing
[0m23:25:47.187908 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:25:47.187908 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:25:47.198186 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:25:47.198186 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:25:47.198186 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:25:47.421861 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=031d1756-92fb-462d-a5fd-7e8a6bef7316) - Created
[0m23:25:47.794839 [debug] [ThreadPool]: SQL status: OK in 0.600 seconds
[0m23:25:47.807912 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=031d1756-92fb-462d-a5fd-7e8a6bef7316, command-id=ace887c4-2e48-4b39-b469-82746ee3b7a4) - Closing
[0m23:25:47.813992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5780BF530>]}
[0m23:25:47.821596 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:25:47.822652 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:25:47.823633 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:25:47.823633 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:25:47.824590 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:25:47.831588 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:25:47.832588 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:25:47.864208 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:25:47.870741 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:25:47.871869 [debug] [Thread-1 (]: Safe create: False
[0m23:25:47.887335 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:25:47.887335 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:47.887335 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    try_cast(MIN_AMOUNT AS DOUBLE) as min_salary, 
    try_cast(MAX_AMOUNT AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:25:47.888674 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:25:48.071491 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87) - Created
[0m23:25:48.426748 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m23:25:48.428747 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=d4738d06-e444-4456-81e0-176c4d46ae66) - Closing
[0m23:25:48.428747 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:25:48.439746 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:48.439746 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:25:48.675736 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:25:48.677811 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=c2ea1b1f-a606-4121-8cc4-32956f926c06) - Closing
[0m23:25:48.682815 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:48.682815 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:25:48.888902 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:25:48.891872 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=5f617e28-856c-496a-90d0-d43c1faf9dfb) - Closing
[0m23:25:48.896863 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:48.897858 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:25:49.168843 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:25:49.170874 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=982edb16-f05a-4153-b824-25c81f66f33f) - Closing
[0m23:25:49.179878 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:49.179878 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:25:49.584531 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m23:25:49.587043 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=0bea3455-a46d-46b4-9d79-4c4f2fb80c17) - Closing
[0m23:25:49.592042 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:49.592042 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:25:50.197574 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m23:25:50.199569 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=82e98270-d171-4b1e-af5c-13ff9864afa8) - Closing
[0m23:25:50.207567 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:50.207567 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:25:50.422900 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:25:50.424903 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=8519e231-ee0a-4272-a0bf-378a94a5acb1) - Closing
[0m23:25:50.427905 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:50.427905 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:25:50.705915 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:25:50.709490 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=14c742cf-8a36-4bae-bc1d-cdefe03566f6) - Closing
[0m23:25:50.714704 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:50.714704 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:25:51.150223 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m23:25:51.152208 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=ce5bd55b-ae71-488d-a9dd-4a72a46ebd3a) - Closing
[0m23:25:51.153208 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:25:51.161292 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:25:51.185299 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:51.185299 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:25:51.474850 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:25:51.476889 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=b836ed4d-969b-495a-84fc-2066855d59e0) - Closing
[0m23:25:51.479965 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:51.479965 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:25:51.720801 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:25:51.723802 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=3a4be862-9622-477f-84b0-86b9677c8bc9) - Closing
[0m23:25:51.731802 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:25:51.734904 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:25:51.735869 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:25:54.441258 [debug] [Thread-1 (]: SQL status: OK in 2.700 seconds
[0m23:25:54.442254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87, command-id=e7ad448a-26de-4a48-9372-f0d781fcedad) - Closing
[0m23:25:54.464894 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bffa654f-7a43-466c-b5f4-a1c49dafc261', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5769E2270>]}
[0m23:25:54.465907 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.64s]
[0m23:25:54.466970 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:25:54.468889 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.6538965702056885s, language=None, compute-name=) - Reusing connection previously named master
[0m23:25:54.468889 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:25:54.470016 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:25:54.470016 [debug] [MainThread]: On list_demo: Close
[0m23:25:54.470016 [debug] [MainThread]: Databricks adapter: Connection(session-id=6ae5bba8-bb08-4846-aa2a-ce601fe6658b) - Closing
[0m23:25:54.528164 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:25:54.529158 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:25:54.529158 [debug] [MainThread]: Databricks adapter: Connection(session-id=031d1756-92fb-462d-a5fd-7e8a6bef7316) - Closing
[0m23:25:54.587102 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:25:54.588117 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:25:54.590168 [debug] [MainThread]: Databricks adapter: Connection(session-id=f78281cf-5696-4bd8-a119-1eb66e168b87) - Closing
[0m23:25:54.804348 [info ] [MainThread]: 
[0m23:25:54.804348 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.07 seconds (8.07s).
[0m23:25:54.805657 [debug] [MainThread]: Command end result
[0m23:25:54.824411 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:25:54.825893 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:25:54.831563 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:25:54.831563 [info ] [MainThread]: 
[0m23:25:54.831563 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:25:54.832661 [info ] [MainThread]: 
[0m23:25:54.832661 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:25:54.833669 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:25:54.833669 [debug] [MainThread]: Command `dbt run` succeeded at 23:25:54.833669 after 10.28 seconds
[0m23:25:54.833669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5748348F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5780C3AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B57679E450>]}
[0m23:25:54.834678 [debug] [MainThread]: Flushing usage events
[0m23:25:55.830621 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:27:53.529656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B57E440B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B55EBBEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B55836600>]}


============================== 23:27:53.533761 | 4a4ce857-6663-436c-99f5-242af6616e1c ==============================
[0m23:27:53.533761 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:27:53.533761 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run -select stg_unified_test', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m23:27:54.241662 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:27:54.242994 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:27:54.242994 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:27:54.899028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B57EDC5F0>]}
[0m23:27:54.964209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B59BAB3E0>]}
[0m23:27:54.965208 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:27:55.177008 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:27:55.252552 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:27:55.252552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5B6ED190>]}
[0m23:27:55.301690 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:27:55.301690 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:27:55.307684 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:27:55.327410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7D1F3D40>]}
[0m23:27:55.381236 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:27:55.383184 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:27:55.410638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7C0D2B70>]}
[0m23:27:55.410638 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:27:55.411638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B59B813A0>]}
[0m23:27:55.411638 [warn ] [MainThread]: The selection criterion 'elect' does not match any enabled nodes
[0m23:27:55.412652 [info ] [MainThread]: 
[0m23:27:55.413648 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:27:55.413648 [info ] [MainThread]: 
[0m23:27:55.413648 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:27:55.414914 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:27:55.414914 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:27:55.416145 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:27:55.416145 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:27:55.416145 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:27:55.416145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:27:55.610728 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a236ddc8-ca09-4fc2-bfd2-3e90cc5fc587) - Created
[0m23:27:55.742327 [debug] [ThreadPool]: SQL status: OK in 0.330 seconds
[0m23:27:55.745331 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a236ddc8-ca09-4fc2-bfd2-3e90cc5fc587, command-id=0569358b-bd5a-4314-94a4-943fcab8de04) - Closing
[0m23:27:55.757715 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:27:55.757715 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:27:55.773776 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:27:55.774783 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:27:55.774783 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:27:55.967993 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7429fa79-397a-45e4-89b4-819416730bfb) - Created
[0m23:27:56.335947 [debug] [ThreadPool]: SQL status: OK in 0.560 seconds
[0m23:27:56.344240 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7429fa79-397a-45e4-89b4-819416730bfb, command-id=9e30b8fd-9a91-44ce-9b3b-8e63ab30ec83) - Closing
[0m23:27:56.346234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7BD0B5C0>]}
[0m23:27:56.351916 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:27:56.352914 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:27:56.354856 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:27:56.354856 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:27:56.354856 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:27:56.366839 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:27:56.366839 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:27:56.398672 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:27:56.407752 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:27:56.407752 [debug] [Thread-1 (]: Safe create: False
[0m23:27:56.420756 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:27:56.420756 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:56.422262 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m23:27:56.422262 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:27:56.614773 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=36858e50-ba63-4557-9f46-c511a23383d8) - Created
[0m23:27:57.036944 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m23:27:57.040940 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=ea214dab-a22e-4a2f-98fd-a43a88aa5429) - Closing
[0m23:27:57.041937 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:27:57.052930 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:57.052930 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:27:57.275208 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:27:57.279732 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=8215ea82-e71d-48c3-a7d1-56b069836fe9) - Closing
[0m23:27:57.286988 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:57.286988 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:27:57.574835 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:27:57.576895 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=816de386-48b9-42c6-915f-bf5de094da0b) - Closing
[0m23:27:57.581416 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:57.582427 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:27:57.847609 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:27:57.850351 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=ace1bb3c-dad6-4000-b22d-a19050f39f2c) - Closing
[0m23:27:57.859796 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:57.859796 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:27:58.287092 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m23:27:58.290037 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=b4d121a5-e0a8-4239-a91d-e48cdac05f8e) - Closing
[0m23:27:58.301066 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:58.301066 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:27:59.091928 [debug] [Thread-1 (]: SQL status: OK in 0.790 seconds
[0m23:27:59.095061 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=22be2e07-faac-4038-9006-6766e1f1c990) - Closing
[0m23:27:59.102527 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:59.102527 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:27:59.298080 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m23:27:59.304081 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=ea20c373-3bce-4925-b18b-328bf37a38f8) - Closing
[0m23:27:59.313084 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:59.313084 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:27:59.554088 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:27:59.558254 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=fa29039a-be50-4b9a-ac71-53df6fa47c0e) - Closing
[0m23:27:59.564582 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:59.565603 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:27:59.875427 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m23:27:59.881428 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=891b3ba3-c470-436d-82a1-741f8ea42cfa) - Closing
[0m23:27:59.882428 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:27:59.888717 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:27:59.910508 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:27:59.910508 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:28:00.193222 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m23:28:00.198368 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=9331126d-c7fd-4031-9e0a-c143210b458f) - Closing
[0m23:28:00.201297 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:28:00.202335 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:28:00.410776 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m23:28:00.415797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=36858e50-ba63-4557-9f46-c511a23383d8, command-id=1bafad12-a3a1-454a-bf47-6b65440a5814) - Closing
[0m23:28:00.429888 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:28:00.430834 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:28:00.430834 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:28:04.046603 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419020 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6729 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d627d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b90 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a584 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef602 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=8e06f946-5ebd-4955-ac58-150e971039ef
[0m23:28:04.056878 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:28:04.058863 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4a4ce857-6663-436c-99f5-242af6616e1c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7D359130>]}
[0m23:28:04.058863 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 7.70s]
[0m23:28:04.060130 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:28:04.061170 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m23:28:04.064416 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.717141151428223s, language=None, compute-name=) - Reusing connection previously named master
[0m23:28:04.065692 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:28:04.065692 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:28:04.065692 [debug] [MainThread]: On list_demo: Close
[0m23:28:04.065692 [debug] [MainThread]: Databricks adapter: Connection(session-id=a236ddc8-ca09-4fc2-bfd2-3e90cc5fc587) - Closing
[0m23:28:04.123793 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:28:04.123793 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:28:04.124312 [debug] [MainThread]: Databricks adapter: Connection(session-id=7429fa79-397a-45e4-89b4-819416730bfb) - Closing
[0m23:28:04.180082 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:28:04.181386 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:28:04.181386 [debug] [MainThread]: Databricks adapter: Connection(session-id=36858e50-ba63-4557-9f46-c511a23383d8) - Closing
[0m23:28:04.464258 [info ] [MainThread]: 
[0m23:28:04.466367 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 9.05 seconds (9.05s).
[0m23:28:04.469712 [debug] [MainThread]: Command end result
[0m23:28:04.496834 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:28:04.498848 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:28:04.506650 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:28:04.506650 [info ] [MainThread]: 
[0m23:28:04.507655 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:28:04.507655 [info ] [MainThread]: 
[0m23:28:04.508643 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m23:28:04.508643 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:28:04.509643 [info ] [MainThread]: 
[0m23:28:04.509643 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m23:28:04.509643 [info ] [MainThread]: 
[0m23:28:04.510902 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:28:04.510902 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:28:04.510902 [debug] [MainThread]: Command `dbt run` failed at 23:28:04.510902 after 11.08 seconds
[0m23:28:04.512374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B579D4980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B579D77A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B58128800>]}
[0m23:28:04.512374 [debug] [MainThread]: Flushing usage events
[0m23:28:05.520627 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:30:42.733044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011837C57F80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011837D7B230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011834F86540>]}


============================== 23:30:42.737008 | ce411e21-812e-4adf-b491-ff4b2c9e9c90 ==============================
[0m23:30:42.737008 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:30:42.737008 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run -select stg_unified_test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m23:30:43.456225 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:30:43.456225 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:30:43.457534 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:30:44.136906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011859296BD0>]}
[0m23:30:44.193631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B1AFF80>]}
[0m23:30:44.194632 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:30:44.434736 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:30:44.493864 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:30:44.494879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B84FCB0>]}
[0m23:30:44.545136 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:30:44.546137 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:30:44.553136 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:30:44.574173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B933E00>]}
[0m23:30:44.624150 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:30:44.626140 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:30:44.653533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B7E4AA0>]}
[0m23:30:44.653533 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:30:44.653533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B857FB0>]}
[0m23:30:44.654527 [warn ] [MainThread]: The selection criterion 'elect' does not match any enabled nodes
[0m23:30:44.655526 [info ] [MainThread]: 
[0m23:30:44.655526 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:30:44.656551 [info ] [MainThread]: 
[0m23:30:44.656551 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:30:44.657706 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:30:44.657706 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:30:44.657706 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:30:44.658765 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:30:44.658765 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:30:44.658765 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:30:45.001529 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=660335ac-3a31-4b8e-8821-2f3e92e29fb7) - Created
[0m23:30:45.226751 [debug] [ThreadPool]: SQL status: OK in 0.570 seconds
[0m23:30:45.228713 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=660335ac-3a31-4b8e-8821-2f3e92e29fb7, command-id=72b3ba68-9c15-42e6-8fb1-cacdc0903b6f) - Closing
[0m23:30:45.245790 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:30:45.246749 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:30:45.258718 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:30:45.258718 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:30:45.259722 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:30:45.478176 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d77b15cb-9c41-4b16-81db-76b39b8fdf0e) - Created
[0m23:30:45.912023 [debug] [ThreadPool]: SQL status: OK in 0.650 seconds
[0m23:30:45.916996 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d77b15cb-9c41-4b16-81db-76b39b8fdf0e, command-id=592fb28a-8de3-477d-a28b-8ed73f9cd5bf) - Closing
[0m23:30:45.919024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B95B4A0>]}
[0m23:30:45.921024 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:30:45.922193 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:30:45.922193 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:30:45.923275 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:30:45.923275 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:30:45.932653 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:30:45.933679 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:30:45.957524 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:30:45.966100 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:30:45.967098 [debug] [Thread-1 (]: Safe create: False
[0m23:30:45.980307 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:30:45.981630 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:45.981630 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m23:30:45.982639 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:30:46.169090 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9) - Created
[0m23:30:46.560157 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m23:30:46.561157 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=d1192c44-b300-4c9c-aca6-a3d2e7db6f27) - Closing
[0m23:30:46.561157 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:30:46.575286 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:46.576287 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m23:30:46.831438 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:30:46.834116 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=356b888b-03fe-4de7-8b00-1768232f1e1c) - Closing
[0m23:30:46.839112 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:46.840112 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:30:47.082301 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:30:47.087333 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=47b37143-85bf-4652-80c3-00ce3692780e) - Closing
[0m23:30:47.092613 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:47.093730 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m23:30:47.379055 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:30:47.382052 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=4be60570-a1a1-45e8-997b-4fb3bb3b8830) - Closing
[0m23:30:47.388404 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:47.388404 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:30:47.815906 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m23:30:47.817927 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=4c87726e-d8a7-4b2b-93c0-b2566d0f0327) - Closing
[0m23:30:47.822926 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:47.822926 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:30:48.470649 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m23:30:48.473681 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=936f3115-8cc5-4ed0-a8f0-0e06be3fcd9b) - Closing
[0m23:30:48.486779 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:48.486779 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m23:30:48.676986 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m23:30:48.677969 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=12e58da5-2ece-495e-9996-24659efbc226) - Closing
[0m23:30:48.681840 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:48.681840 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:30:48.944131 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m23:30:48.950135 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=c117d7e4-9643-4888-80f2-f44027a184da) - Closing
[0m23:30:48.953135 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:48.954133 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m23:30:49.319633 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m23:30:49.322738 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=931f796e-fbfa-4f8a-9928-416559ccbd2b) - Closing
[0m23:30:49.325740 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m23:30:49.344167 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m23:30:49.365166 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:49.365166 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m23:30:49.617086 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m23:30:49.623017 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=4be9a815-e875-4d4c-b5a0-104dc11fff4d) - Closing
[0m23:30:49.629289 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:49.629289 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:30:49.827302 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:30:49.830402 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9, command-id=2163c2f7-39a0-402e-9948-d4acba1f3cbf) - Closing
[0m23:30:49.846166 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:30:49.847160 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:30:49.848178 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:30:53.162864 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419020 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6729 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d627d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b90 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a584 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef602 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:98)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=7e61703b-ba86-4e4d-bafe-e4fc274a757b
[0m23:30:53.168908 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:30:53.172108 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ce411e21-812e-4adf-b491-ff4b2c9e9c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185CEDC320>]}
[0m23:30:53.174126 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 7.25s]
[0m23:30:53.175146 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:30:53.177602 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m23:30:53.179611 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.259593963623047s, language=None, compute-name=) - Reusing connection previously named master
[0m23:30:53.179611 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:30:53.181113 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:30:53.181113 [debug] [MainThread]: On list_demo: Close
[0m23:30:53.181113 [debug] [MainThread]: Databricks adapter: Connection(session-id=660335ac-3a31-4b8e-8821-2f3e92e29fb7) - Closing
[0m23:30:53.240273 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:30:53.240273 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:30:53.240273 [debug] [MainThread]: Databricks adapter: Connection(session-id=d77b15cb-9c41-4b16-81db-76b39b8fdf0e) - Closing
[0m23:30:53.299182 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:30:53.299182 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:30:53.300529 [debug] [MainThread]: Databricks adapter: Connection(session-id=2dd2f1a9-664d-4519-87a5-3918ba3a30a9) - Closing
[0m23:30:53.498314 [info ] [MainThread]: 
[0m23:30:53.500727 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.84 seconds (8.84s).
[0m23:30:53.503801 [debug] [MainThread]: Command end result
[0m23:30:53.529471 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:30:53.530874 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:30:53.534879 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:30:53.534879 [info ] [MainThread]: 
[0m23:30:53.536351 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m23:30:53.536351 [info ] [MainThread]: 
[0m23:30:53.537350 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m23:30:53.537350 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m23:30:53.538352 [info ] [MainThread]: 
[0m23:30:53.538352 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m23:30:53.538352 [info ] [MainThread]: 
[0m23:30:53.539358 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m23:30:53.539358 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:30:53.540569 [debug] [MainThread]: Command `dbt run` failed at 23:30:53.540569 after 10.92 seconds
[0m23:30:53.540569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001183760C5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001183760C080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001185B46AAE0>]}
[0m23:30:53.541569 [debug] [MainThread]: Flushing usage events
[0m23:30:54.555039 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:31:00.618926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235ADA0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235AEE66F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235C361220>]}


============================== 23:31:00.623228 | f1f305de-7e9e-4f60-8be4-b23ecd5b868b ==============================
[0m23:31:00.623228 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:31:00.623228 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m23:31:01.327246 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:31:01.327246 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:31:01.327246 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:31:01.988285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E28FB00>]}
[0m23:31:02.046291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E177F20>]}
[0m23:31:02.046291 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:31:02.274056 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:31:02.330668 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:31:02.331671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E192000>]}
[0m23:31:02.378141 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:31:02.379147 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:31:02.383779 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:31:02.403740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E5A9A60>]}
[0m23:31:02.454277 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:31:02.456232 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:31:02.487410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237FA4FDD0>]}
[0m23:31:02.487410 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:31:02.487410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E5B75C0>]}
[0m23:31:02.488487 [info ] [MainThread]: 
[0m23:31:02.489520 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:31:02.489520 [info ] [MainThread]: 
[0m23:31:02.490500 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:31:02.490500 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:31:02.491966 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:31:02.491966 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:31:02.491966 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:31:02.491966 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:31:02.491966 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:31:02.688916 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=80ddaa41-1b3a-461e-8c53-94ab29201f2a) - Created
[0m23:31:02.811147 [debug] [ThreadPool]: SQL status: OK in 0.320 seconds
[0m23:31:02.812080 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=80ddaa41-1b3a-461e-8c53-94ab29201f2a, command-id=49e3164a-b62a-4253-9ce2-d9c1b0c88833) - Closing
[0m23:31:02.818967 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:31:02.820964 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:31:02.835340 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:31:02.835340 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:31:02.835340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:31:03.025304 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=d758dbdd-94f5-49f3-bec8-6c952d4c0977) - Created
[0m23:31:03.317012 [debug] [ThreadPool]: SQL status: OK in 0.480 seconds
[0m23:31:03.326236 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=d758dbdd-94f5-49f3-bec8-6c952d4c0977, command-id=5160c1ea-4d3c-43f6-97e3-2f8b34ef18b4) - Closing
[0m23:31:03.329333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001237E54AF90>]}
[0m23:31:03.335506 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:31:03.335506 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:31:03.337518 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:31:03.337518 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:31:03.337518 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:31:03.346911 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:31:03.347952 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:31:03.373970 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:31:03.380963 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:31:03.380963 [debug] [Thread-1 (]: Safe create: False
[0m23:31:03.393442 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:31:03.394391 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:03.394391 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    try_cast(MIN_AMOUNT AS DOUBLE) as min_salary, 
    try_cast(MAX_AMOUNT AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:31:03.394391 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:31:03.579618 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297) - Created
[0m23:31:03.839152 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m23:31:03.842159 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=9945d958-d0a8-4d29-918b-8d799a4a224d) - Closing
[0m23:31:03.843150 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:31:03.862111 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:03.862111 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:31:04.063711 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:31:04.067714 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=b17d3a11-a205-4023-ae95-c1b73d870627) - Closing
[0m23:31:04.073000 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:04.074030 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:31:04.253626 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m23:31:04.254881 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=61a667bc-3331-47d8-8a31-2e7895fc13a0) - Closing
[0m23:31:04.259454 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:04.260017 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:31:04.530350 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:31:04.533376 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=6e8b3607-c338-4bfd-8f8f-da2dd81fb7eb) - Closing
[0m23:31:04.538774 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:04.538774 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:31:04.955991 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m23:31:04.957956 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=5454b6f7-8474-4878-8bc1-d8a0aaa80804) - Closing
[0m23:31:04.964119 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:04.964119 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:31:05.557073 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m23:31:05.559599 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=be5a46ae-7639-42b3-91d6-99b92622adee) - Closing
[0m23:31:05.565626 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:05.565626 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:31:05.787093 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m23:31:05.791315 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=3e3e41a6-697f-4e3a-8546-5279e7768c19) - Closing
[0m23:31:05.798923 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:05.798923 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:31:06.087408 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:31:06.092478 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=6966e54b-5fe3-4f04-880f-79eb733408a3) - Closing
[0m23:31:06.103263 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:06.104249 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:31:06.431097 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m23:31:06.433089 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=2bfd000b-2e80-48fd-9d00-22f140f42eaf) - Closing
[0m23:31:06.434086 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:31:06.442171 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:31:06.464940 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:06.464940 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:31:06.700158 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m23:31:06.702597 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=5c5b94e6-03c0-4014-880d-dc4a1a9e6c8f) - Closing
[0m23:31:06.703877 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:06.703877 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:31:06.905706 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m23:31:06.908095 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=3ad7c41d-49fc-492c-9463-98194e44d6f2) - Closing
[0m23:31:06.917316 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:31:06.920128 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:31:06.921117 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:31:09.856315 [debug] [Thread-1 (]: SQL status: OK in 2.930 seconds
[0m23:31:09.860245 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297, command-id=41ec7aba-6910-489f-8f61-674e4cfab6c3) - Closing
[0m23:31:09.873816 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f1f305de-7e9e-4f60-8be4-b23ecd5b868b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000123582B4380>]}
[0m23:31:09.873816 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.54s]
[0m23:31:09.875754 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:31:09.879473 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.5491485595703125s, language=None, compute-name=) - Reusing connection previously named master
[0m23:31:09.880492 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:31:09.881491 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:31:09.882486 [debug] [MainThread]: On list_demo: Close
[0m23:31:09.882486 [debug] [MainThread]: Databricks adapter: Connection(session-id=80ddaa41-1b3a-461e-8c53-94ab29201f2a) - Closing
[0m23:31:09.959606 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:31:09.959606 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:31:09.959606 [debug] [MainThread]: Databricks adapter: Connection(session-id=d758dbdd-94f5-49f3-bec8-6c952d4c0977) - Closing
[0m23:31:10.033484 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:31:10.033484 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:31:10.034427 [debug] [MainThread]: Databricks adapter: Connection(session-id=a7472fc0-f8ab-4345-9268-e26433ffd297) - Closing
[0m23:31:10.264975 [info ] [MainThread]: 
[0m23:31:10.264975 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 7.77 seconds (7.77s).
[0m23:31:10.266411 [debug] [MainThread]: Command end result
[0m23:31:10.285813 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:31:10.287841 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:31:10.292172 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:31:10.292172 [info ] [MainThread]: 
[0m23:31:10.293206 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:31:10.293206 [info ] [MainThread]: 
[0m23:31:10.294209 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:31:10.294209 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:31:10.295209 [debug] [MainThread]: Command `dbt run` succeeded at 23:31:10.295209 after 9.79 seconds
[0m23:31:10.295209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235ADA0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235C3618E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001235C361220>]}
[0m23:31:10.295209 [debug] [MainThread]: Flushing usage events
[0m23:31:11.332771 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:49:09.675897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBC6A22630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBC67DC7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBC67DE870>]}


============================== 22:49:09.681322 | 292fb11a-b4fb-4ebe-837a-389e204e0292 ==============================
[0m22:49:09.681322 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:49:09.682321 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m22:49:12.162546 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:49:12.162546 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:49:12.163613 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:49:14.547403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBE97209E0>]}
[0m22:49:14.604460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBE9B07140>]}
[0m22:49:14.604460 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:49:14.985380 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:49:15.097170 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:49:15.098170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBC6E087A0>]}
[0m22:49:16.252749 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:49:16.253776 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m22:49:16.463618 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:49:16.481317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB903DA0>]}
[0m22:49:16.530709 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:49:16.545806 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:49:16.584058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB888440>]}
[0m22:49:16.585055 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m22:49:16.585055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB507E60>]}
[0m22:49:16.586059 [info ] [MainThread]: 
[0m22:49:16.587056 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:49:16.587056 [info ] [MainThread]: 
[0m22:49:16.587056 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:16.588056 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:49:16.588056 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:16.589064 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:49:16.589064 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:49:16.589064 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:49:16.590064 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:18.146379 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=7a927430-5fd1-4183-85ea-062866a128e6) - Created
[0m22:49:22.827121 [debug] [ThreadPool]: SQL status: OK in 6.240 seconds
[0m22:49:22.829116 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=7a927430-5fd1-4183-85ea-062866a128e6, command-id=cd56110a-03c8-4329-a0a7-7d08cc89ac34) - Closing
[0m22:49:22.839811 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:22.840793 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:49:22.849110 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:49:22.850108 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:49:22.850108 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:49:23.236885 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f45818ae-311a-4e68-bd22-6ad6f59f3347) - Created
[0m22:49:38.590230 [debug] [ThreadPool]: SQL status: OK in 15.740 seconds
[0m22:49:38.607756 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f45818ae-311a-4e68-bd22-6ad6f59f3347, command-id=56a73317-63c6-4e49-9255-c44efd695fff) - Closing
[0m22:49:38.773573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB8F9A90>]}
[0m22:49:38.784389 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m22:49:38.785436 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m22:49:38.786390 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:49:38.787438 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m22:49:38.787438 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m22:49:38.796385 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m22:49:38.799031 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m22:49:38.824120 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:49:38.831132 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:49:38.831132 [debug] [Thread-1 (]: Safe create: False
[0m22:49:38.844438 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:49:38.845432 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:38.845432 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    try_cast(NULLIF(TRIM(MIN_AMOUNT),' ') AS DOUBLE) as min_salary, 
    try_cast(NULLIF(TRIM(MAX_AMOUNT),' ') AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m22:49:38.846389 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:49:39.110891 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=873fdd94-917f-474b-82e7-bb8fd294f535) - Created
[0m22:49:48.653167 [debug] [Thread-1 (]: SQL status: OK in 9.810 seconds
[0m22:49:48.656166 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=0c03954c-7306-485c-b9d6-46acc3914d8c) - Closing
[0m22:49:48.732102 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:49:48.743058 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:48.743058 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m22:49:49.318870 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m22:49:49.322898 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=2fbe312e-7702-4321-909a-20874bd35b0a) - Closing
[0m22:49:49.329867 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:49.330906 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:49:49.890416 [debug] [Thread-1 (]: SQL status: OK in 0.560 seconds
[0m22:49:49.894418 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=b70f1bcb-06b9-4757-be12-8b8742629483) - Closing
[0m22:49:49.902659 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:49.902659 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m22:49:51.175923 [debug] [Thread-1 (]: SQL status: OK in 1.270 seconds
[0m22:49:51.180913 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=777ae64b-1f5a-4e8f-bd24-a42a2624f94e) - Closing
[0m22:49:51.187880 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:51.187880 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:49:54.157713 [debug] [Thread-1 (]: SQL status: OK in 2.970 seconds
[0m22:49:54.163774 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=bae0a65f-a560-43df-be79-7c0274534953) - Closing
[0m22:49:54.176089 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:54.176089 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:49:57.274822 [debug] [Thread-1 (]: SQL status: OK in 3.100 seconds
[0m22:49:57.278797 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=8986d4c3-7a86-4707-bffb-78b3190c6031) - Closing
[0m22:49:57.284851 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:57.284851 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:49:57.790020 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m22:49:57.790959 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=af0694c6-1cd5-4953-8eb8-109c0fad86f7) - Closing
[0m22:49:57.793946 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:57.793946 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:49:58.853370 [debug] [Thread-1 (]: SQL status: OK in 1.060 seconds
[0m22:49:58.860068 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=1ac33af3-dd08-4ab7-9768-230170b2b418) - Closing
[0m22:49:58.867066 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:49:58.868074 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:50:00.672600 [debug] [Thread-1 (]: SQL status: OK in 1.800 seconds
[0m22:50:00.674770 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=eb8c275b-3cbb-4283-b85d-9262d852555e) - Closing
[0m22:50:00.687671 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:50:00.698653 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m22:50:00.722825 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:50:00.722825 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m22:50:01.291682 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m22:50:01.292671 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=ac60b408-dd8c-4da8-a935-69460b6d10bc) - Closing
[0m22:50:01.294672 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:50:01.295679 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m22:50:01.994669 [debug] [Thread-1 (]: SQL status: OK in 0.700 seconds
[0m22:50:01.998599 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=09289ef9-086c-47ee-ae10-27cc08f6fd95) - Closing
[0m22:50:02.006686 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:50:02.007647 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:50:02.007647 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:50:19.177472 [debug] [Thread-1 (]: SQL status: OK in 17.170 seconds
[0m22:50:19.178475 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=873fdd94-917f-474b-82e7-bb8fd294f535, command-id=ecec352a-cd8f-43fe-a5e7-c63bc6dc8af1) - Closing
[0m22:50:19.293423 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '292fb11a-b4fb-4ebe-837a-389e204e0292', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB95E0C0>]}
[0m22:50:19.293423 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 40.50s]
[0m22:50:19.295366 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m22:50:19.296444 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=40.5218710899353s, language=None, compute-name=) - Reusing connection previously named master
[0m22:50:19.296444 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:50:19.296444 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:50:19.297742 [debug] [MainThread]: On list_demo: Close
[0m22:50:19.297742 [debug] [MainThread]: Databricks adapter: Connection(session-id=7a927430-5fd1-4183-85ea-062866a128e6) - Closing
[0m22:50:19.834792 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:50:19.835792 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:50:19.835792 [debug] [MainThread]: Databricks adapter: Connection(session-id=f45818ae-311a-4e68-bd22-6ad6f59f3347) - Closing
[0m22:50:19.934688 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m22:50:19.935688 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m22:50:19.935688 [debug] [MainThread]: Databricks adapter: Connection(session-id=873fdd94-917f-474b-82e7-bb8fd294f535) - Closing
[0m22:50:20.377763 [info ] [MainThread]: 
[0m22:50:20.379755 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 3.79 seconds (63.79s).
[0m22:50:20.380698 [debug] [MainThread]: Command end result
[0m22:50:20.406360 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:50:20.407361 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:50:20.411443 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:50:20.411443 [info ] [MainThread]: 
[0m22:50:20.412373 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:50:20.412373 [info ] [MainThread]: 
[0m22:50:20.413348 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:50:20.413348 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:50:20.414344 [debug] [MainThread]: Command `dbt run` succeeded at 22:50:20.414344 after 70.88 seconds
[0m22:50:20.414344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBC67DE870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEB8FDD30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CBEA03FEF0>]}
[0m22:50:20.415347 [debug] [MainThread]: Flushing usage events
[0m22:50:21.412649 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:59:17.901653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E1B028A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E14D6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E14D4FB0>]}


============================== 22:59:17.906646 | 5d3bf390-451f-47ef-8d3a-758e76026090 ==============================
[0m22:59:17.906646 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:59:17.907660 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m22:59:18.630653 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:59:18.630653 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:59:18.631645 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:59:19.571897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E30F0440>]}
[0m22:59:19.629543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E08602A210>]}
[0m22:59:19.630557 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:59:19.870136 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:59:19.958745 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:59:19.959631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E08667EE10>]}
[0m22:59:20.031020 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m22:59:20.032013 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m22:59:20.032013 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m22:59:20.349927 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:59:20.356967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E08796EA20>]}
[0m22:59:20.453260 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:59:20.456957 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:59:20.484507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E08795C650>]}
[0m22:59:20.484507 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m22:59:20.484507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E204C080>]}
[0m22:59:20.485509 [info ] [MainThread]: 
[0m22:59:20.486514 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:59:20.486514 [info ] [MainThread]: 
[0m22:59:20.486514 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:20.486514 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:59:20.487769 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:20.488798 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:59:20.488798 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:59:20.488798 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:59:20.488798 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:59:20.876438 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=80838ba5-fb9d-4972-8392-c07b24b145a7) - Created
[0m22:59:22.236759 [debug] [ThreadPool]: SQL status: OK in 1.750 seconds
[0m22:59:22.238763 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=80838ba5-fb9d-4972-8392-c07b24b145a7, command-id=cb39ee93-f17c-4b7d-96b9-777aab61bb9e) - Closing
[0m22:59:22.245749 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:22.245749 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m22:59:22.254225 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m22:59:22.254225 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m22:59:22.255233 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:59:22.513426 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=b73e25b1-fed2-47f3-accb-e98f95f52ba2) - Created
[0m22:59:23.337046 [debug] [ThreadPool]: SQL status: OK in 1.080 seconds
[0m22:59:23.355520 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=b73e25b1-fed2-47f3-accb-e98f95f52ba2, command-id=e78fca21-66bf-440c-aa55-3d3fe6b95e12) - Closing
[0m22:59:23.357526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E4CD2210>]}
[0m22:59:23.367853 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m22:59:23.370117 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m22:59:23.370117 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:23.370117 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m22:59:23.371139 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m22:59:23.388881 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m22:59:23.389889 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m22:59:23.439786 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m22:59:23.456317 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m22:59:23.456317 [debug] [Thread-1 (]: Safe create: False
[0m22:59:23.475317 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:59:23.476317 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:23.476317 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    cast(NULLIF(TRIM(MIN_AMOUNT),' ') AS DOUBLE) as min_salary, 
    cast(NULLIF(TRIM(MAX_AMOUNT),' ') AS DOUBLE) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m22:59:23.476317 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:59:23.720722 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=ed975e02-2755-498e-a058-3512797463a6) - Created
[0m22:59:24.192773 [debug] [Thread-1 (]: SQL status: OK in 0.720 seconds
[0m22:59:24.196825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=c0e95636-b971-4d63-948b-5917da523462) - Closing
[0m22:59:24.197825 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m22:59:24.220890 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:24.221894 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m22:59:24.590836 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m22:59:24.598931 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=3e564b42-821a-4264-8816-83877d3ccf81) - Closing
[0m22:59:24.623523 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:24.625596 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:59:25.045444 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m22:59:25.054571 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=8fbf8e06-c1b6-4c4a-b67d-f5aeb2fd2c5b) - Closing
[0m22:59:25.077787 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:25.080318 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m22:59:25.525207 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m22:59:25.530586 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=eca9d8c2-3fb6-45f8-bda4-6197fa3b4a9c) - Closing
[0m22:59:25.557307 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:25.559303 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:26.311295 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m22:59:26.317290 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=184f6867-1451-44ea-8db9-c38ae681f9a8) - Closing
[0m22:59:26.345859 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:26.348437 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m22:59:27.926817 [debug] [Thread-1 (]: SQL status: OK in 1.580 seconds
[0m22:59:27.931915 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=c0c5a0a0-c163-4ce6-827b-ccf74e0d0438) - Closing
[0m22:59:27.941606 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:27.943138 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m22:59:28.259717 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m22:59:28.263667 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=58266128-a04e-4c42-a83b-781a7cd236fe) - Closing
[0m22:59:28.277727 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:28.279816 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:59:28.852644 [debug] [Thread-1 (]: SQL status: OK in 0.570 seconds
[0m22:59:28.864643 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=bdb2b8e2-fab4-4696-aa73-735cd5c1d74b) - Closing
[0m22:59:28.878643 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:28.879978 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m22:59:29.722783 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m22:59:29.733213 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=e82428e2-3a7f-4108-858b-484ff022a189) - Closing
[0m22:59:29.738910 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m22:59:29.776155 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m22:59:29.881710 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:29.882741 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m22:59:30.230973 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m22:59:30.239493 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=9e522785-3d40-4e05-97ca-4c5b997a6ba2) - Closing
[0m22:59:30.249116 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:30.251055 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m22:59:30.529212 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m22:59:30.538360 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=55868f79-6c0e-423b-9b22-dd7d6d5e6daf) - Closing
[0m22:59:30.580173 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m22:59:30.581171 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m22:59:30.581171 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m22:59:35.648071 [debug] [Thread-1 (]: SQL status: OK in 5.070 seconds
[0m22:59:35.652715 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ed975e02-2755-498e-a058-3512797463a6, command-id=0f6ce8e2-ded3-43f9-944c-08e88a7406d2) - Closing
[0m22:59:35.730673 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5d3bf390-451f-47ef-8d3a-758e76026090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0DF014470>]}
[0m22:59:35.734693 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 12.35s]
[0m22:59:35.736753 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m22:59:35.741705 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=12.383145570755005s, language=None, compute-name=) - Reusing connection previously named master
[0m22:59:35.744684 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:59:35.746681 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m22:59:35.748683 [debug] [MainThread]: On list_demo: Close
[0m22:59:35.750683 [debug] [MainThread]: Databricks adapter: Connection(session-id=80838ba5-fb9d-4972-8392-c07b24b145a7) - Closing
[0m22:59:35.823826 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m22:59:35.824824 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m22:59:35.827104 [debug] [MainThread]: Databricks adapter: Connection(session-id=b73e25b1-fed2-47f3-accb-e98f95f52ba2) - Closing
[0m22:59:35.936875 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m22:59:35.938874 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m22:59:35.939812 [debug] [MainThread]: Databricks adapter: Connection(session-id=ed975e02-2755-498e-a058-3512797463a6) - Closing
[0m22:59:36.265823 [info ] [MainThread]: 
[0m22:59:36.267902 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 15.78 seconds (15.78s).
[0m22:59:36.271853 [debug] [MainThread]: Command end result
[0m22:59:36.332531 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:59:36.334510 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:59:36.337604 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m22:59:36.338618 [info ] [MainThread]: 
[0m22:59:36.339567 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:59:36.341558 [info ] [MainThread]: 
[0m22:59:36.343578 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m22:59:36.343578 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m22:59:36.346589 [debug] [MainThread]: Command `dbt run` succeeded at 22:59:36.345611 after 18.55 seconds
[0m22:59:36.347588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E17E68A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E14D6930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0E14D4FB0>]}
[0m22:59:36.348607 [debug] [MainThread]: Flushing usage events
[0m22:59:37.359137 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:00:19.481244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319B9011C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319B900320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319B9027E0>]}


============================== 23:00:19.485250 | 2243c007-5ec0-4e03-a24b-e207ddb59e08 ==============================
[0m23:00:19.485250 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:00:19.485250 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_cleaned', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m23:00:20.202040 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:00:20.202040 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:00:20.203010 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:00:20.847538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023198E14F20>]}
[0m23:00:20.903244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319EA34A10>]}
[0m23:00:20.903244 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:00:21.118639 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:00:21.203752 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:00:21.205005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231BF119520>]}
[0m23:00:21.250514 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:00:21.251515 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m23:00:21.467389 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:00:21.476399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C0803E30>]}
[0m23:00:21.524872 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:00:21.525860 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:00:21.563873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C0784290>]}
[0m23:00:21.563873 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:00:21.563873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C071A5A0>]}
[0m23:00:21.564966 [info ] [MainThread]: 
[0m23:00:21.565980 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:00:21.566966 [info ] [MainThread]: 
[0m23:00:21.567966 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:21.567966 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:00:21.568836 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:21.568836 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:00:21.569340 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:00:21.569340 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:00:21.569340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:00:21.891633 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f2e02d74-61e1-4933-90bf-150b5caf103b) - Created
[0m23:00:22.041147 [debug] [ThreadPool]: SQL status: OK in 0.470 seconds
[0m23:00:22.043107 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f2e02d74-61e1-4933-90bf-150b5caf103b, command-id=1aa21fd0-b388-4e8f-986c-ecec65ad7098) - Closing
[0m23:00:22.052657 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:22.053672 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:00:22.066675 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:00:22.066675 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:00:22.067675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:00:22.276690 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=9550e582-7905-404b-9efb-b45d74177bda) - Created
[0m23:00:22.791742 [debug] [ThreadPool]: SQL status: OK in 0.730 seconds
[0m23:00:22.796004 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9550e582-7905-404b-9efb-b45d74177bda, command-id=919c96c6-5ca0-4f71-b659-770629481602) - Closing
[0m23:00:22.798530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C068BBC0>]}
[0m23:00:22.800611 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:00:22.800611 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:00:22.801618 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:22.801618 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:00:22.801618 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:00:22.808617 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:00:22.809617 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:00:22.833857 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:00:22.839938 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:00:22.840900 [debug] [Thread-1 (]: Safe create: False
[0m23:00:22.853604 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:00:22.854643 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:22.854643 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    try_cast(NULLIF(TRIM(MIN_AMOUNT),' ') AS int) as min_salary, 
    try_cast(NULLIF(TRIM(MAX_AMOUNT),' ') AS int) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m23:00:22.854643 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:00:23.064793 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=e662e0ca-a550-4826-adb4-f954104f5cec) - Created
[0m23:00:23.500269 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m23:00:23.502334 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=9938af5c-778b-4fda-b71a-8f279e9e2008) - Closing
[0m23:00:23.502334 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m23:00:23.515262 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:23.515262 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m23:00:23.787209 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m23:00:23.790170 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=be5c3a3c-270a-4fef-8362-20550245f0e7) - Closing
[0m23:00:23.797246 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:23.797246 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:00:24.126466 [debug] [Thread-1 (]: SQL status: OK in 0.330 seconds
[0m23:00:24.132475 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=70635743-7024-4b7c-baee-c11f40d6ca4d) - Closing
[0m23:00:24.141489 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:24.142436 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m23:00:24.572196 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m23:00:24.576240 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=ee148601-97a4-48d8-b14a-12b4857074e0) - Closing
[0m23:00:24.583459 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:24.583459 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:00:25.300671 [debug] [Thread-1 (]: SQL status: OK in 0.720 seconds
[0m23:00:25.305743 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=ecb2abe0-eacc-4d54-be0f-c26d89efb002) - Closing
[0m23:00:25.315701 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:25.315701 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m23:00:26.342167 [debug] [Thread-1 (]: SQL status: OK in 1.030 seconds
[0m23:00:26.345209 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=8b867f28-ad4c-4ce6-b7e6-3d511b1404b9) - Closing
[0m23:00:26.352268 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:26.352268 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m23:00:26.715930 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m23:00:26.721615 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=6c8dbf28-d997-499c-9690-3cae4f12a224) - Closing
[0m23:00:26.724966 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:26.724966 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:00:27.147176 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m23:00:27.154275 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=31ce77d1-c911-412e-a1c5-046eabdb96d6) - Closing
[0m23:00:27.161254 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:27.161254 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m23:00:27.810025 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m23:00:27.813997 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=af7480e5-73b0-41fa-b812-25174bba469a) - Closing
[0m23:00:27.815001 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m23:00:27.821541 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m23:00:27.845418 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:27.845418 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m23:00:28.325234 [debug] [Thread-1 (]: SQL status: OK in 0.480 seconds
[0m23:00:28.327879 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=096174e0-23b4-4059-a87d-c985f1e7ceb4) - Closing
[0m23:00:28.330878 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:28.330878 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:00:28.622148 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:00:28.626089 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=f2729767-2994-412a-8764-327f80fb5c27) - Closing
[0m23:00:28.637614 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:00:28.638614 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:00:28.639615 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m23:00:33.326484 [debug] [Thread-1 (]: SQL status: OK in 4.690 seconds
[0m23:00:33.327334 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e662e0ca-a550-4826-adb4-f954104f5cec, command-id=50c481b0-af7d-4ec2-9620-9ed835214f9c) - Closing
[0m23:00:33.342088 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2243c007-5ec0-4e03-a24b-e207ddb59e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C077EAE0>]}
[0m23:00:33.342088 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 10.54s]
[0m23:00:33.343507 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:00:33.345601 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=10.547070980072021s, language=None, compute-name=) - Reusing connection previously named master
[0m23:00:33.345601 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:00:33.346770 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:00:33.346770 [debug] [MainThread]: On list_demo: Close
[0m23:00:33.346770 [debug] [MainThread]: Databricks adapter: Connection(session-id=f2e02d74-61e1-4933-90bf-150b5caf103b) - Closing
[0m23:00:33.437091 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:00:33.438044 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:00:33.438044 [debug] [MainThread]: Databricks adapter: Connection(session-id=9550e582-7905-404b-9efb-b45d74177bda) - Closing
[0m23:00:33.505434 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:00:33.506433 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:00:33.506433 [debug] [MainThread]: Databricks adapter: Connection(session-id=e662e0ca-a550-4826-adb4-f954104f5cec) - Closing
[0m23:00:33.923741 [info ] [MainThread]: 
[0m23:00:33.924847 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 12.36 seconds (12.36s).
[0m23:00:33.929737 [debug] [MainThread]: Command end result
[0m23:00:33.956236 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:00:33.958243 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:00:33.963243 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:00:33.963243 [info ] [MainThread]: 
[0m23:00:33.963243 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:00:33.964245 [info ] [MainThread]: 
[0m23:00:33.964245 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:00:33.965396 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:00:33.965396 [debug] [MainThread]: Command `dbt run` succeeded at 23:00:33.965396 after 14.61 seconds
[0m23:00:33.965396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319BDCD880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000231C055AB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002319B9BA2A0>]}
[0m23:00:33.965396 [debug] [MainThread]: Flushing usage events
[0m23:00:34.932735 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:33:16.211512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C802105C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C806BDA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C80146480>]}


============================== 13:33:16.217690 | d7dd9ffb-1529-4a76-9c4f-87856ad7cbc8 ==============================
[0m13:33:16.217690 [info ] [MainThread]: Running with dbt=1.10.4
[0m13:33:16.218687 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_test', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m13:33:19.595526 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:33:19.595526 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:33:19.595526 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:33:22.371841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CFD6D4DA0>]}
[0m13:33:22.449428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C81802450>]}
[0m13:33:22.450480 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m13:33:22.955194 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m13:33:23.096992 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m13:33:23.097991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CA3A77CE0>]}
[0m13:33:24.490304 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:33:24.490304 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:33:24.498056 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m13:33:24.522301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CA3C193A0>]}
[0m13:33:24.593643 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m13:33:24.605599 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m13:33:24.643269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CA3D308F0>]}
[0m13:33:24.644411 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m13:33:24.644411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CFFB304D0>]}
[0m13:33:24.646407 [info ] [MainThread]: 
[0m13:33:24.646407 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:33:24.647407 [info ] [MainThread]: 
[0m13:33:24.648409 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:33:24.648409 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:33:24.649407 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:33:24.650407 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m13:33:24.650407 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m13:33:24.650407 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m13:33:24.650407 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:25.738611 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=907333fa-5791-4f1e-b71e-0ee9c7163a22) - Created
[0m13:33:27.983969 [debug] [ThreadPool]: SQL status: OK in 3.330 seconds
[0m13:33:27.984974 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=907333fa-5791-4f1e-b71e-0ee9c7163a22, command-id=91b8bf7a-8b85-4c23-b155-a21bf0a30a1e) - Closing
[0m13:33:27.992337 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:33:27.992337 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m13:33:28.003136 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m13:33:28.004155 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m13:33:28.004155 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:33:28.309036 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=425dd703-555c-4c5d-b5c9-28e9ba180d19) - Created
[0m13:33:39.743453 [debug] [ThreadPool]: SQL status: OK in 11.740 seconds
[0m13:33:39.757971 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=425dd703-555c-4c5d-b5c9-28e9ba180d19, command-id=8ec99df4-216f-41d4-9597-8b7f61055661) - Closing
[0m13:33:39.857753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CA4E79B20>]}
[0m13:33:39.864263 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m13:33:39.865298 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m13:33:39.866433 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:33:39.866433 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_test'
[0m13:33:39.867438 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m13:33:39.879313 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m13:33:39.880311 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m13:33:39.909371 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m13:33:39.918223 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m13:33:39.919222 [debug] [Thread-1 (]: Safe create: False
[0m13:33:39.934427 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m13:33:39.935464 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:39.935464 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_test`
    )
    
[0m13:33:39.935464 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:33:40.231810 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94) - Created
[0m13:33:52.152900 [debug] [Thread-1 (]: SQL status: OK in 12.220 seconds
[0m13:33:52.154390 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=dc82b6d9-e586-457f-ac80-7c25196fead7) - Closing
[0m13:33:52.226032 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m13:33:52.238023 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:52.238023 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test'
  
[0m13:33:52.681838 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m13:33:52.682973 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=1cf1089c-f536-4bb1-a6d6-332d2b81ed49) - Closing
[0m13:33:52.687800 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:52.687800 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m13:33:53.090658 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m13:33:53.091694 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=e230ab20-a39e-4945-9292-b04d783cbe6b) - Closing
[0m13:33:53.096262 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:53.096262 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test'
    AND is_nullable = 'NO';
  
[0m13:33:55.439549 [debug] [Thread-1 (]: SQL status: OK in 2.340 seconds
[0m13:33:55.441727 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=7c5e0cdd-fe8a-4764-91cb-06088c799b7b) - Closing
[0m13:33:55.449346 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:55.449346 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m13:33:57.590527 [debug] [Thread-1 (]: SQL status: OK in 2.140 seconds
[0m13:33:57.591506 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=4d705b1c-3fb4-4b1d-b9fe-a7690fa3ba5c) - Closing
[0m13:33:57.597346 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:33:57.597346 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m13:34:00.195966 [debug] [Thread-1 (]: SQL status: OK in 2.600 seconds
[0m13:34:00.197966 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=8a264f66-7278-4129-ae95-6ba70f895460) - Closing
[0m13:34:00.202000 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:00.202969 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m13:34:00.607340 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m13:34:00.608611 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=087e36bc-aa4b-4f41-81f4-fb6f9959844f) - Closing
[0m13:34:00.613267 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:00.613267 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_test`
  
[0m13:34:01.515765 [debug] [Thread-1 (]: SQL status: OK in 0.900 seconds
[0m13:34:01.516771 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=d3047f2a-42e4-4652-a612-dbb224a97d90) - Closing
[0m13:34:01.520005 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:01.521072 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
describe extended `demo`.`demo_schema`.`indeed_test`
  
[0m13:34:02.972076 [debug] [Thread-1 (]: SQL status: OK in 1.450 seconds
[0m13:34:02.974098 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=92646e60-bc99-4b4c-b4e7-2eb7c57b6fe9) - Closing
[0m13:34:02.982740 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m13:34:02.989781 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_test`
[0m13:34:03.012713 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:03.013831 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_test`

  
[0m13:34:03.533359 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m13:34:03.536387 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=c859ddbd-10d4-4b87-bf4f-88b2b5b3002a) - Closing
[0m13:34:03.538685 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:03.538685 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m13:34:04.086256 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m13:34:04.088312 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=45ea077b-5698-4cb4-b4f3-59fbb34c9ade) - Closing
[0m13:34:04.097646 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m13:34:04.098706 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m13:34:04.099716 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_test` as DBT_INTERNAL_DEST
    using
        `indeed_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m13:34:24.564828 [debug] [Thread-1 (]: SQL status: OK in 20.470 seconds
[0m13:34:24.565838 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94, command-id=570661cc-26bd-4ddf-992e-741dccf4606a) - Closing
[0m13:34:24.664924 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd7dd9ffb-1529-4a76-9c4f-87856ad7cbc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CFD6AFFE0>]}
[0m13:34:24.665987 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 44.80s]
[0m13:34:24.665987 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m13:34:24.667177 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=44.80844020843506s, language=None, compute-name=) - Reusing connection previously named master
[0m13:34:24.668476 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:34:24.668476 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m13:34:24.668476 [debug] [MainThread]: On list_demo: Close
[0m13:34:24.668476 [debug] [MainThread]: Databricks adapter: Connection(session-id=907333fa-5791-4f1e-b71e-0ee9c7163a22) - Closing
[0m13:34:25.000384 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m13:34:25.000384 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m13:34:25.000384 [debug] [MainThread]: Databricks adapter: Connection(session-id=425dd703-555c-4c5d-b5c9-28e9ba180d19) - Closing
[0m13:34:25.121597 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_test' was properly closed.
[0m13:34:25.121597 [debug] [MainThread]: On model.jra_dbt.indeed_test: Close
[0m13:34:25.121597 [debug] [MainThread]: Databricks adapter: Connection(session-id=3c9555e7-068b-47ef-879d-07527c0f1b94) - Closing
[0m13:34:25.913064 [info ] [MainThread]: 
[0m13:34:25.913064 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 1.27 seconds (61.27s).
[0m13:34:25.914386 [debug] [MainThread]: Command end result
[0m13:34:25.935608 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m13:34:25.937609 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m13:34:25.942115 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m13:34:25.942115 [info ] [MainThread]: 
[0m13:34:25.943120 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:34:25.943120 [info ] [MainThread]: 
[0m13:34:25.943120 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m13:34:25.944121 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:34:25.945121 [debug] [MainThread]: Command `dbt run` succeeded at 13:34:25.945121 after 69.88 seconds
[0m13:34:25.945121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C81802FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C80172900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C807FD520>]}
[0m13:34:25.946121 [debug] [MainThread]: Flushing usage events
[0m13:34:26.962179 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:34:39.309721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022068E91820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022068E929C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022068E90590>]}


============================== 13:34:39.315184 | a65c6441-9fa0-4730-8551-f095b28aa731 ==============================
[0m13:34:39.315184 [info ] [MainThread]: Running with dbt=1.10.4
[0m13:34:39.315184 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m13:34:40.100830 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m13:34:40.102087 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m13:34:40.102087 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m13:34:40.880433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220688659A0>]}
[0m13:34:40.947138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002200C002870>]}
[0m13:34:40.947138 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m13:34:41.247804 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m13:34:41.325730 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m13:34:41.325730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002206BC34170>]}
[0m13:34:41.385935 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:34:41.385935 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:34:41.392797 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m13:34:41.419544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002200DB17E00>]}
[0m13:34:41.485060 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m13:34:41.487861 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m13:34:41.509971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002200C9F2B10>]}
[0m13:34:41.511054 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m13:34:41.511054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002200C67F7D0>]}
[0m13:34:41.512263 [info ] [MainThread]: 
[0m13:34:41.513264 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:34:41.513264 [info ] [MainThread]: 
[0m13:34:41.514261 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:34:41.514261 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m13:34:41.515267 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:34:41.515267 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m13:34:41.516290 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m13:34:41.516290 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m13:34:41.516290 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:41.837069 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f36c9727-27ac-4c79-8782-8782a34f5f72) - Created
[0m13:34:41.977243 [debug] [ThreadPool]: SQL status: OK in 0.460 seconds
[0m13:34:41.978244 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f36c9727-27ac-4c79-8782-8782a34f5f72, command-id=a83766fb-cb63-4c3a-97fe-5b421c90bbd3) - Closing
[0m13:34:41.985622 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:34:41.985622 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m13:34:41.995838 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m13:34:41.997043 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m13:34:41.997043 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:34:42.225729 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=0d800b27-4046-4d5f-b202-650e82c11d85) - Created
[0m13:34:42.855041 [debug] [ThreadPool]: SQL status: OK in 0.860 seconds
[0m13:34:42.860830 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0d800b27-4046-4d5f-b202-650e82c11d85, command-id=8564a996-8221-4453-863d-be6fa5c8f007) - Closing
[0m13:34:42.862844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220665A7470>]}
[0m13:34:42.865877 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m13:34:42.866878 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m13:34:42.866878 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m13:34:42.866878 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m13:34:42.867900 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m13:34:42.879752 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m13:34:42.881759 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m13:34:42.908944 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m13:34:42.916683 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m13:34:42.916683 [debug] [Thread-1 (]: Safe create: False
[0m13:34:42.933079 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m13:34:42.934077 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:42.934077 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    try_cast(NULLIF(TRIM(MIN_AMOUNT),' ') AS int) as min_salary, 
    try_cast(NULLIF(TRIM(MAX_AMOUNT),' ') AS int) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m13:34:42.934077 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:34:43.167877 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3) - Created
[0m13:34:44.093473 [debug] [Thread-1 (]: SQL status: OK in 1.160 seconds
[0m13:34:44.095006 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=35de2c25-4867-4519-919b-017d7f005825) - Closing
[0m13:34:44.095006 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m13:34:44.105841 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:44.105841 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m13:34:44.391891 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m13:34:44.393466 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=898e0fac-55b2-4fba-915d-6fa07273ae59) - Closing
[0m13:34:44.397477 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:44.397477 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m13:34:44.773956 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m13:34:44.776559 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=e17c07bc-b5ae-448c-a0b3-5a53b1d3e995) - Closing
[0m13:34:44.780861 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:44.780861 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m13:34:45.328514 [debug] [Thread-1 (]: SQL status: OK in 0.550 seconds
[0m13:34:45.330013 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=28dc1959-d1b0-4065-9250-a128b5f451c9) - Closing
[0m13:34:45.336952 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:45.336952 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m13:34:46.078028 [debug] [Thread-1 (]: SQL status: OK in 0.740 seconds
[0m13:34:46.079929 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=b2c4a31a-b7b5-4247-8ffd-414096cb9970) - Closing
[0m13:34:46.087469 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:46.087469 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m13:34:47.214258 [debug] [Thread-1 (]: SQL status: OK in 1.130 seconds
[0m13:34:47.216623 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=8687f21c-1691-4b00-95b4-ff7b75c07091) - Closing
[0m13:34:47.221333 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:47.221333 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m13:34:47.714170 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m13:34:47.715309 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=7ae2b684-07d3-4d9d-9dac-d518f0454cec) - Closing
[0m13:34:47.718842 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:47.718842 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m13:34:48.459430 [debug] [Thread-1 (]: SQL status: OK in 0.740 seconds
[0m13:34:48.461560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=b4264931-11a8-4ace-8704-7cb9dab4af2b) - Closing
[0m13:34:48.465152 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:48.466181 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m13:34:49.548699 [debug] [Thread-1 (]: SQL status: OK in 1.080 seconds
[0m13:34:49.549720 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=d3c1ed60-8702-4831-91f9-1a114966f45c) - Closing
[0m13:34:49.550723 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m13:34:49.558800 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m13:34:49.587691 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:49.588701 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m13:34:50.175326 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m13:34:50.177831 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=14756176-8a67-4c01-98d1-57e08bb25ad6) - Closing
[0m13:34:50.182148 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:50.182148 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m13:34:50.546185 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m13:34:50.548700 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=65e4826b-6f11-4be6-a19e-8fcb1bbb2533) - Closing
[0m13:34:50.558086 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m13:34:50.559574 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m13:34:50.559574 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m13:35:00.042945 [debug] [Thread-1 (]: SQL status: OK in 9.480 seconds
[0m13:35:00.044304 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3, command-id=207ad3ca-f5ad-48a5-b093-30aeb5a4174c) - Closing
[0m13:35:00.152342 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a65c6441-9fa0-4730-8551-f095b28aa731', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220663A4290>]}
[0m13:35:00.152342 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 17.28s]
[0m13:35:00.153597 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m13:35:00.154601 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=17.291756629943848s, language=None, compute-name=) - Reusing connection previously named master
[0m13:35:00.154601 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:35:00.154601 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m13:35:00.155603 [debug] [MainThread]: On list_demo: Close
[0m13:35:00.155603 [debug] [MainThread]: Databricks adapter: Connection(session-id=f36c9727-27ac-4c79-8782-8782a34f5f72) - Closing
[0m13:35:00.250521 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m13:35:00.250521 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m13:35:00.251717 [debug] [MainThread]: Databricks adapter: Connection(session-id=0d800b27-4046-4d5f-b202-650e82c11d85) - Closing
[0m13:35:00.335023 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m13:35:00.335023 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m13:35:00.336172 [debug] [MainThread]: Databricks adapter: Connection(session-id=222d18cd-291d-4807-baf8-8281cc85e0d3) - Closing
[0m13:35:00.872363 [info ] [MainThread]: 
[0m13:35:00.873366 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 19.36 seconds (19.36s).
[0m13:35:00.873366 [debug] [MainThread]: Command end result
[0m13:35:00.901959 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m13:35:00.904980 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m13:35:00.911651 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m13:35:00.911651 [info ] [MainThread]: 
[0m13:35:00.912661 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:35:00.912661 [info ] [MainThread]: 
[0m13:35:00.913692 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m13:35:00.914653 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m13:35:00.914653 [debug] [MainThread]: Command `dbt run` succeeded at 13:35:00.914653 after 21.72 seconds
[0m13:35:00.915684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002206A4839B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022068EF7E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002206947DA60>]}
[0m13:35:00.915684 [debug] [MainThread]: Flushing usage events
[0m13:35:01.873014 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:00:14.135800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F206E6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F206E330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F1D537D0>]}


============================== 14:00:14.138926 | 782f57a8-b143-4892-801f-6e37ef472527 ==============================
[0m14:00:14.138926 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:00:14.139933 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_test', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m14:00:14.829157 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:00:14.829157 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:00:14.830157 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:00:15.449059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F17958B0>]}
[0m14:00:15.508228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028096182720>]}
[0m14:00:15.508228 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:00:15.730059 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:00:15.788120 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:00:15.789118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002809629FCE0>]}
[0m14:00:15.835092 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:00:15.836092 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:00:15.840092 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:00:15.859958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028095C613A0>]}
[0m14:00:15.910163 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:00:15.912163 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:00:15.931181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028096614D40>]}
[0m14:00:15.931181 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:00:15.932350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002809778FC80>]}
[0m14:00:15.933350 [info ] [MainThread]: 
[0m14:00:15.933350 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:00:15.933350 [info ] [MainThread]: 
[0m14:00:15.934348 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:00:15.934348 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:00:15.935469 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:00:15.935469 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:00:15.935469 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:00:15.936475 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:00:15.936475 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:00:16.366230 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=545138de-1c83-45f5-bde6-e141fb1ece59) - Created
[0m14:00:16.544580 [debug] [ThreadPool]: SQL status: OK in 0.610 seconds
[0m14:00:16.546831 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=545138de-1c83-45f5-bde6-e141fb1ece59, command-id=cd5592ea-829c-418b-b604-0bd864eeb210) - Closing
[0m14:00:16.555951 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:00:16.556939 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:00:16.567451 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:00:16.568473 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:00:16.568473 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:00:16.806953 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=0229adee-65b5-43ed-b56c-b4cbf32c6122) - Created
[0m14:00:17.368749 [debug] [ThreadPool]: SQL status: OK in 0.800 seconds
[0m14:00:17.373878 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0229adee-65b5-43ed-b56c-b4cbf32c6122, command-id=c2bc7096-e3c8-43d9-8eb6-0536290d4b32) - Closing
[0m14:00:17.375895 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002809773F980>]}
[0m14:00:17.380078 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m14:00:17.380078 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m14:00:17.382128 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:00:17.382128 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_test'
[0m14:00:17.382128 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m14:00:17.390166 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m14:00:17.390166 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m14:00:17.417052 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:00:17.425028 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:00:17.425028 [debug] [Thread-1 (]: Safe create: False
[0m14:00:17.438438 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m14:00:17.439744 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:00:17.439744 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)
    
[0m14:00:17.439744 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:00:17.697124 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5) - Created
[0m14:00:18.186264 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m14:00:18.187259 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5, command-id=b3a60922-efae-42a8-a5ee-cd83f40c163c) - Closing
[0m14:00:18.188207 [debug] [Thread-1 (]: No existing relation found
[0m14:00:18.200832 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:00:18.200832 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m14:00:18.457876 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m14:00:18.461964 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5, command-id=47f5fabe-6366-4f9b-a04f-9b56eb92505f) - Closing
[0m14:00:18.489323 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m14:00:18.490319 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:00:18.490319 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_test`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      MIN_AMOUNT double,
    
      MAX_AMOUNT double,
    
      timestamp timestamp_ntz,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/indeed_test'
  
  

  
[0m14:00:20.398320 [debug] [Thread-1 (]: SQL status: OK in 1.910 seconds
[0m14:00:20.400404 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5, command-id=3d5662ec-15a7-4faa-b63e-252ddaeeb423) - Closing
[0m14:00:20.406357 [debug] [Thread-1 (]: Applying tags to relation None
[0m14:00:20.406357 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:00:20.407293 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    insert into `demo`.`demo_schema`.`indeed_test` select * from `indeed_test__dbt_tmp`
  
[0m14:00:24.634125 [debug] [Thread-1 (]: SQL status: OK in 4.230 seconds
[0m14:00:24.635132 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5, command-id=b0aace31-b58e-444a-8312-3d3a236a2a3c) - Closing
[0m14:00:24.648221 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '782f57a8-b143-4892-801f-6e37ef472527', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280EF2AFE90>]}
[0m14:00:24.649207 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 7.27s]
[0m14:00:24.649207 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m14:00:24.650207 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=7.274312257766724s, language=None, compute-name=) - Reusing connection previously named master
[0m14:00:24.650207 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:00:24.651360 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:00:24.651360 [debug] [MainThread]: On list_demo: Close
[0m14:00:24.651360 [debug] [MainThread]: Databricks adapter: Connection(session-id=545138de-1c83-45f5-bde6-e141fb1ece59) - Closing
[0m14:00:24.729124 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:00:24.729124 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:00:24.730126 [debug] [MainThread]: Databricks adapter: Connection(session-id=0229adee-65b5-43ed-b56c-b4cbf32c6122) - Closing
[0m14:00:24.814843 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_test' was properly closed.
[0m14:00:24.815856 [debug] [MainThread]: On model.jra_dbt.indeed_test: Close
[0m14:00:24.816848 [debug] [MainThread]: Databricks adapter: Connection(session-id=ebb7388d-d087-4da8-9e1b-04e926bfb0c5) - Closing
[0m14:00:25.235528 [info ] [MainThread]: 
[0m14:00:25.236571 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 9.30 seconds (9.30s).
[0m14:00:25.237579 [debug] [MainThread]: Command end result
[0m14:00:25.259143 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:00:25.260486 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:00:25.265509 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:00:25.265509 [info ] [MainThread]: 
[0m14:00:25.266509 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:00:25.266509 [info ] [MainThread]: 
[0m14:00:25.267550 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:00:25.267550 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:00:25.267550 [debug] [MainThread]: Command `dbt run` succeeded at 14:00:25.267550 after 11.24 seconds
[0m14:00:25.268873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F3381670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F1797C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028094177980>]}
[0m14:00:25.268873 [debug] [MainThread]: Flushing usage events
[0m14:00:26.282070 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:18:05.904959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A4BE0740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A4044050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A4046E40>]}


============================== 14:18:05.908964 | 2fb25e96-14ee-470c-8491-2d9a85740186 ==============================
[0m14:18:05.908964 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:18:05.908964 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m14:18:06.632146 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:18:06.632146 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:18:06.632146 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:18:07.278915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C7FE2A80>]}
[0m14:18:07.340764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A508C230>]}
[0m14:18:07.341803 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:18:07.592183 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:18:07.653006 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:18:07.653006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C82C9FD0>]}
[0m14:18:07.700785 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:18:07.700785 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:18:07.706305 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:18:07.725734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A4BE13A0>]}
[0m14:18:07.777715 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:18:07.779765 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:18:07.805553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C86F0800>]}
[0m14:18:07.806644 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:18:07.806644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C986FB30>]}
[0m14:18:07.808162 [info ] [MainThread]: 
[0m14:18:07.808162 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:18:07.808162 [info ] [MainThread]: 
[0m14:18:07.809181 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:07.809181 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:18:07.810180 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:07.810180 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:18:07.810180 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:18:07.810180 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:18:07.811276 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:08.094585 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ff077634-3aa6-4dfe-ad41-98aaafe6a0a7) - Created
[0m14:18:08.255973 [debug] [ThreadPool]: SQL status: OK in 0.440 seconds
[0m14:18:08.257126 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ff077634-3aa6-4dfe-ad41-98aaafe6a0a7, command-id=cbefa590-82c5-4075-a84d-1e6ca42bcb81) - Closing
[0m14:18:08.262625 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:08.262625 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:18:08.275873 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:18:08.276854 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:18:08.277904 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:08.467225 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=c27bdabc-2790-4587-b82a-847fa6d582c3) - Created
[0m14:18:08.933001 [debug] [ThreadPool]: SQL status: OK in 0.660 seconds
[0m14:18:08.940599 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c27bdabc-2790-4587-b82a-847fa6d582c3, command-id=5347609b-49fa-4cde-8512-e6e4f4a15f92) - Closing
[0m14:18:08.944137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C7DBF3E0>]}
[0m14:18:08.947939 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m14:18:08.947939 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m14:18:08.949084 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:08.949084 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m14:18:08.949084 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m14:18:08.959975 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m14:18:08.960977 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m14:18:08.987890 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:18:08.995644 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:18:08.995644 [debug] [Thread-1 (]: Safe create: False
[0m14:18:09.011211 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:18:09.012208 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:09.012208 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    try_cast(NULLIF(TRIM(MIN_AMOUNT),' ') AS int) as min_salary, 
    try_cast(NULLIF(TRIM(MAX_AMOUNT),' ') AS int) as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m14:18:09.012208 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:18:09.196851 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2) - Created
[0m14:18:09.714487 [debug] [Thread-1 (]: SQL status: OK in 0.700 seconds
[0m14:18:09.717302 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=c2bd87b1-e06b-4f1e-ae5d-b4fa5dab19a6) - Closing
[0m14:18:09.717302 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:18:09.729901 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:09.729901 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m14:18:09.940177 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:18:09.943224 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=d39770a1-2984-4c29-a7d2-83177f586e7f) - Closing
[0m14:18:09.948900 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:09.948900 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:18:10.251260 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:18:10.252263 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=981acf19-a5b2-43b1-a9d6-1427bc944015) - Closing
[0m14:18:10.256232 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:10.257736 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m14:18:10.612812 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:18:10.616549 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=66f3ba2c-0913-4145-b320-122c11501446) - Closing
[0m14:18:10.624180 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:10.624180 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:18:11.211783 [debug] [Thread-1 (]: SQL status: OK in 0.590 seconds
[0m14:18:11.215463 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=d611824d-70f9-4bff-b596-d788a16b1a97) - Closing
[0m14:18:11.220373 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:11.221381 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:18:12.289493 [debug] [Thread-1 (]: SQL status: OK in 1.070 seconds
[0m14:18:12.291508 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=341e5e88-5dbf-423b-b5f6-5e8c1c16d0dd) - Closing
[0m14:18:12.295842 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:12.295842 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:18:12.586086 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m14:18:12.589037 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=e1f05eff-d31c-4205-9ab9-8df453550f0d) - Closing
[0m14:18:12.595668 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:12.596725 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:18:12.931736 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m14:18:12.932749 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=c0d5f779-bcca-4c26-a330-a37450fb8080) - Closing
[0m14:18:12.936445 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:12.936445 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:18:13.392268 [debug] [Thread-1 (]: SQL status: OK in 0.450 seconds
[0m14:18:13.394270 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=af23aa10-f814-44e2-81db-c29e4dea9f5f) - Closing
[0m14:18:13.395706 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m14:18:13.401056 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m14:18:13.422083 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:13.422083 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m14:18:13.662868 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:18:13.664474 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=6399b4af-a494-4ca3-a564-5926a0294e8a) - Closing
[0m14:18:13.666580 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:13.666580 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m14:18:13.886747 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m14:18:13.888251 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=f2b4e6ec-a1ee-4391-b79b-4c78e85913e5) - Closing
[0m14:18:13.895825 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:18:13.896823 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:18:13.896823 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:18:18.323675 [debug] [Thread-1 (]: SQL status: OK in 4.430 seconds
[0m14:18:18.325245 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2, command-id=cf6f5f66-81d4-4e6c-a9e4-10c36c193949) - Closing
[0m14:18:18.344006 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fb25e96-14ee-470c-8491-2d9a85740186', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A20F4230>]}
[0m14:18:18.344006 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 9.39s]
[0m14:18:18.345523 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m14:18:18.346539 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=9.402401208877563s, language=None, compute-name=) - Reusing connection previously named master
[0m14:18:18.346539 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:18:18.348085 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:18:18.348085 [debug] [MainThread]: On list_demo: Close
[0m14:18:18.348085 [debug] [MainThread]: Databricks adapter: Connection(session-id=ff077634-3aa6-4dfe-ad41-98aaafe6a0a7) - Closing
[0m14:18:18.404115 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:18:18.405107 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:18:18.405107 [debug] [MainThread]: Databricks adapter: Connection(session-id=c27bdabc-2790-4587-b82a-847fa6d582c3) - Closing
[0m14:18:18.467895 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m14:18:18.469280 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m14:18:18.469280 [debug] [MainThread]: Databricks adapter: Connection(session-id=782b84c8-d704-4b65-9d4c-bb9625aad9e2) - Closing
[0m14:18:18.725327 [info ] [MainThread]: 
[0m14:18:18.726381 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.92 seconds (10.92s).
[0m14:18:18.726381 [debug] [MainThread]: Command end result
[0m14:18:18.747153 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:18:18.748164 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:18:18.752194 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:18:18.752194 [info ] [MainThread]: 
[0m14:18:18.753186 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:18:18.753186 [info ] [MainThread]: 
[0m14:18:18.754233 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:18:18.754233 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:18:18.755233 [debug] [MainThread]: Command `dbt run` succeeded at 14:18:18.755233 after 12.96 seconds
[0m14:18:18.755233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A4E8E8A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C873B7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254C832A330>]}
[0m14:18:18.755233 [debug] [MainThread]: Flushing usage events
[0m14:18:19.749558 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:18:41.643434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168E013590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168E011940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168E011970>]}


============================== 14:18:41.646430 | ecafc825-363a-40b8-b7f9-95158e800719 ==============================
[0m14:18:41.646430 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:18:41.647738 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_unified_test', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m14:18:42.358181 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:18:42.359179 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:18:42.359179 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:18:42.990458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168DA55640>]}
[0m14:18:43.050038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B0BE8470>]}
[0m14:18:43.050038 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:18:43.290173 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:18:43.347731 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:18:43.349028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021690E24530>]}
[0m14:18:43.396694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:18:43.397711 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:18:43.401952 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:18:43.421224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B18193A0>]}
[0m14:18:43.474248 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:18:43.476244 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:18:43.496333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B2D1BD70>]}
[0m14:18:43.497331 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:18:43.497331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B2BA87D0>]}
[0m14:18:43.498331 [info ] [MainThread]: 
[0m14:18:43.498331 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:18:43.499330 [info ] [MainThread]: 
[0m14:18:43.499330 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:43.499330 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:18:43.500328 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:43.500328 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:18:43.500328 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:18:43.501630 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:18:43.501630 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:43.743747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=33f31230-7689-4e36-a17c-e8d899958376) - Created
[0m14:18:43.876209 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m14:18:43.877212 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=33f31230-7689-4e36-a17c-e8d899958376, command-id=461ca33e-3b6c-4ee8-8bc3-f50e38deeb41) - Closing
[0m14:18:43.883787 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:43.883787 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:18:43.893387 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:18:43.893387 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:18:43.893387 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:18:44.101654 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=20f766e9-9a74-46e9-bb27-7410e30acd46) - Created
[0m14:18:44.518924 [debug] [ThreadPool]: SQL status: OK in 0.630 seconds
[0m14:18:44.523921 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=20f766e9-9a74-46e9-bb27-7410e30acd46, command-id=f3f845c3-e726-4d36-a413-0c463f8118be) - Closing
[0m14:18:44.527071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B2CC2570>]}
[0m14:18:44.530073 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m14:18:44.530073 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m14:18:44.531072 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:18:44.531072 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m14:18:44.531072 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m14:18:44.542952 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m14:18:44.543953 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m14:18:44.569969 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:18:44.577485 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:18:44.577485 [debug] [Thread-1 (]: Safe create: False
[0m14:18:44.591504 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:18:44.592504 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:44.593507 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m14:18:44.593507 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:18:44.789590 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3) - Created
[0m14:18:45.906441 [debug] [Thread-1 (]: SQL status: OK in 1.310 seconds
[0m14:18:45.906441 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=42e78f3b-8f42-4c20-9304-5fcc35b854e7) - Closing
[0m14:18:45.907512 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:18:45.917729 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:45.919731 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m14:18:46.201715 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m14:18:46.204992 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=5d49a289-29c6-4e09-85e9-a77f71683a5d) - Closing
[0m14:18:46.210145 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:46.210145 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:18:46.440173 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:18:46.441174 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=445d6326-3836-4548-a6ee-4a0da8015660) - Closing
[0m14:18:46.446171 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:46.447169 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m14:18:46.750453 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:18:46.754855 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=651a45db-b5c9-4875-9237-f7b666d9bc15) - Closing
[0m14:18:46.763022 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:46.763022 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:18:47.418375 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m14:18:47.419373 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=cd5c7a30-e695-4958-b11f-079dfcea1507) - Closing
[0m14:18:47.424570 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:47.425571 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:18:48.331443 [debug] [Thread-1 (]: SQL status: OK in 0.910 seconds
[0m14:18:48.335472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=eedcf47d-5dc2-42d4-9fb4-a0ebe74f531e) - Closing
[0m14:18:48.344463 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:48.344463 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:18:48.569712 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:18:48.570788 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=9f265bb4-758e-4c17-900e-38b5ce627f12) - Closing
[0m14:18:48.573787 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:48.573787 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:18:48.917029 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m14:18:48.919189 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=70d5e03b-d6e2-4f67-85b6-272bd746703e) - Closing
[0m14:18:48.922215 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:48.923204 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:18:49.453629 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m14:18:49.455642 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=aab82e2f-d451-4d8c-88f0-88fe69b646ca) - Closing
[0m14:18:49.456640 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m14:18:49.465646 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m14:18:49.486605 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:49.487553 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m14:18:49.852483 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:18:49.854482 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=b16159ac-4f74-46c5-bcfd-7a2fca587dfa) - Closing
[0m14:18:49.856474 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:49.856474 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m14:18:50.110869 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m14:18:50.113054 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3, command-id=57ac81de-39a4-460a-bf5f-9c9ab82074f6) - Closing
[0m14:18:50.121055 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:18:50.121055 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:18:50.122055 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:18:55.465290 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [CAST_INVALID_INPUT] org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:690)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
	at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
	at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
	at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
	at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
	at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
	at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:105)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1225)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1229)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1081)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1518)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1502)
	at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1715)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$15(TransactionalWriteEdge.scala:722)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
	at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
	at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
	at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
	at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:304)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
	at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:100)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
	at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
	at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
	at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
	at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
	at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
	at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:350)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: com.databricks.photon.PhotonException: Error class: InvalidInputSyntaxForNumericError. Parameters: double,
		at 0xc419010 <photon>.InvalidInputSyntaxForNumericError(external/workspace_spark_3_5/photon/common/status.cc:343)
		at 0x86d6719 <photon>.OnInvalidInput(external/workspace_spark_3_5/photon/exprs/cast-functions.h:673)
		at 0x86d626d <photon>.ProjectBatch(external/workspace_spark_3_5/photon/exprs/compute-function.h:1125)
		at 0x86d5b80 <photon>.Eval(external/workspace_spark_3_5/photon/exprs/unary-expr.h:120)
		at 0x6a4a574 <photon>.NextImpl(external/workspace_spark_3_5/photon/exec-nodes/project-node.cc:90)
		at 0x692bb0d <photon>.Next(external/workspace_spark_3_5/photon/exec-nodes/exec-node.cc:204)
		at 0x69ef5f2 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:205)
		at com.databricks.photon.JniApiImpl.open(Native Method)
		at com.databricks.photon.JniApi.open(JniApi.scala)
		at com.databricks.photon.JniExecNode.open(JniExecNode.java:73)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)
		at com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)
		at com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)
		at com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1225)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1229)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1081)
		... 3 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:188)
		at com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:205)
		at com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)
		at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
		at org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)
		at com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)
		at com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
		at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)
		at org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)
		at org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)
		at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
		at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
		at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)
		at scala.util.Using$.resource(Using.scala:269)
		at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)
		at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.scheduler.Task.run(Task.scala:105)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1225)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
		at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
		at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)
		at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1229)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
		at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1081)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at java.base/java.lang.Thread.run(Thread.java:840)
		at org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1518)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1502)
		at org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)
		at org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1715)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)
		at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$15(TransactionalWriteEdge.scala:722)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:837)
		at com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:821)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:209)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:634)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeFiles(MergeIntoCommandEdge.scala:598)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.$anonfun$writeOnlyInserts$1(InsertOnlyMergeExecutor.scala:133)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts(InsertOnlyMergeExecutor.scala:72)
		at com.databricks.sql.transaction.tahoe.commands.merge.InsertOnlyMergeExecutor.writeOnlyInserts$(InsertOnlyMergeExecutor.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.writeOnlyInserts(MergeIntoCommandEdge.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMergeInsertOnly$1(LowShuffleMergeExecutor.scala:219)
		at com.databricks.sql.transaction.tahoe.NoOpLowShuffleMergeExecutionObserver$.writeOnlyInserts(LowShuffleMergeExecutionObserver.scala:95)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMergeInsertOnly(LowShuffleMergeExecutor.scala:214)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:264)
		at com.databricks.sql.transaction.tahoe.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:223)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.super$runLowShuffleMerge(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMergeInternal$1(MergeIntoCommandEdge.scala:411)
		at com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMergeInternal(MergeIntoCommandEdge.scala:398)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2(MergeIntoCommandEdge.scala:347)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$2$adapted(MergeIntoCommandEdge.scala:300)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:304)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction(DeltaLogEdge.scala:156)
		at com.databricks.sql.transaction.tahoe.DeltaLogEdge.withExistingOrNewTransaction$(DeltaLogEdge.scala:137)
		at com.databricks.sql.transaction.tahoe.DeltaLog.withExistingOrNewTransaction(DeltaLog.scala:100)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$runMerge$1(MergeIntoCommandEdge.scala:300)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:449)
		at com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)
		at com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)
		at com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:448)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$8(MergeIntoCommandBase.scala:472)
		at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:455)
		at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
		at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withStatusCode(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:472)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.withOperationTypeTag(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordFrameProfile(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.recordDeltaOperation(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:469)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:425)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.runMerge(MergeIntoCommandEdge.scala:298)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:144)
		at com.databricks.sql.transaction.tahoe.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:126)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runWithMaterializedSourceLostRetries(MergeIntoCommandEdge.scala:67)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:202)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.runCommand(MergeIntoCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runFunc$1(DeltaDMLCommandEdge.scala:75)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.$anonfun$run$1(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession(DMLUtils.scala:128)
		at com.databricks.sql.transaction.tahoe.commands.RunWithClonedSparkSession.runWithClonedSparkSession$(DMLUtils.scala:121)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.runWithClonedSparkSession(DeltaDMLCommandEdge.scala:58)
		at com.databricks.sql.transaction.tahoe.commands.DeltaDMLCommandEdge.run(DeltaDMLCommandEdge.scala:94)
		at com.databricks.sql.transaction.tahoe.commands.MergeIntoWithDeltaDMLCommandEdge.run(MergeIntoCommandEdge.scala:72)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)
		at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:469)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:469)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:468)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:479)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:415)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:779)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:345)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:214)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:716)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:464)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:460)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:389)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:458)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:537)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:532)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:532)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:532)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:345)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=79294f31-c5b7-4255-8d7c-86d859e23dd1
[0m14:18:55.579337 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m14:18:55.581313 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ecafc825-363a-40b8-b7f9-95158e800719', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B3290200>]}
[0m14:18:55.581313 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 11.05s]
[0m14:18:55.582305 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m14:18:55.582305 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m14:18:55.583847 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=11.056775569915771s, language=None, compute-name=) - Reusing connection previously named master
[0m14:18:55.583847 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:18:55.584788 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:18:55.584788 [debug] [MainThread]: On list_demo: Close
[0m14:18:55.584788 [debug] [MainThread]: Databricks adapter: Connection(session-id=33f31230-7689-4e36-a17c-e8d899958376) - Closing
[0m14:18:55.670180 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:18:55.671183 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:18:55.671183 [debug] [MainThread]: Databricks adapter: Connection(session-id=20f766e9-9a74-46e9-bb27-7410e30acd46) - Closing
[0m14:18:55.742872 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m14:18:55.742872 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m14:18:55.743893 [debug] [MainThread]: Databricks adapter: Connection(session-id=b39fe736-a2be-4963-a7eb-a7a24c1b47d3) - Closing
[0m14:18:56.094588 [info ] [MainThread]: 
[0m14:18:56.096614 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 12.59 seconds (12.59s).
[0m14:18:56.099535 [debug] [MainThread]: Command end result
[0m14:18:56.124187 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:18:56.126158 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:18:56.130163 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:18:56.131215 [info ] [MainThread]: 
[0m14:18:56.132164 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:18:56.133158 [info ] [MainThread]: 
[0m14:18:56.133158 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m14:18:56.134532 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [CAST_INVALID_INPUT] The value '' of the type "STRING" cannot be cast to "DOUBLE" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m14:18:56.134532 [info ] [MainThread]: 
[0m14:18:56.135533 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m14:18:56.135533 [info ] [MainThread]: 
[0m14:18:56.135533 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m14:18:56.137023 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:18:56.137023 [debug] [MainThread]: Command `dbt run` failed at 14:18:56.137023 after 14.59 seconds
[0m14:18:56.138054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168DB87C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216B0F94C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002168F673F50>]}
[0m14:18:56.138054 [debug] [MainThread]: Flushing usage events
[0m14:18:57.093879 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:20:52.238489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B9D3C0500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B9D3C2180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B9D3C37D0>]}


============================== 14:20:52.243019 | 8e31f0b6-adfe-4a31-a82d-100e9b60a12e ==============================
[0m14:20:52.243019 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:20:52.243019 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m14:20:52.966761 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:20:52.967755 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:20:52.967755 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:20:53.605102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B9CD31E20>]}
[0m14:20:53.661436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC0B0B4D0>]}
[0m14:20:53.662435 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:20:53.889238 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:20:53.953812 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:20:53.954756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC1F12810>]}
[0m14:20:54.001611 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:20:54.002611 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m14:20:54.228536 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:20:54.237068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC22A3AA0>]}
[0m14:20:54.293677 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:20:54.294663 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:20:54.323869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC224FF20>]}
[0m14:20:54.323869 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:20:54.324873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC2277D40>]}
[0m14:20:54.325880 [info ] [MainThread]: 
[0m14:20:54.326219 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:20:54.326219 [info ] [MainThread]: 
[0m14:20:54.327253 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:20:54.327253 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:20:54.328258 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:20:54.328780 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:20:54.328780 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:20:54.328780 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:20:54.328780 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:20:54.611159 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=9e835e6f-8341-4b6e-a893-a7535601ec00) - Created
[0m14:20:54.740469 [debug] [ThreadPool]: SQL status: OK in 0.410 seconds
[0m14:20:54.741468 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9e835e6f-8341-4b6e-a893-a7535601ec00, command-id=f8f23416-484e-4a59-b553-78c45ecc6c5e) - Closing
[0m14:20:54.748119 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:20:54.748119 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:20:54.757130 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:20:54.757130 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:20:54.758131 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:20:54.952599 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=17e8ff85-f7c4-475d-b186-8efbda858835) - Created
[0m14:20:55.391654 [debug] [ThreadPool]: SQL status: OK in 0.630 seconds
[0m14:20:55.400870 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=17e8ff85-f7c4-475d-b186-8efbda858835, command-id=a53f3b15-6cab-49a2-ade5-1432b40517ef) - Closing
[0m14:20:55.404380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC22A3A10>]}
[0m14:20:55.409393 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m14:20:55.409393 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m14:20:55.410383 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:20:55.411381 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m14:20:55.411381 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m14:20:55.419380 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m14:20:55.420380 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m14:20:55.445391 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:20:55.452580 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:20:55.452580 [debug] [Thread-1 (]: Safe create: False
[0m14:20:55.466112 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:20:55.467378 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:55.467378 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m14:20:55.468388 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:20:55.684322 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926) - Created
[0m14:20:56.068669 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m14:20:56.070799 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=e59b2ee1-14f0-438a-a049-e906a2b02a62) - Closing
[0m14:20:56.071798 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:20:56.081742 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:56.083025 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m14:20:56.426500 [debug] [Thread-1 (]: SQL status: OK in 0.340 seconds
[0m14:20:56.430465 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=a4e8640f-aef9-4113-8b35-df1ab421b475) - Closing
[0m14:20:56.436802 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:56.437326 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:20:56.667101 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:20:56.673187 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=3c099ae9-9535-4f19-bada-941b9575877e) - Closing
[0m14:20:56.684168 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:56.684168 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m14:20:56.991282 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m14:20:56.994534 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=e1c463f3-ef4d-440c-9135-2cfd929aceff) - Closing
[0m14:20:57.000158 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:57.001156 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:20:57.483747 [debug] [Thread-1 (]: SQL status: OK in 0.480 seconds
[0m14:20:57.484747 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=66400f25-cd75-44c1-a901-992227cf5d61) - Closing
[0m14:20:57.489785 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:57.490785 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:20:58.335145 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m14:20:58.340389 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=aa90a79d-e17a-4060-9f7a-1a427cefe495) - Closing
[0m14:20:58.349766 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:58.349766 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:20:58.606521 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m14:20:58.608526 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=1a689842-2ce2-481c-9a4a-58057499aed7) - Closing
[0m14:20:58.611518 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:58.611518 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:20:59.029195 [debug] [Thread-1 (]: SQL status: OK in 0.420 seconds
[0m14:20:59.034125 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=62949b68-ff9e-4ba8-b634-fac41ae5eda2) - Closing
[0m14:20:59.039713 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:59.039713 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:20:59.716891 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m14:20:59.719651 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=40ff1945-d334-48ce-b9e7-5fbddc517607) - Closing
[0m14:20:59.720651 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m14:20:59.728818 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m14:20:59.750699 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:20:59.751703 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m14:21:00.061523 [debug] [Thread-1 (]: SQL status: OK in 0.310 seconds
[0m14:21:00.065087 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=b2980cfc-498b-4442-a59a-b1b123dba2a0) - Closing
[0m14:21:00.067307 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:21:00.067307 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m14:21:00.273572 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:21:00.275602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=35cc616a-6032-4504-879b-4bee6698ce1a) - Closing
[0m14:21:00.284051 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:21:00.285043 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:21:00.285043 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:21:03.987001 [debug] [Thread-1 (]: SQL status: OK in 3.700 seconds
[0m14:21:03.987991 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926, command-id=69fd5669-cdf8-4408-8e31-908f905e73c2) - Closing
[0m14:21:04.002037 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8e31f0b6-adfe-4a31-a82d-100e9b60a12e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC230CA40>]}
[0m14:21:04.003037 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 8.59s]
[0m14:21:04.003037 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m14:21:04.004036 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=8.599655628204346s, language=None, compute-name=) - Reusing connection previously named master
[0m14:21:04.005035 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:21:04.005035 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:21:04.005035 [debug] [MainThread]: On list_demo: Close
[0m14:21:04.005035 [debug] [MainThread]: Databricks adapter: Connection(session-id=9e835e6f-8341-4b6e-a893-a7535601ec00) - Closing
[0m14:21:04.084532 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:21:04.085602 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:21:04.085602 [debug] [MainThread]: Databricks adapter: Connection(session-id=17e8ff85-f7c4-475d-b186-8efbda858835) - Closing
[0m14:21:04.156540 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m14:21:04.156540 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m14:21:04.157549 [debug] [MainThread]: Databricks adapter: Connection(session-id=e9f431ad-31cf-4196-9a9d-46309ec07926) - Closing
[0m14:21:04.414219 [info ] [MainThread]: 
[0m14:21:04.414219 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.09 seconds (10.09s).
[0m14:21:04.415614 [debug] [MainThread]: Command end result
[0m14:21:04.437528 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:21:04.438527 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:21:04.442546 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:21:04.443536 [info ] [MainThread]: 
[0m14:21:04.443536 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:21:04.443536 [info ] [MainThread]: 
[0m14:21:04.444547 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:21:04.444547 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:21:04.445741 [debug] [MainThread]: Command `dbt run` succeeded at 14:21:04.444547 after 12.30 seconds
[0m14:21:04.445741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B9B341580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC1F39580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018BC0B0BD70>]}
[0m14:21:04.445741 [debug] [MainThread]: Flushing usage events
[0m14:21:05.409695 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:23:23.523215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023304EF32F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233064E0CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233064E2840>]}


============================== 14:23:23.527231 | 89dd9ece-8f99-4cd4-984e-651387ae8f91 ==============================
[0m14:23:23.527231 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:23:23.527231 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select indeed_cleaned', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m14:23:24.224716 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:23:24.224716 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:23:24.224716 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:23:24.880781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233283EEFF0>]}
[0m14:23:24.947357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002332665AEA0>]}
[0m14:23:24.948364 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:23:25.178933 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:23:25.260526 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:23:25.260526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023328A92ED0>]}
[0m14:23:25.308128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:23:25.308128 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:23:25.313128 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:23:25.332481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330539C230>]}
[0m14:23:25.383051 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:23:25.385065 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:23:25.418515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023329BA21B0>]}
[0m14:23:25.418515 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:23:25.420011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023329B53A70>]}
[0m14:23:25.421054 [info ] [MainThread]: 
[0m14:23:25.421054 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:23:25.421054 [info ] [MainThread]: 
[0m14:23:25.422201 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:23:25.422201 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:23:25.423241 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:23:25.423241 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:23:25.423241 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:23:25.424241 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:23:25.424241 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:23:25.647662 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=9464ea76-8261-41b7-8864-7f6da1db71bb) - Created
[0m14:23:25.790193 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m14:23:25.791184 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9464ea76-8261-41b7-8864-7f6da1db71bb, command-id=3a35de15-83d0-4391-b3e1-f5b9e5b12c71) - Closing
[0m14:23:25.797191 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:23:25.798117 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:23:25.810210 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:23:25.810210 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:23:25.810210 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:23:26.026747 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a77a69d6-d20b-4b49-aae6-00c980d773d6) - Created
[0m14:23:26.388277 [debug] [ThreadPool]: SQL status: OK in 0.580 seconds
[0m14:23:26.395581 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a77a69d6-d20b-4b49-aae6-00c980d773d6, command-id=3b864ee8-2bba-499d-8f55-988236c9d795) - Closing
[0m14:23:26.398587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002332870AD50>]}
[0m14:23:26.401818 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m14:23:26.401818 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m14:23:26.402962 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:23:26.402962 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m14:23:26.402962 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m14:23:26.412109 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m14:23:26.413109 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m14:23:26.439598 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:23:26.447783 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:23:26.448783 [debug] [Thread-1 (]: Safe create: False
[0m14:23:26.461813 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:23:26.461813 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:23:26.462842 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`
    
[0m14:23:26.462842 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:23:26.654274 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=16b1efed-f18c-4523-9a0e-23332b11af84) - Created
[0m14:23:26.992599 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m14:23:26.992599 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16b1efed-f18c-4523-9a0e-23332b11af84, command-id=18756451-0747-45db-9546-51eafd44fb6f) - Closing
[0m14:23:26.993654 [debug] [Thread-1 (]: No existing relation found
[0m14:23:27.000687 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:23:27.001687 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m14:23:27.190481 [debug] [Thread-1 (]: SQL status: OK in 0.190 seconds
[0m14:23:27.191481 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16b1efed-f18c-4523-9a0e-23332b11af84, command-id=4f70cfec-91f0-49ba-a323-dfaef49b7cca) - Closing
[0m14:23:27.221153 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:23:27.222153 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:23:27.223155 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
[0m14:23:28.271697 [debug] [Thread-1 (]: SQL status: OK in 1.050 seconds
[0m14:23:28.271697 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16b1efed-f18c-4523-9a0e-23332b11af84, command-id=b271f8d6-6003-42de-afcd-5b71f515e8ee) - Closing
[0m14:23:28.274640 [debug] [Thread-1 (]: Applying tags to relation None
[0m14:23:28.275678 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:23:28.275678 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    insert into `demo`.`demo_schema`.`indeed_cleaned` select * from `indeed_cleaned__dbt_tmp`
  
[0m14:23:30.462279 [debug] [Thread-1 (]: SQL status: OK in 2.190 seconds
[0m14:23:30.463347 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=16b1efed-f18c-4523-9a0e-23332b11af84, command-id=0a8f8d23-9dc2-4a11-829f-756d3561e866) - Closing
[0m14:23:30.479206 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '89dd9ece-8f99-4cd4-984e-651387ae8f91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330246FF80>]}
[0m14:23:30.479206 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 4.07s]
[0m14:23:30.480202 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m14:23:30.482226 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=4.08363938331604s, language=None, compute-name=) - Reusing connection previously named master
[0m14:23:30.482226 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:23:30.482226 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:23:30.483271 [debug] [MainThread]: On list_demo: Close
[0m14:23:30.483271 [debug] [MainThread]: Databricks adapter: Connection(session-id=9464ea76-8261-41b7-8864-7f6da1db71bb) - Closing
[0m14:23:30.555238 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:23:30.555238 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:23:30.556290 [debug] [MainThread]: Databricks adapter: Connection(session-id=a77a69d6-d20b-4b49-aae6-00c980d773d6) - Closing
[0m14:23:30.607763 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m14:23:30.608762 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m14:23:30.608762 [debug] [MainThread]: Databricks adapter: Connection(session-id=16b1efed-f18c-4523-9a0e-23332b11af84) - Closing
[0m14:23:30.816145 [info ] [MainThread]: 
[0m14:23:30.817175 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 5.39 seconds (5.39s).
[0m14:23:30.819163 [debug] [MainThread]: Command end result
[0m14:23:30.842079 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:23:30.844088 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:23:30.848690 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:23:30.848690 [info ] [MainThread]: 
[0m14:23:30.848690 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:23:30.849745 [info ] [MainThread]: 
[0m14:23:30.849745 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:23:30.850544 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:23:30.850544 [debug] [MainThread]: Command `dbt run` succeeded at 14:23:30.850544 after 7.43 seconds
[0m14:23:30.851553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233053BDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023328746390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000233064E2840>]}
[0m14:23:30.851553 [debug] [MainThread]: Flushing usage events
[0m14:23:31.841621 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:24:23.245650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611FDCEC90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611FDCEF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261204828D0>]}


============================== 14:24:23.250669 | cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab ==============================
[0m14:24:23.250669 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:24:23.250669 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select stg_unified_test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m14:24:23.960566 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:24:23.961533 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:24:23.961533 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:24:24.592040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002614387E810>]}
[0m14:24:24.651539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261235FAC00>]}
[0m14:24:24.652540 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:24:24.888589 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:24:24.946658 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:24:24.947635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261232E4530>]}
[0m14:24:24.995299 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:24:24.995299 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:24:25.000225 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:24:25.019695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026145138500>]}
[0m14:24:25.070440 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:24:25.071764 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:24:25.099437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026144000650>]}
[0m14:24:25.099437 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:24:25.100444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026145171010>]}
[0m14:24:25.101457 [info ] [MainThread]: 
[0m14:24:25.102008 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:24:25.102008 [info ] [MainThread]: 
[0m14:24:25.102008 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:24:25.102008 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:24:25.103531 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:24:25.103531 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:24:25.104557 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:24:25.104557 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:24:25.104557 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:25.345870 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=42342b69-9de3-4c01-af22-1b90c43e6117) - Created
[0m14:24:25.492614 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m14:24:25.494618 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=42342b69-9de3-4c01-af22-1b90c43e6117, command-id=4952f831-1e0c-4fbb-a2ec-3cd4b13e3400) - Closing
[0m14:24:25.508050 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:24:25.509017 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:24:25.519442 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:24:25.520488 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:24:25.520488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:24:25.738006 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=68c9422c-e194-45a2-8565-0407ea8864f6) - Created
[0m14:24:26.239331 [debug] [ThreadPool]: SQL status: OK in 0.720 seconds
[0m14:24:26.245098 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=68c9422c-e194-45a2-8565-0407ea8864f6, command-id=4a1fdf39-10a4-4af6-9339-f6071c7f2700) - Closing
[0m14:24:26.247618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261451710D0>]}
[0m14:24:26.250790 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m14:24:26.250790 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m14:24:26.252012 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:24:26.252012 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m14:24:26.252012 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m14:24:26.262321 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m14:24:26.263305 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m14:24:26.289801 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:24:26.297125 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:24:26.297125 [debug] [Thread-1 (]: Safe create: False
[0m14:24:26.311864 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:24:26.311864 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:26.312850 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m14:24:26.312850 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:26.511894 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=3106d09a-994f-4f57-881e-3e71edd019fb) - Created
[0m14:24:26.910166 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m14:24:26.910731 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=1864ab6b-2f9e-473c-bb46-010742559d4d) - Closing
[0m14:24:26.911804 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:24:26.922799 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:26.922799 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m14:24:27.195254 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:24:27.198488 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=d559b585-13bd-476d-87aa-04e6f226de2b) - Closing
[0m14:24:27.205290 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:27.205290 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:24:27.444653 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:24:27.447756 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=19ccb50b-1c29-4cf7-a4bc-9233124f3b72) - Closing
[0m14:24:27.456985 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:27.456985 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m14:24:27.741899 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m14:24:27.746330 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=c8005490-69f2-4409-b955-90221a8919f1) - Closing
[0m14:24:27.754790 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:27.755789 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:24:28.377175 [debug] [Thread-1 (]: SQL status: OK in 0.620 seconds
[0m14:24:28.383168 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=442a998a-0902-47c0-8bf2-56ec53854a55) - Closing
[0m14:24:28.388166 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:28.389332 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:24:29.151361 [debug] [Thread-1 (]: SQL status: OK in 0.760 seconds
[0m14:24:29.154464 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=6bb918b6-6bcb-4f33-9c8b-e6589a597d92) - Closing
[0m14:24:29.162489 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:29.163550 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:24:29.463196 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:24:29.469071 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=fc356273-fcb9-416c-aa0c-eb2339067fcd) - Closing
[0m14:24:29.472064 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:29.473055 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:24:29.820160 [debug] [Thread-1 (]: SQL status: OK in 0.350 seconds
[0m14:24:29.826158 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=5a4933be-4649-41f8-8ed6-075dbf0a2018) - Closing
[0m14:24:29.838221 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:29.839244 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:24:30.330100 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m14:24:30.335144 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=24e2f06b-5d14-4130-bc20-4623cdd7c04c) - Closing
[0m14:24:30.337148 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m14:24:30.343202 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m14:24:30.362906 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:30.362906 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m14:24:30.631874 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:24:30.633874 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=913b011c-d5a1-4e94-8e56-b2ffcfff2c23) - Closing
[0m14:24:30.635841 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:30.635841 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m14:24:30.865306 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:24:30.869383 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=4dbe7e61-cb57-408a-ad7b-ac8ee17f6ac8) - Closing
[0m14:24:30.884463 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:24:30.885462 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:24:30.885462 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:24:35.386180 [debug] [Thread-1 (]: SQL status: OK in 4.500 seconds
[0m14:24:35.386180 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=3106d09a-994f-4f57-881e-3e71edd019fb, command-id=1b9d89d5-380b-469c-a430-bc5db6d03305) - Closing
[0m14:24:35.399413 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd113dce-8fb5-4346-9bdd-ea6cf1caf3ab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026143C8BE30>]}
[0m14:24:35.399915 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 9.15s]
[0m14:24:35.401707 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m14:24:35.405744 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=9.157079935073853s, language=None, compute-name=) - Reusing connection previously named master
[0m14:24:35.406686 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:24:35.407691 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:24:35.407691 [debug] [MainThread]: On list_demo: Close
[0m14:24:35.408719 [debug] [MainThread]: Databricks adapter: Connection(session-id=42342b69-9de3-4c01-af22-1b90c43e6117) - Closing
[0m14:24:35.477479 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:24:35.478934 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:24:35.480052 [debug] [MainThread]: Databricks adapter: Connection(session-id=68c9422c-e194-45a2-8565-0407ea8864f6) - Closing
[0m14:24:35.535406 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m14:24:35.537399 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m14:24:35.538400 [debug] [MainThread]: Databricks adapter: Connection(session-id=3106d09a-994f-4f57-881e-3e71edd019fb) - Closing
[0m14:24:35.809648 [info ] [MainThread]: 
[0m14:24:35.811001 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.71 seconds (10.71s).
[0m14:24:35.815478 [debug] [MainThread]: Command end result
[0m14:24:35.843730 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:24:35.844730 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:24:35.848737 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:24:35.849731 [info ] [MainThread]: 
[0m14:24:35.849731 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:24:35.850731 [info ] [MainThread]: 
[0m14:24:35.851983 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:24:35.852990 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:24:35.852990 [debug] [MainThread]: Command `dbt run` succeeded at 14:24:35.852990 after 12.72 seconds
[0m14:24:35.853993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261208C21B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261202F45C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000261204AE810>]}
[0m14:24:35.853993 [debug] [MainThread]: Flushing usage events
[0m14:24:36.860547 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:25:16.007087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021968F932C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021968F900E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021968F92BD0>]}


============================== 14:25:16.011101 | fe34874a-7792-4f89-b0fb-6dabfd695cab ==============================
[0m14:25:16.011101 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:25:16.012088 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select stg_jobs_description_new', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m14:25:16.740206 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:25:16.740206 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:25:16.740206 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:25:17.370382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002196C0A4890>]}
[0m14:25:17.427724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190E042780>]}
[0m14:25:17.428783 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:25:17.657297 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:25:17.714567 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:25:17.715556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190E2F9E50>]}
[0m14:25:17.773496 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:17.773496 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:17.778487 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:25:17.798392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190C0C6F90>]}
[0m14:25:17.852058 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:25:17.854051 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:25:17.875986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190E604DD0>]}
[0m14:25:17.875986 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:25:17.877097 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190F737560>]}
[0m14:25:17.878098 [info ] [MainThread]: 
[0m14:25:17.878098 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:17.878098 [info ] [MainThread]: 
[0m14:25:17.879155 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:25:17.879155 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:25:17.880200 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:25:17.880200 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:25:17.880200 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:25:17.881202 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:25:17.881202 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:18.109518 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=500b8e51-0158-4a6f-9103-28b9b4a078f4) - Created
[0m14:25:18.248461 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m14:25:18.251406 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=500b8e51-0158-4a6f-9103-28b9b4a078f4, command-id=a7fd6cc4-e013-4aef-98f9-8f2bd713af3a) - Closing
[0m14:25:18.264293 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:25:18.265362 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:25:18.274359 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:25:18.276498 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:25:18.276498 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:18.485728 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=28e20a4c-a544-4f1c-a429-b535b4d22f7c) - Created
[0m14:25:18.820033 [debug] [ThreadPool]: SQL status: OK in 0.540 seconds
[0m14:25:18.833443 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=28e20a4c-a544-4f1c-a429-b535b4d22f7c, command-id=e29c2f9e-0608-4342-844f-e0d0eaf64e26) - Closing
[0m14:25:18.836442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190E23A330>]}
[0m14:25:18.841506 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m14:25:18.841506 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m14:25:18.842504 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_jobs_description_new, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:25:18.843460 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_jobs_description_new'
[0m14:25:18.843460 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m14:25:18.852832 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m14:25:18.853854 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m14:25:18.877434 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:25:18.885454 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:25:18.885454 [debug] [Thread-1 (]: Safe create: False
[0m14:25:18.898503 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m14:25:18.907100 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:18.907100 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`indeed_cleaned`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m14:25:18.908104 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:25:19.087748 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266) - Created
[0m14:25:19.593379 [debug] [Thread-1 (]: SQL status: OK in 0.680 seconds
[0m14:25:19.595375 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=ed835a33-5c52-4cee-bc4c-961a4f82435d) - Closing
[0m14:25:19.596384 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:25:19.612907 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:19.612907 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m14:25:19.821595 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:25:19.825602 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=701552c2-ba9b-4fef-8984-c75af225d14c) - Closing
[0m14:25:19.832037 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:19.832037 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m14:25:20.052515 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m14:25:20.054910 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=663f8d37-fb07-4e0f-a300-50ee8931c412) - Closing
[0m14:25:20.059526 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:20.059526 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m14:25:20.342193 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m14:25:20.347261 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=5ce40687-95bf-4a69-96c2-eb5973b14da4) - Closing
[0m14:25:20.360183 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:20.361185 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:25:20.886417 [debug] [Thread-1 (]: SQL status: OK in 0.530 seconds
[0m14:25:20.887422 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=53203d43-cb06-40a4-8b2b-0d8058700c16) - Closing
[0m14:25:20.893188 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:20.893188 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:25:21.584824 [debug] [Thread-1 (]: SQL status: OK in 0.690 seconds
[0m14:25:21.588057 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=2e53b8e7-984a-4f01-aeb8-7ffa5fa7a0e4) - Closing
[0m14:25:21.597601 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:21.598600 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m14:25:21.797584 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m14:25:21.802575 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=43de4fef-3b61-43c0-96c1-e4fd46e3bf70) - Closing
[0m14:25:21.810029 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:21.810029 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m14:25:22.514560 [debug] [Thread-1 (]: SQL status: OK in 0.700 seconds
[0m14:25:22.518643 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=55709bba-9747-47e2-8c43-2d7ff04ce2ce) - Closing
[0m14:25:22.524640 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:22.525628 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m14:25:23.136372 [debug] [Thread-1 (]: SQL status: OK in 0.610 seconds
[0m14:25:23.139383 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=14356482-ea6c-452a-9996-064dbbb7d0f3) - Closing
[0m14:25:23.140407 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m14:25:23.146416 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m14:25:23.176328 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:23.176328 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m14:25:23.481016 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:25:23.483082 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=9eeaff7b-3270-479d-a805-c0d7a45698af) - Closing
[0m14:25:23.488192 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:23.488192 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m14:25:23.673503 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m14:25:23.674502 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=2d6c172d-3d68-4573-8de6-a490dae0a068) - Closing
[0m14:25:23.683209 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m14:25:23.684210 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:25:23.684210 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:25:30.375035 [debug] [Thread-1 (]: SQL status: OK in 6.690 seconds
[0m14:25:30.377030 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266, command-id=6041c3dc-5635-4ea3-9684-ff7d580c0a5e) - Closing
[0m14:25:30.452453 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe34874a-7792-4f89-b0fb-6dabfd695cab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002196647FFB0>]}
[0m14:25:30.453450 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 11.61s]
[0m14:25:30.455443 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m14:25:30.458437 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=11.620969295501709s, language=None, compute-name=) - Reusing connection previously named master
[0m14:25:30.458437 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:25:30.458437 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:25:30.459440 [debug] [MainThread]: On list_demo: Close
[0m14:25:30.459440 [debug] [MainThread]: Databricks adapter: Connection(session-id=500b8e51-0158-4a6f-9103-28b9b4a078f4) - Closing
[0m14:25:30.514284 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:25:30.514284 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:25:30.515372 [debug] [MainThread]: Databricks adapter: Connection(session-id=28e20a4c-a544-4f1c-a429-b535b4d22f7c) - Closing
[0m14:25:30.571357 [debug] [MainThread]: Connection 'model.jra_dbt.stg_jobs_description_new' was properly closed.
[0m14:25:30.572374 [debug] [MainThread]: On model.jra_dbt.stg_jobs_description_new: Close
[0m14:25:30.573313 [debug] [MainThread]: Databricks adapter: Connection(session-id=ea8726fa-7848-44d7-9ac2-7a2a1e7b7266) - Closing
[0m14:25:30.812613 [info ] [MainThread]: 
[0m14:25:30.814561 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 12.93 seconds (12.93s).
[0m14:25:30.816552 [debug] [MainThread]: Command end result
[0m14:25:30.846680 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:25:30.848198 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:25:30.853643 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:25:30.853643 [info ] [MainThread]: 
[0m14:25:30.853643 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:25:30.854731 [info ] [MainThread]: 
[0m14:25:30.854731 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:25:30.854731 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:25:30.855843 [debug] [MainThread]: Command `dbt run` succeeded at 14:25:30.855843 after 14.97 seconds
[0m14:25:30.855843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002196945DE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002190F749280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002196943C770>]}
[0m14:25:30.855843 [debug] [MainThread]: Flushing usage events
[0m14:25:31.832576 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:28:04.653512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C098F770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C0866B40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288BE22BFB0>]}


============================== 14:28:04.656893 | 146db2e7-b5d7-407b-9b18-c1257467641a ==============================
[0m14:28:04.656893 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:28:04.657953 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m14:28:05.388235 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:28:05.388235 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:28:05.388235 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:28:06.008181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E3A8F050>]}
[0m14:28:06.064893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E31C23F0>]}
[0m14:28:06.064893 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:28:06.290611 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:28:06.346241 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:28:06.347222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E3D09CA0>]}
[0m14:28:06.394731 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:28:06.394731 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:28:06.399762 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:28:06.417886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E5143AA0>]}
[0m14:28:06.469258 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:28:06.470258 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:28:06.503722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E3FC84A0>]}
[0m14:28:06.503722 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:28:06.505104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E51A2AE0>]}
[0m14:28:06.506186 [info ] [MainThread]: 
[0m14:28:06.506186 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:28:06.507221 [info ] [MainThread]: 
[0m14:28:06.508184 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:28:06.508184 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:28:06.513152 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:28:06.514152 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:28:06.514152 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:28:06.514152 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:28:06.515179 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:06.721435 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=24ecb500-857e-464d-bb13-956c7af67c0e) - Created
[0m14:28:06.893502 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m14:28:06.895500 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=24ecb500-857e-464d-bb13-956c7af67c0e, command-id=a602a5c3-5091-4a4b-a829-ba8e4190f0df) - Closing
[0m14:28:06.898445 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:28:06.899466 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:28:06.908145 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:28:06.909138 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:28:06.909138 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:28:07.108382 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=9f1b677e-8d8f-4eb9-bd91-3e123dd8d7a8) - Created
[0m14:28:07.501077 [debug] [ThreadPool]: SQL status: OK in 0.590 seconds
[0m14:28:07.507671 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=9f1b677e-8d8f-4eb9-bd91-3e123dd8d7a8, command-id=e08f6f96-b2dc-4ac3-a0e0-e1d5ba74ad24) - Closing
[0m14:28:07.510108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C35C9400>]}
[0m14:28:07.517145 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m14:28:07.518154 [info ] [Thread-1 (]: 1 of 6 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m14:28:07.518912 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:28:07.518912 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m14:28:07.520083 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m14:28:07.530108 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m14:28:07.531164 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m14:28:07.557902 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:07.565164 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:07.565164 [debug] [Thread-1 (]: Safe create: False
[0m14:28:07.577253 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:28:07.578288 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:07.578288 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`indeed_cleaned`
    )
    
[0m14:28:07.579221 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:28:07.769576 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=0c90983d-0570-4aac-ae2e-45705679aace) - Created
[0m14:28:08.163049 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m14:28:08.164385 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=22a17a27-86e5-4b78-8712-8db7db065dc9) - Closing
[0m14:28:08.165450 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:28:08.181819 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:08.181819 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned'
  
[0m14:28:08.421532 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:28:08.424505 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=b443f1bb-c8d4-4ebe-8d70-791e5a15ec8f) - Closing
[0m14:28:08.433005 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:08.433005 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:28:08.663616 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:28:08.668686 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=dd766691-b157-4329-aef9-cee9396b70a1) - Closing
[0m14:28:08.680651 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:08.680651 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned'
    AND is_nullable = 'NO';
  
[0m14:28:08.977668 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:28:08.982274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=e1936d32-8fee-4314-87b6-288cb156fcaf) - Closing
[0m14:28:08.989279 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:08.989279 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:09.456227 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m14:28:09.458993 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=2121ab41-23a8-4761-a463-061045b9ea48) - Closing
[0m14:28:09.466766 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:09.467770 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_cleaned'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_cleaned'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:10.291410 [debug] [Thread-1 (]: SQL status: OK in 0.820 seconds
[0m14:28:10.292415 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=50ae9225-de02-4560-9f15-b3ca90696526) - Closing
[0m14:28:10.296416 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:10.297430 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_cleaned';
  
[0m14:28:10.592993 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:28:10.594985 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=65a41464-aeaf-40b5-8fae-b5cdd2bf135c) - Closing
[0m14:28:10.597981 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:10.597981 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:28:11.039926 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m14:28:11.041475 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=1cc4ed0c-0c4c-4f4b-9884-a2ca04837a26) - Closing
[0m14:28:11.044136 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:11.044650 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
describe extended `demo`.`demo_schema`.`indeed_cleaned`
  
[0m14:28:11.488549 [debug] [Thread-1 (]: SQL status: OK in 0.440 seconds
[0m14:28:11.492889 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=79046bf8-d18b-4da1-ab2a-98876484833f) - Closing
[0m14:28:11.495891 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m14:28:11.506315 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_cleaned`
[0m14:28:11.526479 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:11.526479 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_cleaned`

  
[0m14:28:11.794974 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:28:11.801085 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=dde6e4ea-3625-4acd-b5f9-8eb90867dd58) - Closing
[0m14:28:11.807815 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:11.807815 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m14:28:12.183580 [debug] [Thread-1 (]: SQL status: OK in 0.370 seconds
[0m14:28:12.189274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=1bf37734-a8a0-4cc4-be8e-47fe976abf84) - Closing
[0m14:28:12.203620 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m14:28:12.204698 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m14:28:12.204698 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_cleaned` as DBT_INTERNAL_DEST
    using
        `indeed_cleaned__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:28:15.553027 [debug] [Thread-1 (]: SQL status: OK in 3.350 seconds
[0m14:28:15.556034 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=e945a7e7-1e59-4a5e-95d0-554a2f168c94) - Closing
[0m14:28:15.570732 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288BDA0FE60>]}
[0m14:28:15.570732 [info ] [Thread-1 (]: 1 of 6 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 8.05s]
[0m14:28:15.572003 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m14:28:15.572003 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m14:28:15.574306 [info ] [Thread-1 (]: 2 of 6 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m14:28:15.576146 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_cleaned, now model.jra_dbt.indeed_test)
[0m14:28:15.577144 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0c90983d-0570-4aac-ae2e-45705679aace, name=model.jra_dbt.indeed_test, idle-time=0.007124185562133789s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_cleaned
[0m14:28:15.577144 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m14:28:15.583139 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m14:28:15.584138 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m14:28:15.586378 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:15.587413 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:15.587413 [debug] [Thread-1 (]: Safe create: False
[0m14:28:15.588413 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m14:28:15.588413 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:15.589433 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`indeed_test`
    )
    
[0m14:28:16.188237 [debug] [Thread-1 (]: SQL status: OK in 0.600 seconds
[0m14:28:16.190277 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=9c8ab9bf-aea2-401f-946b-63f20f3f44ba) - Closing
[0m14:28:16.190277 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:28:16.192218 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:16.192218 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test'
  
[0m14:28:16.421407 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:28:16.426477 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=3d536529-c234-4467-9a8b-9eda26dbbd62) - Closing
[0m14:28:16.428807 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:16.429838 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m14:28:16.656169 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:28:16.659169 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=f5796782-20ed-481b-9c8e-e6dfed58e0e8) - Closing
[0m14:28:16.661169 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:16.661169 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test'
    AND is_nullable = 'NO';
  
[0m14:28:16.934475 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:28:16.936464 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=8c5fe74e-f5d9-4b84-8e7c-16049799337b) - Closing
[0m14:28:16.939970 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:16.939970 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:17.429995 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m14:28:17.431184 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=d924aa3d-a3b9-4b15-a11e-4be11380cdb1) - Closing
[0m14:28:17.432184 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:17.432184 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'indeed_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'indeed_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:18.264840 [debug] [Thread-1 (]: SQL status: OK in 0.830 seconds
[0m14:28:18.266837 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=508bfc79-7fef-433a-8ec2-04dfd0fae8c6) - Closing
[0m14:28:18.267873 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:18.268848 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'indeed_test';
  
[0m14:28:18.631019 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:28:18.634029 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=171a894f-df82-4fd1-a0d1-76a8ecc0e78e) - Closing
[0m14:28:18.636026 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:18.636026 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`indeed_test`
  
[0m14:28:19.037255 [debug] [Thread-1 (]: SQL status: OK in 0.400 seconds
[0m14:28:19.042190 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=5ae30c0b-cda6-4a10-9304-6b592e4bbb0a) - Closing
[0m14:28:19.048189 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:19.049188 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
describe extended `demo`.`demo_schema`.`indeed_test`
  
[0m14:28:19.437363 [debug] [Thread-1 (]: SQL status: OK in 0.390 seconds
[0m14:28:19.439364 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=0c3fdd47-3714-49b9-b1a6-3ee0b90f2241) - Closing
[0m14:28:19.440363 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'min_amount': '', 'max_amount': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m14:28:19.440363 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`indeed_test`
[0m14:28:19.491195 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:19.491195 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`indeed_test`

  
[0m14:28:19.732814 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:28:19.737866 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=f554fa6a-5de0-4c9a-b44a-c491d2b1e9df) - Closing
[0m14:28:19.740886 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:19.741886 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m14:28:19.953029 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:28:19.957972 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=6144ad52-d080-4d0a-9bbd-d12a5676dfd7) - Closing
[0m14:28:19.960852 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m14:28:19.962926 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m14:28:19.962926 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`indeed_test` as DBT_INTERNAL_DEST
    using
        `indeed_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:28:23.699421 [debug] [Thread-1 (]: SQL status: OK in 3.730 seconds
[0m14:28:23.700419 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=de72a11d-12ae-4c62-8275-738ee23493f6) - Closing
[0m14:28:23.702417 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E52CEB70>]}
[0m14:28:23.702417 [info ] [Thread-1 (]: 2 of 6 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 8.13s]
[0m14:28:23.703426 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m14:28:23.703426 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_cleaned_test
[0m14:28:23.704417 [info ] [Thread-1 (]: 3 of 6 START sql incremental model demo_schema.stg_cleaned_test ................ [RUN]
[0m14:28:23.706423 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.indeed_test, now model.jra_dbt.stg_cleaned_test)
[0m14:28:23.706423 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0c90983d-0570-4aac-ae2e-45705679aace, name=model.jra_dbt.stg_cleaned_test, idle-time=0.005005836486816406s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.indeed_test
[0m14:28:23.706423 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_cleaned_test
[0m14:28:23.711682 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_cleaned_test"
[0m14:28:23.714697 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_cleaned_test
[0m14:28:23.716639 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:23.716639 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:23.716639 [debug] [Thread-1 (]: Safe create: False
[0m14:28:23.717673 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_cleaned_test"
[0m14:28:23.719876 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_cleaned_test"
[0m14:28:23.719876 [debug] [Thread-1 (]: On model.jra_dbt.stg_cleaned_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

      create or replace temporary view `stg_cleaned_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`stg_test2`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_cleaned_test`
    )
    
[0m14:28:23.934519 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_cleaned_test"} */

      create or replace temporary view `stg_cleaned_test__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`stg_test2`


    WHERE timestamp > (
        SELECT COALESCE(MAX(ingest_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_cleaned_test`
    )
    
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:690)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=3212705d-a62b-4265-870e-d9e77d91c681
[0m14:28:23.945617 [debug] [Thread-1 (]: Database Error in model stg_cleaned_test (models\test\stg_cleaned_test.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
  compiled code at target\run\jra_dbt\models\test\stg_cleaned_test.sql
[0m14:28:23.946602 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E51BFD70>]}
[0m14:28:23.947604 [error] [Thread-1 (]: 3 of 6 ERROR creating sql incremental model demo_schema.stg_cleaned_test ....... [[31mERROR[0m in 0.24s]
[0m14:28:23.950630 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_cleaned_test
[0m14:28:23.951544 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_jobs_description_new
[0m14:28:23.952598 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_cleaned_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_cleaned_test (models\test\stg_cleaned_test.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
  compiled code at target\run\jra_dbt\models\test\stg_cleaned_test.sql.
[0m14:28:23.951544 [info ] [Thread-1 (]: 4 of 6 START sql incremental model demo_schema.stg_jobs_description_new ........ [RUN]
[0m14:28:23.954115 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_cleaned_test, now model.jra_dbt.stg_jobs_description_new)
[0m14:28:23.954115 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0c90983d-0570-4aac-ae2e-45705679aace, name=model.jra_dbt.stg_jobs_description_new, idle-time=0.0075130462646484375s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_cleaned_test
[0m14:28:23.955359 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_jobs_description_new
[0m14:28:23.960488 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_jobs_description_new"
[0m14:28:23.960488 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_jobs_description_new
[0m14:28:23.962449 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:23.963448 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:23.963448 [debug] [Thread-1 (]: Safe create: False
[0m14:28:23.963448 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m14:28:23.964487 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:23.964487 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

      create or replace temporary view `stg_jobs_description_new__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`stg_cleaned_test`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`demo_schema`.`indeed_cleaned`


    WHERE scraped_dts > (
        SELECT COALESCE(MAX(scraped_dts), '1900-01-01'::timestamp)
        FROM `demo`.`demo_schema`.`stg_jobs_description_new`
    )
    
[0m14:28:24.266258 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:28:24.269261 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=dc41f1fd-8251-4913-9140-22c8c8686cca) - Closing
[0m14:28:24.270264 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:28:24.274508 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:24.274508 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
  
[0m14:28:24.514339 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:28:24.517444 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=904f2490-733a-40d0-9d16-f05773ed9c89) - Closing
[0m14:28:24.522841 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:24.522841 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m14:28:24.749219 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:28:24.750220 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=6743dda1-ede9-4ff6-8c07-d3a81b9fe1a9) - Closing
[0m14:28:24.754964 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:24.754964 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new'
    AND is_nullable = 'NO';
  
[0m14:28:25.030583 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m14:28:25.033935 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=b4bd96d7-0826-4edc-a677-abd7f18bf034) - Closing
[0m14:28:25.037957 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:25.037957 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:25.526685 [debug] [Thread-1 (]: SQL status: OK in 0.490 seconds
[0m14:28:25.528983 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=2fa792af-7ad7-4d67-b518-2a1226b2b336) - Closing
[0m14:28:25.532978 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:25.533978 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_jobs_description_new'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_jobs_description_new'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:26.178030 [debug] [Thread-1 (]: SQL status: OK in 0.640 seconds
[0m14:28:26.181068 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=49eed5a1-444c-42f2-99c7-3fab88af3274) - Closing
[0m14:28:26.186146 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:26.186146 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_jobs_description_new';
  
[0m14:28:26.402467 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:28:26.405495 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=64490268-9f23-4abd-ae02-de5aa7af147c) - Closing
[0m14:28:26.412068 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:26.413374 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m14:28:26.732699 [debug] [Thread-1 (]: SQL status: OK in 0.320 seconds
[0m14:28:26.736964 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=93d2b29b-aaba-4a60-93d3-95528d057f56) - Closing
[0m14:28:26.740977 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:26.740977 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
describe extended `demo`.`demo_schema`.`stg_jobs_description_new`
  
[0m14:28:27.170447 [debug] [Thread-1 (]: SQL status: OK in 0.430 seconds
[0m14:28:27.174429 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=3bc7ba5a-6b83-429e-a13b-1094ba4c959d) - Closing
[0m14:28:27.175474 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'job_posting_id': '', 'job_summary': '', 'scraped_dts': '', '': ''} quoted={} persist=False
[0m14:28:27.175474 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_jobs_description_new`
[0m14:28:27.179438 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:27.179438 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_jobs_description_new`

  
[0m14:28:27.477446 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:28:27.479472 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=fe85fee3-6238-461a-a7de-bbbc6cb2a3e9) - Closing
[0m14:28:27.482495 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:27.482495 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */

    
DESCRIBE TABLE `stg_jobs_description_new__dbt_tmp`

  
[0m14:28:27.683705 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m14:28:27.687701 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=a5897398-e38a-4c5f-9cde-d96a1af1279b) - Closing
[0m14:28:27.689702 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_jobs_description_new"
[0m14:28:27.690697 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_jobs_description_new"
[0m14:28:27.690697 [debug] [Thread-1 (]: On model.jra_dbt.stg_jobs_description_new: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_jobs_description_new"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_jobs_description_new` as DBT_INTERNAL_DEST
    using
        `stg_jobs_description_new__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:28:33.111269 [debug] [Thread-1 (]: SQL status: OK in 5.420 seconds
[0m14:28:33.112261 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=af9500e4-5c04-4902-9d78-b4baba6a8294) - Closing
[0m14:28:33.176115 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E52CD5B0>]}
[0m14:28:33.178114 [info ] [Thread-1 (]: 4 of 6 OK created sql incremental model demo_schema.stg_jobs_description_new ... [[32mOK[0m in 9.22s]
[0m14:28:33.180123 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_jobs_description_new
[0m14:28:33.180123 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_test2
[0m14:28:33.182116 [info ] [Thread-1 (]: 5 of 6 START sql incremental model demo_schema.stg_test2 ....................... [RUN]
[0m14:28:33.183062 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_jobs_description_new, now model.jra_dbt.stg_test2)
[0m14:28:33.184059 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0c90983d-0570-4aac-ae2e-45705679aace, name=model.jra_dbt.stg_test2, idle-time=0.008940696716308594s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_jobs_description_new
[0m14:28:33.185060 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_test2
[0m14:28:33.193174 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_test2"
[0m14:28:33.195780 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_test2
[0m14:28:33.199815 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:33.200876 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:33.200876 [debug] [Thread-1 (]: Safe create: False
[0m14:28:33.201838 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m14:28:33.202858 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:28:33.203807 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

      create or replace temporary view `stg_test2__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)
    
[0m14:28:33.867017 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m14:28:33.868014 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=54270b7f-76f9-4868-92b4-25d923510e0e) - Closing
[0m14:28:33.869013 [debug] [Thread-1 (]: No existing relation found
[0m14:28:33.884479 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:28:33.885489 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `stg_test2__dbt_tmp`

  
[0m14:28:34.104800 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m14:28:34.108677 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=aaab88fe-a9f0-4ef8-a231-edcb08b6f329) - Closing
[0m14:28:34.136735 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m14:28:34.136735 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:28:34.137726 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
  create or replace table `demo`.`demo_schema`.`stg_test2`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      base_salary struct<currency:string,max_amount:double,min_amount:double,payment_period:string>,
    
      timestamp timestamp,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/stg_test2'
  
  

  
[0m14:28:34.897586 [debug] [Thread-1 (]: SQL status: OK in 0.760 seconds
[0m14:28:34.900533 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=7585e42a-ea6b-48fa-86aa-22f23539fef5) - Closing
[0m14:28:34.907081 [debug] [Thread-1 (]: Applying tags to relation None
[0m14:28:34.907081 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:28:34.908083 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    insert into `demo`.`demo_schema`.`stg_test2` select * from `stg_test2__dbt_tmp`
  
[0m14:28:38.553489 [debug] [Thread-1 (]: SQL status: OK in 3.640 seconds
[0m14:28:38.553489 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=d0db5241-6bbf-4e4a-9a89-301679d171cd) - Closing
[0m14:28:38.554490 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288E3D6F290>]}
[0m14:28:38.555493 [info ] [Thread-1 (]: 5 of 6 OK created sql incremental model demo_schema.stg_test2 .................. [[32mOK[0m in 5.37s]
[0m14:28:38.556510 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_test2
[0m14:28:38.556510 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m14:28:38.556510 [info ] [Thread-1 (]: 6 of 6 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m14:28:38.557491 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.jra_dbt.stg_test2, now model.jra_dbt.stg_unified_test)
[0m14:28:38.558437 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=0c90983d-0570-4aac-ae2e-45705679aace, name=model.jra_dbt.stg_unified_test, idle-time=0.003947257995605469s, language=sql, compute-name=) - Reusing connection previously named model.jra_dbt.stg_test2
[0m14:28:38.559492 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m14:28:38.568493 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m14:28:38.569448 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m14:28:38.571435 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:28:38.572464 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:28:38.572464 [debug] [Thread-1 (]: Safe create: False
[0m14:28:38.573641 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:28:38.573641 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:38.574710 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m14:28:39.047355 [debug] [Thread-1 (]: SQL status: OK in 0.470 seconds
[0m14:28:39.048466 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=1ddf2310-f9e2-4d47-a377-a48136897a8f) - Closing
[0m14:28:39.048466 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:28:39.050465 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:39.050465 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test'
  
[0m14:28:39.270901 [debug] [Thread-1 (]: SQL status: OK in 0.220 seconds
[0m14:28:39.274929 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=52b34844-d4aa-4a50-8343-6fb7c367a828) - Closing
[0m14:28:39.278089 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:39.279019 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:28:39.484265 [debug] [Thread-1 (]: SQL status: OK in 0.210 seconds
[0m14:28:39.487255 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=930a2f0e-d7fb-4a18-bad4-03e7cc04760a) - Closing
[0m14:28:39.490213 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:39.490213 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test'
    AND is_nullable = 'NO';
  
[0m14:28:39.751159 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m14:28:39.753149 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=cecf234f-4e0e-48a6-bd04-cd88c7f62d4e) - Closing
[0m14:28:39.757665 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:39.757665 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:40.383025 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m14:28:40.384704 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=bfaee313-929f-4030-b16e-e654221fa28f) - Closing
[0m14:28:40.385856 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:40.386410 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_unified_test'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_unified_test'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:28:41.291707 [debug] [Thread-1 (]: SQL status: OK in 0.900 seconds
[0m14:28:41.295678 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=761afba2-6e68-4745-969a-b63015285937) - Closing
[0m14:28:41.299664 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:41.299664 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_unified_test';
  
[0m14:28:41.534881 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:28:41.537273 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=25edde22-009d-4377-8730-1da489e6d6d3) - Closing
[0m14:28:41.539445 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:41.539445 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:28:41.897925 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:28:41.901014 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=e6eaac9d-4ffa-489c-874c-f5b5e93d0276) - Closing
[0m14:28:41.902006 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:41.902006 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
describe extended `demo`.`demo_schema`.`stg_unified_test`
  
[0m14:28:42.383361 [debug] [Thread-1 (]: SQL status: OK in 0.480 seconds
[0m14:28:42.385473 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=b790fc82-0fda-47e4-be4b-262dd4f36158) - Closing
[0m14:28:42.385473 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'min_salary': '', 'max_salary': '', 'job_posted_date': '', 'scraped_dts': '', 'ingest_dts': '', '_rescued_data': '', 'is_enriched': '', 'job_source': '', 'is_active': '', '': ''} quoted={} persist=False
[0m14:28:42.386472 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_unified_test`
[0m14:28:42.387471 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:42.388472 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_unified_test`

  
[0m14:28:42.655764 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:28:42.659271 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=ec55b4f4-89bd-4f8e-932d-b4f0e82405dc) - Closing
[0m14:28:42.664329 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:42.664329 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m14:28:42.867314 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m14:28:42.872348 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=fe9a8cd0-8dd4-4660-9995-4f0a4b492ac4) - Closing
[0m14:28:42.875394 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:28:42.876403 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:28:42.877344 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_unified_test` as DBT_INTERNAL_DEST
    using
        `stg_unified_test__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:28:46.593240 [debug] [Thread-1 (]: SQL status: OK in 3.710 seconds
[0m14:28:46.594299 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0c90983d-0570-4aac-ae2e-45705679aace, command-id=1a6618da-d061-4e1b-9784-0ef3fa8074c7) - Closing
[0m14:28:46.595283 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '146db2e7-b5d7-407b-9b18-c1257467641a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C35C9400>]}
[0m14:28:46.596436 [info ] [Thread-1 (]: 6 of 6 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 8.04s]
[0m14:28:46.597734 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m14:28:46.597734 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=39.0876259803772s, language=None, compute-name=) - Reusing connection previously named master
[0m14:28:46.598743 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:28:46.598743 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:28:46.598743 [debug] [MainThread]: On list_demo: Close
[0m14:28:46.598743 [debug] [MainThread]: Databricks adapter: Connection(session-id=24ecb500-857e-464d-bb13-956c7af67c0e) - Closing
[0m14:28:46.665358 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:28:46.666537 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:28:46.666537 [debug] [MainThread]: Databricks adapter: Connection(session-id=9f1b677e-8d8f-4eb9-bd91-3e123dd8d7a8) - Closing
[0m14:28:46.721170 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m14:28:46.722186 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m14:28:46.722186 [debug] [MainThread]: Databricks adapter: Connection(session-id=0c90983d-0570-4aac-ae2e-45705679aace) - Closing
[0m14:28:47.856439 [info ] [MainThread]: 
[0m14:28:47.857671 [info ] [MainThread]: Finished running 6 incremental models in 0 hours 0 minutes and 41.35 seconds (41.35s).
[0m14:28:47.860427 [debug] [MainThread]: Command end result
[0m14:28:47.893093 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:28:47.895089 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:28:47.899167 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:28:47.899167 [info ] [MainThread]: 
[0m14:28:47.899167 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:28:47.900177 [info ] [MainThread]: 
[0m14:28:47.901145 [error] [MainThread]: [31mFailure in model stg_cleaned_test (models\test\stg_cleaned_test.sql)[0m
[0m14:28:47.902142 [error] [MainThread]:   Database Error in model stg_cleaned_test (models\test\stg_cleaned_test.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`demo_schema`.`stg_test2` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 29 pos 5
  compiled code at target\run\jra_dbt\models\test\stg_cleaned_test.sql
[0m14:28:47.903143 [info ] [MainThread]: 
[0m14:28:47.903143 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_cleaned_test.sql
[0m14:28:47.903143 [info ] [MainThread]: 
[0m14:28:47.904290 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m14:28:47.904290 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:28:47.905395 [debug] [MainThread]: Command `dbt run` failed at 14:28:47.905395 after 43.36 seconds
[0m14:28:47.906704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C0806000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C05126C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288C0511640>]}
[0m14:28:47.906704 [debug] [MainThread]: Flushing usage events
[0m14:28:49.114760 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:29:04.015897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F394921E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F397DFDA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F39490380>]}


============================== 14:29:04.019411 | 74691ff1-48e1-4057-bdb1-6b9a2e69c599 ==============================
[0m14:29:04.019411 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:29:04.020428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_test2', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m14:29:04.733489 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:29:04.734488 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:29:04.734488 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:29:05.355572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5CC7BDA0>]}
[0m14:29:05.411649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5C0D9D30>]}
[0m14:29:05.412648 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:29:05.642309 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:29:05.698245 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:29:05.699196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F3AAA7CE0>]}
[0m14:29:05.745871 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:29:05.746873 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:29:05.750811 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:29:05.770282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5E11B8C0>]}
[0m14:29:05.822197 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:29:05.824847 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:29:05.862301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5CF7A5D0>]}
[0m14:29:05.862301 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:29:05.863219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5E160DA0>]}
[0m14:29:05.864278 [info ] [MainThread]: 
[0m14:29:05.864278 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:29:05.864278 [info ] [MainThread]: 
[0m14:29:05.865296 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:29:05.865296 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:29:05.866251 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:29:05.866251 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:29:05.866251 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:29:05.867228 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:29:05.867228 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:06.103314 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=1269d913-d04d-467a-b2f4-b344d3820b95) - Created
[0m14:29:06.265706 [debug] [ThreadPool]: SQL status: OK in 0.400 seconds
[0m14:29:06.268703 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1269d913-d04d-467a-b2f4-b344d3820b95, command-id=ad18a41a-bcae-4b58-a605-0788a4330c32) - Closing
[0m14:29:06.280252 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:29:06.281242 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:29:06.293458 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:29:06.293458 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:29:06.294458 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:29:06.484599 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ead46d80-b20c-44e8-ba73-3bc3fae70b2c) - Created
[0m14:29:06.783431 [debug] [ThreadPool]: SQL status: OK in 0.490 seconds
[0m14:29:06.795570 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ead46d80-b20c-44e8-ba73-3bc3fae70b2c, command-id=ba6254be-f6ee-42af-bcbb-3c09e3f157be) - Closing
[0m14:29:06.798509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5CED88F0>]}
[0m14:29:06.802505 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_test2
[0m14:29:06.803565 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_test2 ....................... [RUN]
[0m14:29:06.804618 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_test2, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:29:06.804618 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_test2'
[0m14:29:06.805544 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_test2
[0m14:29:06.815904 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_test2"
[0m14:29:06.816904 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_test2
[0m14:29:06.841585 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:29:06.848625 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:29:06.848625 [debug] [Thread-1 (]: Safe create: False
[0m14:29:06.861137 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m14:29:06.862170 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:06.862170 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

      create or replace temporary view `stg_test2__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)


    -- Only read files modified since last run
    WHERE timestamp > (
        SELECT COALESCE(MAX(timestamp), '1900-01-01'::timestamp) 
        FROM `demo`.`demo_schema`.`stg_test2`
    )
    
[0m14:29:06.863107 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:29:07.055346 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7) - Created
[0m14:29:07.709944 [debug] [Thread-1 (]: SQL status: OK in 0.850 seconds
[0m14:29:07.712316 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=4eb37ffb-a203-4998-93fb-b631d8fed983) - Closing
[0m14:29:07.713292 [debug] [Thread-1 (]: Existing relation found, proceeding with incremental work
[0m14:29:07.727657 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:07.727657 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT tag_name, tag_value
  FROM `system`.`information_schema`.`table_tags`
  WHERE catalog_name = 'demo' 
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2'
  
[0m14:29:07.927140 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m14:29:07.931198 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=eaf88b84-db4e-47f0-aba1-8ab907eba6ad) - Closing
[0m14:29:07.937978 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:07.937978 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    tag_name,
    tag_value
  FROM `system`.`information_schema`.`column_tags`
  WHERE catalog_name = 'demo'
    AND schema_name = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m14:29:08.178495 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:29:08.179490 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=1a98e057-609d-43b0-8090-7ee5a621993e) - Closing
[0m14:29:08.183962 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:08.183962 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT column_name
  FROM `demo`.`information_schema`.`columns`
  WHERE table_catalog = 'demo' 
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2'
    AND is_nullable = 'NO';
  
[0m14:29:08.482753 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m14:29:08.488754 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=795512dc-d110-4e06-85b6-81d569194da9) - Closing
[0m14:29:08.500787 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:08.500787 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT kcu.constraint_name, kcu.column_name
  FROM `demo`.information_schema.key_column_usage kcu
  WHERE kcu.table_catalog = 'demo' 
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2' 
    AND kcu.constraint_name = (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2' 
        AND constraint_type = 'PRIMARY KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:29:09.024081 [debug] [Thread-1 (]: SQL status: OK in 0.520 seconds
[0m14:29:09.027113 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=b68c5ad5-b7e2-4383-8cad-861dfbcfe8aa) - Closing
[0m14:29:09.033121 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:09.033121 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT
    kcu.constraint_name,
    kcu.column_name AS from_column,
    ukcu.table_catalog AS to_catalog,
    ukcu.table_schema AS to_schema,
    ukcu.table_name AS to_table,
    ukcu.column_name AS to_column
  FROM `demo`.information_schema.key_column_usage kcu
  JOIN `demo`.information_schema.referential_constraints rc
    ON kcu.constraint_name = rc.constraint_name
  JOIN `demo`.information_schema.key_column_usage ukcu
    ON rc.unique_constraint_name = ukcu.constraint_name
    AND kcu.ordinal_position = ukcu.ordinal_position
  WHERE kcu.table_catalog = 'demo'
    AND kcu.table_schema = 'demo_schema'
    AND kcu.table_name = 'stg_test2'
    AND kcu.constraint_name IN (
      SELECT constraint_name
      FROM `demo`.information_schema.table_constraints
      WHERE table_catalog = 'demo'
        AND table_schema = 'demo_schema'
        AND table_name = 'stg_test2'
        AND constraint_type = 'FOREIGN KEY'
    )
  ORDER BY kcu.ordinal_position;
  
[0m14:29:09.787044 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m14:29:09.790969 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=9d04d607-044b-4c19-b12f-980366851618) - Closing
[0m14:29:09.796967 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:09.796967 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SELECT 
    column_name,
    mask_name,
    using_columns
  FROM `system`.`information_schema`.`column_masks`
  WHERE table_catalog = 'demo'
    AND table_schema = 'demo_schema'
    AND table_name = 'stg_test2';
  
[0m14:29:10.030576 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m14:29:10.035112 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=71d049ca-fa17-44af-ba38-f6d7255e4115) - Closing
[0m14:29:10.042318 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:10.043327 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
SHOW TBLPROPERTIES `demo`.`demo_schema`.`stg_test2`
  
[0m14:29:10.318040 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m14:29:10.323453 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=30b95021-1533-4a29-b7d7-118512311a3f) - Closing
[0m14:29:10.334405 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:10.335733 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
describe extended `demo`.`demo_schema`.`stg_test2`
  
[0m14:29:10.699238 [debug] [Thread-1 (]: SQL status: OK in 0.360 seconds
[0m14:29:10.706173 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=25b404f7-2018-4bb0-893d-c444747537d9) - Closing
[0m14:29:10.707173 [debug] [Thread-1 (]: Databricks adapter: Getting diff for ColumnCommentsConfig: comments={} quoted={} persist=False and comments={'url': '', 'job_posting_id': '', 'job_title': '', 'company_name': '', 'job_location': '', 'job_summary': '', 'job_seniority_level': '', 'job_function': '', 'job_employment_type': '', 'job_industries': '', 'job_posted_date': '', 'base_salary': '', 'timestamp': '', '_rescued_data': '', 'ingest_dts': '', 'source_file': '', 'year': '', 'month': '', 'day': ''} quoted={} persist=False
[0m14:29:10.713216 [debug] [Thread-1 (]: Applying configuration changes to relation `demo`.`demo_schema`.`stg_test2`
[0m14:29:10.731893 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:10.732895 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `demo`.`demo_schema`.`stg_test2`

  
[0m14:29:10.978464 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m14:29:10.983473 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=ed763b53-8ac1-4731-9b99-b0eca0fdb6e9) - Closing
[0m14:29:10.987464 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:10.988530 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */

    
DESCRIBE TABLE `stg_test2__dbt_tmp`

  
[0m14:29:11.170787 [debug] [Thread-1 (]: SQL status: OK in 0.180 seconds
[0m14:29:11.173001 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=59fac45d-2fed-49d0-a210-068665c683dc) - Closing
[0m14:29:11.180997 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_test2"
[0m14:29:11.181938 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_test2"
[0m14:29:11.181938 [debug] [Thread-1 (]: On model.jra_dbt.stg_test2: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_test2"} */
-- back compat for old kwarg name
  
  
  
  
  
  
      
          
          
      
  

    merge
    into
        `demo`.`demo_schema`.`stg_test2` as DBT_INTERNAL_DEST
    using
        `stg_test2__dbt_tmp` as DBT_INTERNAL_SOURCE
    on
        
              DBT_INTERNAL_SOURCE.job_posting_id <=> DBT_INTERNAL_DEST.job_posting_id
          
    when matched
        then update set
            *
    when not matched
        then insert
            *


[0m14:29:15.135134 [debug] [Thread-1 (]: SQL status: OK in 3.950 seconds
[0m14:29:15.137165 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7, command-id=6cfc780d-fd85-4a6c-93bc-beb6b8b085c7) - Closing
[0m14:29:15.150107 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '74691ff1-48e1-4057-bdb1-6b9a2e69c599', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F369A41D0>]}
[0m14:29:15.150107 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_test2 .................. [[32mOK[0m in 8.34s]
[0m14:29:15.151089 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_test2
[0m14:29:15.152106 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=8.353596687316895s, language=None, compute-name=) - Reusing connection previously named master
[0m14:29:15.153314 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:29:15.153314 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:29:15.153314 [debug] [MainThread]: On list_demo: Close
[0m14:29:15.153314 [debug] [MainThread]: Databricks adapter: Connection(session-id=1269d913-d04d-467a-b2f4-b344d3820b95) - Closing
[0m14:29:15.212128 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:29:15.213061 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:29:15.213061 [debug] [MainThread]: Databricks adapter: Connection(session-id=ead46d80-b20c-44e8-ba73-3bc3fae70b2c) - Closing
[0m14:29:15.292508 [debug] [MainThread]: Connection 'model.jra_dbt.stg_test2' was properly closed.
[0m14:29:15.293623 [debug] [MainThread]: On model.jra_dbt.stg_test2: Close
[0m14:29:15.295620 [debug] [MainThread]: Databricks adapter: Connection(session-id=19b2b940-efa8-4c5f-b0e4-b3a76a30e5d7) - Closing
[0m14:29:15.933726 [info ] [MainThread]: 
[0m14:29:15.935731 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 10.07 seconds (10.07s).
[0m14:29:15.939031 [debug] [MainThread]: Command end result
[0m14:29:15.964944 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:29:15.966935 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:29:15.971939 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:29:15.971939 [info ] [MainThread]: 
[0m14:29:15.972935 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:15.972935 [info ] [MainThread]: 
[0m14:29:15.973936 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:29:15.973936 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:29:15.974936 [debug] [MainThread]: Command `dbt run` succeeded at 14:29:15.974936 after 12.07 seconds
[0m14:29:15.974936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F39816AB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F393C8680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F5E13B620>]}
[0m14:29:15.974936 [debug] [MainThread]: Flushing usage events
[0m14:29:16.977862 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:31:58.798065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB29F1FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB28417F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB2E336E0>]}


============================== 14:31:58.802337 | d2f5d0c3-8d47-43c9-a195-d516a825d422 ==============================
[0m14:31:58.802337 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:31:58.802337 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select stg_unified_test', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m14:31:59.526164 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:31:59.526164 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:31:59.526164 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:32:00.174727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD5BBF1D0>]}
[0m14:32:00.235202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB4027A10>]}
[0m14:32:00.236203 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:32:00.476403 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:32:00.537408 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:32:00.537408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD6009B20>]}
[0m14:32:00.585929 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:32:00.585929 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:32:00.590930 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:32:00.610924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD762BEC0>]}
[0m14:32:00.663927 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:32:00.665930 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:32:00.684941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD74D6D50>]}
[0m14:32:00.684941 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:32:00.684941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD764B950>]}
[0m14:32:00.686941 [info ] [MainThread]: 
[0m14:32:00.686941 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:32:00.686941 [info ] [MainThread]: 
[0m14:32:00.687943 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:00.687943 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:32:00.688943 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:00.688943 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:32:00.688943 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:32:00.689939 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:32:00.689939 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:00.927612 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=e1aa9b55-de3e-4f89-b5ea-2f690dbed8a6) - Created
[0m14:32:01.050918 [debug] [ThreadPool]: SQL status: OK in 0.360 seconds
[0m14:32:01.050918 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=e1aa9b55-de3e-4f89-b5ea-2f690dbed8a6, command-id=40bbf538-69d7-4b6e-b088-f36e253463c2) - Closing
[0m14:32:01.055983 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:01.056991 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:32:01.065186 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:32:01.066183 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:32:01.066183 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:01.237637 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=443da0b2-7780-4cf2-8e68-915cfa7aa8ce) - Created
[0m14:32:01.539411 [debug] [ThreadPool]: SQL status: OK in 0.470 seconds
[0m14:32:01.546897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=443da0b2-7780-4cf2-8e68-915cfa7aa8ce, command-id=a48c9de7-9b82-462b-b28c-8f1a49edf015) - Closing
[0m14:32:01.548136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB29832C0>]}
[0m14:32:01.552703 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m14:32:01.552703 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m14:32:01.553586 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:01.553586 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m14:32:01.554702 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m14:32:01.566322 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m14:32:01.567274 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m14:32:01.592847 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:32:01.599978 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:32:01.600971 [debug] [Thread-1 (]: Safe create: False
[0m14:32:01.613478 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:32:01.614581 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:32:01.614581 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
[0m14:32:01.615778 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:32:01.789878 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=9db62c2c-13df-49b9-b7f4-577273b58a15) - Created
[0m14:32:02.193075 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件


  -- 这个 WHERE 条件只会在增量运行时被应用
  -- 它会从合并后的数据中，只选择 ingest_dts 大于目标表中已存在的最大 ingest_dts 的记录
  WHERE ingest_dts > (SELECT max(ingest_dts) FROM `demo`.`demo_schema`.`stg_unified_test`)
    
: [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_TABLE_NOT_FOUND] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:690)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1655)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1654)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3963)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:441)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:426)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:425)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:2211)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:2196)
	at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:2210)
	at com.databricks.sql.transaction.tahoe.FallbackToV1DeltaRelation$.unapply(FallbackToV1Relations.scala:30)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:887)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
	at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2346)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1686)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
	at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:507)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3064)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:413)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3064)
	at com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3063)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3090)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3088)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1359)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:839)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:229)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:229)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:241)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:252)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:252)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:159)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:274)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:272)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:272)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3088)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$26.applyOrElse(Analyzer.scala:3120)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$26.applyOrElse(Analyzer.scala:3110)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:93)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at scala.collection.immutable.List.map(List.scala:293)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3110)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3048)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
	at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:507)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:406)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:500)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:425)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:487)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:487)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:308)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:703)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:152)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:703)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:696)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:692)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:692)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:295)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:294)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:340)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:274)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:758)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable(DeltaErrors.scala:1655)
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.nonExistentDeltaTable$(DeltaErrors.scala:1654)
		at com.databricks.sql.transaction.tahoe.DeltaErrors$.nonExistentDeltaTable(DeltaErrors.scala:3963)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.$anonfun$toBaseRelation$1(DeltaTableV2.scala:441)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.recordFrameProfile(DeltaTableV2.scala:83)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation$lzycompute(DeltaTableV2.scala:426)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:425)
		at com.databricks.sql.transaction.tahoe.DeltaRelation$.$anonfun$fromV2Relation$1(DeltaAnalysis.scala:2211)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.DeltaRelation$.recordFrameProfile(DeltaAnalysis.scala:2196)
		at com.databricks.sql.transaction.tahoe.DeltaRelation$.fromV2Relation(DeltaAnalysis.scala:2210)
		at com.databricks.sql.transaction.tahoe.FallbackToV1DeltaRelation$.unapply(FallbackToV1Relations.scala:30)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:887)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis$$anonfun$apply$1.applyOrElse(DeltaAnalysis.scala:126)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
		at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2346)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
		at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:1686)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:206)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:126)
		at com.databricks.sql.transaction.tahoe.DeltaAnalysis.apply(DeltaAnalysis.scala:120)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
		at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
		at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
		at scala.collection.immutable.List.foldLeft(List.scala:91)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:507)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3064)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:413)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3064)
		at com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3063)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3090)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3088)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1359)
		at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1356)
		at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:839)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:526)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:188)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:229)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:229)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:241)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:252)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:363)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:252)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:188)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:159)
		at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:274)
		at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:272)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:487)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:272)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3088)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$26.applyOrElse(Analyzer.scala:3120)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$26.applyOrElse(Analyzer.scala:3110)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1330)
		at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1329)
		at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:93)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
		at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
		at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
		at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
		at scala.collection.TraversableLike.map(TraversableLike.scala:286)
		at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
		at scala.collection.AbstractTraversable.map(Traversable.scala:108)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
		at scala.collection.immutable.List.map(List.scala:293)
		at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:760)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3110)
		at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3048)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:480)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:629)
		at org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:613)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:480)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:479)
		at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
		at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
		at scala.collection.immutable.List.foldLeft(List.scala:91)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:475)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22(RuleExecutor.scala:585)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$22$adapted(RuleExecutor.scala:585)
		at scala.collection.immutable.List.foreach(List.scala:431)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:585)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:507)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:406)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:500)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:425)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:487)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:487)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:308)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:548)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:703)
		at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:152)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:703)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1342)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:696)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:692)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:692)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:295)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:294)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=2b0ab2e4-4d08-41fe-9599-d2b2122e599c
[0m14:32:02.199076 [debug] [Thread-1 (]: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m14:32:02.200076 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd2f5d0c3-8d47-43c9-a195-d516a825d422', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB2316060>]}
[0m14:32:02.200076 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model demo_schema.stg_unified_test ....... [[31mERROR[0m in 0.65s]
[0m14:32:02.201570 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m14:32:02.201570 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.stg_unified_test' to be skipped because of status 'error'.  Reason: Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql.
[0m14:32:02.203672 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0.6545414924621582s, language=None, compute-name=) - Reusing connection previously named master
[0m14:32:02.203672 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:02.203672 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:32:02.203672 [debug] [MainThread]: On list_demo: Close
[0m14:32:02.204793 [debug] [MainThread]: Databricks adapter: Connection(session-id=e1aa9b55-de3e-4f89-b5ea-2f690dbed8a6) - Closing
[0m14:32:02.262811 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:32:02.262811 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:32:02.262811 [debug] [MainThread]: Databricks adapter: Connection(session-id=443da0b2-7780-4cf2-8e68-915cfa7aa8ce) - Closing
[0m14:32:02.316237 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m14:32:02.316237 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m14:32:02.316237 [debug] [MainThread]: Databricks adapter: Connection(session-id=9db62c2c-13df-49b9-b7f4-577273b58a15) - Closing
[0m14:32:02.386478 [info ] [MainThread]: 
[0m14:32:02.387505 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 1.70 seconds (1.70s).
[0m14:32:02.388510 [debug] [MainThread]: Command end result
[0m14:32:02.409355 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:32:02.410446 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:32:02.415252 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:32:02.415923 [info ] [MainThread]: 
[0m14:32:02.415923 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m14:32:02.416550 [info ] [MainThread]: 
[0m14:32:02.416550 [error] [MainThread]: [31mFailure in model stg_unified_test (models\test\stg_unified_test.sql)[0m
[0m14:32:02.417115 [error] [MainThread]:   Database Error in model stg_unified_test (models\test\stg_unified_test.sql)
  [DELTA_TABLE_NOT_FOUND] Delta table `demo_schema`.`stg_unified_test` doesn't exist.
  compiled code at target\run\jra_dbt\models\test\stg_unified_test.sql
[0m14:32:02.417115 [info ] [MainThread]: 
[0m14:32:02.417738 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\stg_unified_test.sql
[0m14:32:02.418388 [info ] [MainThread]: 
[0m14:32:02.419517 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m14:32:02.419517 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:32:02.420664 [debug] [MainThread]: Command `dbt run` failed at 14:32:02.420664 after 3.71 seconds
[0m14:32:02.420664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB2CE1D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AD60D9BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024AB2953D40>]}
[0m14:32:02.421219 [debug] [MainThread]: Flushing usage events
[0m14:32:03.511081 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:32:25.670237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021288397590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212883953A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021288395430>]}


============================== 14:32:25.673270 | ceb7ed9b-c981-4c11-bbe5-321935b0c8ae ==============================
[0m14:32:25.673270 [info ] [MainThread]: Running with dbt=1.10.4
[0m14:32:25.674355 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_unified_test', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m14:32:26.394795 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m14:32:26.394795 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m14:32:26.394795 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m14:32:27.037146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A83C9130>]}
[0m14:32:27.097142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212A9CF50D0>]}
[0m14:32:27.097142 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m14:32:27.324144 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m14:32:27.381089 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m14:32:27.382091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA5E6AE0>]}
[0m14:32:27.429575 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:32:27.429575 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:32:27.433605 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m14:32:27.453752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA4313A0>]}
[0m14:32:27.503961 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:32:27.504959 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:32:27.536303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA8F0440>]}
[0m14:32:27.536303 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m14:32:27.537703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212ABA2B440>]}
[0m14:32:27.538705 [info ] [MainThread]: 
[0m14:32:27.538705 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:32:27.538705 [info ] [MainThread]: 
[0m14:32:27.539710 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:27.539710 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m14:32:27.540830 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:27.540830 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m14:32:27.540830 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m14:32:27.541821 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m14:32:27.541821 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:27.761030 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=fa1b87dc-32dc-4b8f-b373-6b9f1996bb9f) - Created
[0m14:32:27.888896 [debug] [ThreadPool]: SQL status: OK in 0.350 seconds
[0m14:32:27.890884 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=fa1b87dc-32dc-4b8f-b373-6b9f1996bb9f, command-id=e8326da9-0c2c-4153-9729-036834fc6f23) - Closing
[0m14:32:27.895880 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:27.896883 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m14:32:27.906347 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m14:32:27.906347 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m14:32:27.906347 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:32:28.113856 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=c900cd21-d356-48c5-8e90-3fcc4617bf17) - Created
[0m14:32:28.416282 [debug] [ThreadPool]: SQL status: OK in 0.510 seconds
[0m14:32:28.427538 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c900cd21-d356-48c5-8e90-3fcc4617bf17, command-id=ed061bc7-a7ec-459f-8150-0ba3072b0913) - Closing
[0m14:32:28.431192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA98BF80>]}
[0m14:32:28.436214 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m14:32:28.437208 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m14:32:28.437208 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m14:32:28.438201 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m14:32:28.438201 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m14:32:28.446716 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m14:32:28.447717 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m14:32:28.473033 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m14:32:28.481032 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m14:32:28.482041 [debug] [Thread-1 (]: Safe create: False
[0m14:32:28.496214 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:32:28.496214 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:32:28.497234 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件
    
[0m14:32:28.497234 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:32:28.694653 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66) - Created
[0m14:32:29.041760 [debug] [Thread-1 (]: SQL status: OK in 0.540 seconds
[0m14:32:29.042761 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66, command-id=ad0e4148-42ec-4dde-848c-1d4027667054) - Closing
[0m14:32:29.043698 [debug] [Thread-1 (]: No existing relation found
[0m14:32:29.052209 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:32:29.052209 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m14:32:29.253265 [debug] [Thread-1 (]: SQL status: OK in 0.200 seconds
[0m14:32:29.255274 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66, command-id=c1ab829e-1393-4ab5-9322-29b57a12da47) - Closing
[0m14:32:29.286638 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m14:32:29.287842 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:32:29.287842 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
  create or replace table `demo`.`demo_schema`.`stg_unified_test`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/stg_unified_test'
  
  

  
[0m14:32:30.166817 [debug] [Thread-1 (]: SQL status: OK in 0.880 seconds
[0m14:32:30.167817 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66, command-id=3f2a9791-5b05-4c5b-82e2-e8dc50f045de) - Closing
[0m14:32:30.171824 [debug] [Thread-1 (]: Applying tags to relation None
[0m14:32:30.171824 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m14:32:30.172824 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    insert into `demo`.`demo_schema`.`stg_unified_test` select * from `stg_unified_test__dbt_tmp`
  
[0m14:32:32.254825 [debug] [Thread-1 (]: SQL status: OK in 2.080 seconds
[0m14:32:32.255822 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66, command-id=7692a863-7596-4656-b71b-0728f96ad0fd) - Closing
[0m14:32:32.268237 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ceb7ed9b-c981-4c11-bbe5-321935b0c8ae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA63EA20>]}
[0m14:32:32.268237 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 3.83s]
[0m14:32:32.271242 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m14:32:32.275302 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=3.842986583709717s, language=None, compute-name=) - Reusing connection previously named master
[0m14:32:32.275302 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:32:32.275302 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m14:32:32.276311 [debug] [MainThread]: On list_demo: Close
[0m14:32:32.277365 [debug] [MainThread]: Databricks adapter: Connection(session-id=fa1b87dc-32dc-4b8f-b373-6b9f1996bb9f) - Closing
[0m14:32:32.339276 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m14:32:32.339276 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m14:32:32.340276 [debug] [MainThread]: Databricks adapter: Connection(session-id=c900cd21-d356-48c5-8e90-3fcc4617bf17) - Closing
[0m14:32:32.397974 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m14:32:32.397974 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m14:32:32.397974 [debug] [MainThread]: Databricks adapter: Connection(session-id=15dc861e-6a9e-4631-ad1d-9d5bfb252b66) - Closing
[0m14:32:32.614259 [info ] [MainThread]: 
[0m14:32:32.616261 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 5.07 seconds (5.07s).
[0m14:32:32.619244 [debug] [MainThread]: Command end result
[0m14:32:32.649330 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m14:32:32.651405 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m14:32:32.655398 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m14:32:32.655398 [info ] [MainThread]: 
[0m14:32:32.656344 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:32:32.656344 [info ] [MainThread]: 
[0m14:32:32.657362 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m14:32:32.657362 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m14:32:32.658501 [debug] [MainThread]: Command `dbt run` succeeded at 14:32:32.658501 after 7.11 seconds
[0m14:32:32.659500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002128671C5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002128671C380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212AA548710>]}
[0m14:32:32.659500 [debug] [MainThread]: Flushing usage events
[0m14:32:33.626283 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m22:59:52.408879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F0B25A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F0E221E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F0B01730>]}


============================== 22:59:52.414986 | 0e526d8a-905e-440a-9e8c-bbd6553265dd ==============================
[0m22:59:52.414986 [info ] [MainThread]: Running with dbt=1.10.4
[0m22:59:52.416448 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_test', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m22:59:54.972868 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m22:59:54.972868 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m22:59:54.973876 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m22:59:57.191667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F034E000>]}
[0m22:59:57.247965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240FFE99670>]}
[0m22:59:57.248965 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m22:59:57.634173 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m22:59:57.751657 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m22:59:57.752657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240942F7CE0>]}
[0m22:59:58.782900 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m22:59:58.782900 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m22:59:58.788996 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m22:59:58.811588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002409572BD40>]}
[0m22:59:58.860907 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m22:59:58.871863 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m22:59:58.899798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240944E8C20>]}
[0m22:59:58.899798 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m22:59:58.899798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002409574BB60>]}
[0m22:59:58.901038 [info ] [MainThread]: 
[0m22:59:58.901038 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:59:58.902046 [info ] [MainThread]: 
[0m22:59:58.902046 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:58.902046 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m22:59:58.903090 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m22:59:58.903090 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m22:59:58.904138 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m22:59:58.904592 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m22:59:58.904592 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:59:59.928952 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=83f3a067-99eb-43be-9255-cf70f7dc615f) - Created
[0m23:00:02.046992 [debug] [ThreadPool]: SQL status: OK in 3.140 seconds
[0m23:00:02.049986 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=83f3a067-99eb-43be-9255-cf70f7dc615f, command-id=779b755f-b2d9-4def-9d1a-42e589e1a454) - Closing
[0m23:00:02.059768 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:02.060709 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:00:02.070398 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:00:02.070398 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:00:02.070398 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:00:02.359532 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=5b970bf2-77ca-4f06-82cb-acc70da2642e) - Created
[0m23:00:16.440576 [debug] [ThreadPool]: SQL status: OK in 14.370 seconds
[0m23:00:16.451147 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=5b970bf2-77ca-4f06-82cb-acc70da2642e, command-id=a693bcd0-b289-47ba-acb8-e360851aa7f2) - Closing
[0m23:00:16.643837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024093FEFFB0>]}
[0m23:00:16.654954 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_test
[0m23:00:16.655535 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_test ..................... [RUN]
[0m23:00:16.656542 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:00:16.657528 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_test'
[0m23:00:16.657528 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_test
[0m23:00:16.665032 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_test"
[0m23:00:16.666038 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_test
[0m23:00:16.690190 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:00:16.697798 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:00:16.698792 [debug] [Thread-1 (]: Safe create: False
[0m23:00:16.711522 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m23:00:16.712471 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:00:16.712471 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

      create or replace temporary view `indeed_test__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)
    
[0m23:00:16.712471 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:00:16.996903 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=0a07a590-d650-42bb-9e47-b3f254943faf) - Created
[0m23:00:23.127175 [debug] [Thread-1 (]: SQL status: OK in 6.410 seconds
[0m23:00:23.130560 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0a07a590-d650-42bb-9e47-b3f254943faf, command-id=f961b8da-86f7-4207-bc1a-f20805ead6e0) - Closing
[0m23:00:23.203706 [debug] [Thread-1 (]: No existing relation found
[0m23:00:23.213472 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:00:23.213472 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
DESCRIBE TABLE `indeed_test__dbt_tmp`

  
[0m23:00:24.117278 [debug] [Thread-1 (]: SQL status: OK in 0.900 seconds
[0m23:00:24.120298 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0a07a590-d650-42bb-9e47-b3f254943faf, command-id=62a74546-7772-49b5-9e94-2636afc10dea) - Closing
[0m23:00:24.151670 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_test"
[0m23:00:24.153613 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:00:24.154606 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_test`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      MIN_AMOUNT double,
    
      MAX_AMOUNT double,
    
      timestamp timestamp_ntz,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/indeed_test'
  
  

  
[0m23:00:34.406515 [debug] [Thread-1 (]: SQL status: OK in 10.250 seconds
[0m23:00:34.408543 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0a07a590-d650-42bb-9e47-b3f254943faf, command-id=c4247770-ed7c-4f64-9006-57f88bc94bda) - Closing
[0m23:00:34.500428 [debug] [Thread-1 (]: Applying tags to relation None
[0m23:00:34.511429 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_test"
[0m23:00:34.512429 [debug] [Thread-1 (]: On model.jra_dbt.indeed_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_test"} */

    insert into `demo`.`demo_schema`.`indeed_test` select * from `indeed_test__dbt_tmp`
  
[0m23:00:46.212574 [debug] [Thread-1 (]: SQL status: OK in 11.700 seconds
[0m23:00:46.214554 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=0a07a590-d650-42bb-9e47-b3f254943faf, command-id=3a18d4eb-cd7c-418a-9a3a-16162e022da1) - Closing
[0m23:00:46.295311 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e526d8a-905e-440a-9e8c-bbd6553265dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240EDECFF50>]}
[0m23:00:46.296316 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_test ................ [[32mOK[0m in 29.64s]
[0m23:00:46.296316 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_test
[0m23:00:46.297347 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=29.65194010734558s, language=None, compute-name=) - Reusing connection previously named master
[0m23:00:46.297347 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:00:46.298347 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:00:46.298347 [debug] [MainThread]: On list_demo: Close
[0m23:00:46.298347 [debug] [MainThread]: Databricks adapter: Connection(session-id=83f3a067-99eb-43be-9255-cf70f7dc615f) - Closing
[0m23:00:46.541035 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:00:46.542040 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:00:46.543049 [debug] [MainThread]: Databricks adapter: Connection(session-id=5b970bf2-77ca-4f06-82cb-acc70da2642e) - Closing
[0m23:00:46.606203 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_test' was properly closed.
[0m23:00:46.607349 [debug] [MainThread]: On model.jra_dbt.indeed_test: Close
[0m23:00:46.608441 [debug] [MainThread]: Databricks adapter: Connection(session-id=0a07a590-d650-42bb-9e47-b3f254943faf) - Closing
[0m23:00:47.248414 [info ] [MainThread]: 
[0m23:00:47.248918 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 48.35 seconds (48.35s).
[0m23:00:47.249934 [debug] [MainThread]: Command end result
[0m23:00:47.282476 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:00:47.284441 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:00:47.288123 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:00:47.288123 [info ] [MainThread]: 
[0m23:00:47.289051 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:00:47.290205 [info ] [MainThread]: 
[0m23:00:47.291054 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:00:47.292064 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:00:47.293049 [debug] [MainThread]: Command `dbt run` succeeded at 23:00:47.293049 after 55.03 seconds
[0m23:00:47.293049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F0BAB0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002409574BA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240F0708680>]}
[0m23:00:47.293049 [debug] [MainThread]: Flushing usage events
[0m23:00:48.346094 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:11:32.721393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EFC5010A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EFC502420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EFC503140>]}


============================== 23:11:32.725477 | 08d88db4-e2d8-4686-b4fe-398403597b45 ==============================
[0m23:11:32.725477 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:11:32.725477 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select indeed_cleaned', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m23:11:33.428785 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:11:33.428785 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:11:33.429829 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:11:34.053469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EF9A14E00>]}
[0m23:11:34.112697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E9F649D30>]}
[0m23:11:34.113694 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:11:34.352610 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:11:34.418681 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:11:34.419690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA012E8A0>]}
[0m23:11:34.465347 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:11:34.466861 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m23:11:34.690230 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:11:34.702880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA1490BF0>]}
[0m23:11:34.750934 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:11:34.751936 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:11:34.776854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA1414800>]}
[0m23:11:34.776854 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:11:34.776854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA0154380>]}
[0m23:11:34.778005 [info ] [MainThread]: 
[0m23:11:34.779024 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:11:34.779024 [info ] [MainThread]: 
[0m23:11:34.779751 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:11:34.779751 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:11:34.780789 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:11:34.780789 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:11:34.780789 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:11:34.781790 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:11:34.781790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:11:35.233398 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=2d87b8cc-ebd9-4612-b2f3-5933e38d1d96) - Created
[0m23:11:35.387336 [debug] [ThreadPool]: SQL status: OK in 0.610 seconds
[0m23:11:35.388353 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2d87b8cc-ebd9-4612-b2f3-5933e38d1d96, command-id=97cbe5bd-3c9e-448d-afaf-45dea8616a79) - Closing
[0m23:11:35.392916 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:11:35.394306 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:11:35.401099 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:11:35.402129 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:11:35.402129 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:11:35.613087 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=36d27157-cd15-4da9-9087-e87e0a0437ab) - Created
[0m23:11:36.308126 [debug] [ThreadPool]: SQL status: OK in 0.910 seconds
[0m23:11:36.312146 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=36d27157-cd15-4da9-9087-e87e0a0437ab, command-id=b4303c8a-f274-491d-a552-b145d0bf457f) - Closing
[0m23:11:36.314241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EFF6541D0>]}
[0m23:11:36.316746 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m23:11:36.316746 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.indeed_cleaned .................. [RUN]
[0m23:11:36.316746 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:11:36.316746 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m23:11:36.318212 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m23:11:36.323843 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m23:11:36.324856 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m23:11:36.351227 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:11:36.359059 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:11:36.360058 [debug] [Thread-1 (]: Safe create: False
[0m23:11:36.372609 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:11:36.373596 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:11:36.374596 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(cast(job_posted_date As timestamp), 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`demo_schema`.`indeed_test`
    
[0m23:11:36.374596 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:11:36.608136 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c) - Created
[0m23:11:37.034004 [debug] [Thread-1 (]: SQL status: OK in 0.660 seconds
[0m23:11:37.036024 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c, command-id=96d197cd-fa91-4e62-a402-f379cbac4116) - Closing
[0m23:11:37.036024 [debug] [Thread-1 (]: No existing relation found
[0m23:11:37.043733 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:11:37.044672 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m23:11:37.285860 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m23:11:37.288631 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c, command-id=c90440df-2444-4ea0-b73b-6e64bcaa6dc1) - Closing
[0m23:11:37.322371 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m23:11:37.322371 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:11:37.323403 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`demo_schema`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
[0m23:11:38.724701 [debug] [Thread-1 (]: SQL status: OK in 1.400 seconds
[0m23:11:38.724701 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c, command-id=e5e3344f-ee68-496b-af8e-67b4786e1228) - Closing
[0m23:11:38.728278 [debug] [Thread-1 (]: Applying tags to relation None
[0m23:11:38.728278 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m23:11:38.728278 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    insert into `demo`.`demo_schema`.`indeed_cleaned` select * from `indeed_cleaned__dbt_tmp`
  
[0m23:11:42.313516 [debug] [Thread-1 (]: SQL status: OK in 3.580 seconds
[0m23:11:42.314516 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c, command-id=0e4b0356-492a-4d4e-ba46-4f4e3399c1db) - Closing
[0m23:11:42.329128 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08d88db4-e2d8-4686-b4fe-398403597b45', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028E9FD19D00>]}
[0m23:11:42.329128 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.indeed_cleaned ............. [[32mOK[0m in 6.01s]
[0m23:11:42.330127 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m23:11:42.331126 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.0168845653533936s, language=None, compute-name=) - Reusing connection previously named master
[0m23:11:42.331126 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:11:42.332544 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:11:42.332544 [debug] [MainThread]: On list_demo: Close
[0m23:11:42.333546 [debug] [MainThread]: Databricks adapter: Connection(session-id=2d87b8cc-ebd9-4612-b2f3-5933e38d1d96) - Closing
[0m23:11:42.422293 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:11:42.422293 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:11:42.422293 [debug] [MainThread]: Databricks adapter: Connection(session-id=36d27157-cd15-4da9-9087-e87e0a0437ab) - Closing
[0m23:11:42.503534 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m23:11:42.503534 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m23:11:42.503534 [debug] [MainThread]: Databricks adapter: Connection(session-id=ca84b0d1-101d-46cf-9cfa-458d4ba5a11c) - Closing
[0m23:11:42.803159 [info ] [MainThread]: 
[0m23:11:42.804103 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.02 seconds (8.02s).
[0m23:11:42.806133 [debug] [MainThread]: Command end result
[0m23:11:42.828410 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:11:42.830372 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:11:42.834958 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:11:42.835922 [info ] [MainThread]: 
[0m23:11:42.835922 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:11:42.836921 [info ] [MainThread]: 
[0m23:11:42.836921 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:11:42.839168 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:11:42.839168 [debug] [MainThread]: Command `dbt run` succeeded at 23:11:42.839168 after 10.23 seconds
[0m23:11:42.840428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EFC5B9BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA120E3F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028EA120CDD0>]}
[0m23:11:42.840428 [debug] [MainThread]: Flushing usage events
[0m23:11:43.819111 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:12:44.121443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F77DB86030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F77DB87A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F77DA2B230>]}


============================== 23:12:44.125740 | 03df3790-60f6-45f3-b7b9-8dcee8627dbb ==============================
[0m23:12:44.125740 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:12:44.126386 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select stg_unified', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m23:12:44.826668 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:12:44.827704 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:12:44.827704 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:12:45.488250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F720B6ADB0>]}
[0m23:12:45.547576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7009AC920>]}
[0m23:12:45.547576 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:12:45.780106 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:12:45.866549 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:12:45.867565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F720DD9D30>]}
[0m23:12:45.915700 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:12:45.915700 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:12:45.921253 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:12:45.939313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7224E7C20>]}
[0m23:12:45.990789 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:12:45.991820 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:12:46.011750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F722396CF0>]}
[0m23:12:46.011750 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:12:46.011750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '03df3790-60f6-45f3-b7b9-8dcee8627dbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F72253E5D0>]}
[0m23:12:46.012750 [warn ] [MainThread]: The selection criterion 'stg_unified' does not match any enabled nodes
[0m23:12:46.013763 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m23:12:46.014752 [debug] [MainThread]: Command end result
[0m23:12:46.033755 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:12:46.034758 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:12:46.037749 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:12:46.039263 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:12:46.040275 [debug] [MainThread]: Command `dbt run` succeeded at 23:12:46.040275 after 2.05 seconds
[0m23:12:46.040275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7210BBE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F722407FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7009C4D10>]}
[0m23:12:46.041271 [debug] [MainThread]: Flushing usage events
[0m23:12:47.053637 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m23:12:52.551444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AEDD3A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AF30DE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AEE430B0>]}


============================== 23:12:52.555453 | b9064104-4f46-4a59-85cc-12f1b19d582c ==============================
[0m23:12:52.555453 [info ] [MainThread]: Running with dbt=1.10.4
[0m23:12:52.555453 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select stg_unified_test', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m23:12:53.268333 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m23:12:53.268333 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m23:12:53.268333 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m23:12:53.903169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AC354E60>]}
[0m23:12:53.970586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249B04013D0>]}
[0m23:12:53.971651 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m23:12:54.196331 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m23:12:54.256051 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m23:12:54.257051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D29EED20>]}
[0m23:12:54.306562 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m23:12:54.306562 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m23:12:54.312086 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m23:12:54.332739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D26993A0>]}
[0m23:12:54.380152 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:12:54.381146 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:12:54.411894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D2986D50>]}
[0m23:12:54.411894 [info ] [MainThread]: Found 6 models, 5 sources, 682 macros
[0m23:12:54.412691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D26984D0>]}
[0m23:12:54.413815 [info ] [MainThread]: 
[0m23:12:54.413815 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:12:54.413815 [info ] [MainThread]: 
[0m23:12:54.414823 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:54.414823 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m23:12:54.415814 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:54.416802 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m23:12:54.416802 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m23:12:54.416802 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m23:12:54.416802 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:12:54.656639 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=8526ca5f-e29a-404e-9963-b1c55c19934e) - Created
[0m23:12:54.826218 [debug] [ThreadPool]: SQL status: OK in 0.410 seconds
[0m23:12:54.828224 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8526ca5f-e29a-404e-9963-b1c55c19934e, command-id=b70c8420-7e5c-437b-92ac-4836e28c5960) - Closing
[0m23:12:54.841011 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:54.841011 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m23:12:54.854725 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m23:12:54.855722 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m23:12:54.855722 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:12:55.078256 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=57ed3640-cff6-44bc-8cb4-536be547ff9c) - Created
[0m23:12:55.645437 [debug] [ThreadPool]: SQL status: OK in 0.790 seconds
[0m23:12:55.649652 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=57ed3640-cff6-44bc-8cb4-536be547ff9c, command-id=aed0befa-76c8-4e9b-bcdd-ca8abb4e7b52) - Closing
[0m23:12:55.651811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D28695E0>]}
[0m23:12:55.655793 [debug] [Thread-1 (]: Began running node model.jra_dbt.stg_unified_test
[0m23:12:55.655793 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.stg_unified_test ................ [RUN]
[0m23:12:55.656799 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.stg_unified_test, idle-time=0s, language=None, compute-name=) - Creating connection
[0m23:12:55.656799 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.stg_unified_test'
[0m23:12:55.656799 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.stg_unified_test
[0m23:12:55.669626 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.stg_unified_test"
[0m23:12:55.670799 [debug] [Thread-1 (]: Began executing node model.jra_dbt.stg_unified_test
[0m23:12:55.698305 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m23:12:55.706356 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m23:12:55.707362 [debug] [Thread-1 (]: Safe create: False
[0m23:12:55.719451 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:12:55.720463 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:12:55.720463 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

      create or replace temporary view `stg_unified_test__dbt_tmp` as
      -- 第 1 步：将 UNION ALL 的逻辑放到一个 CTE (Common Table Expression) 中
with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`stg_cleaned_test`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`demo_schema`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件
    
[0m23:12:55.720463 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:12:55.983233 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e) - Created
[0m23:12:56.777494 [debug] [Thread-1 (]: SQL status: OK in 1.060 seconds
[0m23:12:56.780031 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e, command-id=460a5f24-343a-4649-ac13-d365a73cc40b) - Closing
[0m23:12:56.780031 [debug] [Thread-1 (]: No existing relation found
[0m23:12:56.789549 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:12:56.789549 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
DESCRIBE TABLE `stg_unified_test__dbt_tmp`

  
[0m23:12:57.083108 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m23:12:57.087100 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e, command-id=8d6a96e2-74f4-42ff-8198-d016db51627b) - Closing
[0m23:12:57.113872 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.stg_unified_test"
[0m23:12:57.114870 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:12:57.114870 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    
  create or replace table `demo`.`demo_schema`.`stg_unified_test`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/stg_unified_test'
  
  

  
[0m23:12:58.361121 [debug] [Thread-1 (]: SQL status: OK in 1.250 seconds
[0m23:12:58.362110 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e, command-id=0ee6dd62-7dd2-4bfc-af79-2f93bf137887) - Closing
[0m23:12:58.365121 [debug] [Thread-1 (]: Applying tags to relation None
[0m23:12:58.365121 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.stg_unified_test"
[0m23:12:58.366222 [debug] [Thread-1 (]: On model.jra_dbt.stg_unified_test: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.stg_unified_test"} */

    insert into `demo`.`demo_schema`.`stg_unified_test` select * from `stg_unified_test__dbt_tmp`
  
[0m23:13:01.874991 [debug] [Thread-1 (]: SQL status: OK in 3.510 seconds
[0m23:13:01.876958 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e, command-id=e89c6b1c-4dbc-4cf4-b304-efdab03f8188) - Closing
[0m23:13:01.896349 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9064104-4f46-4a59-85cc-12f1b19d582c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AC3542C0>]}
[0m23:13:01.898351 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.stg_unified_test ........... [[32mOK[0m in 6.24s]
[0m23:13:01.899919 [debug] [Thread-1 (]: Finished running node model.jra_dbt.stg_unified_test
[0m23:13:01.902860 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.251048564910889s, language=None, compute-name=) - Reusing connection previously named master
[0m23:13:01.903918 [debug] [MainThread]: Connection 'master' was properly closed.
[0m23:13:01.903918 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m23:13:01.904943 [debug] [MainThread]: On list_demo: Close
[0m23:13:01.904943 [debug] [MainThread]: Databricks adapter: Connection(session-id=8526ca5f-e29a-404e-9963-b1c55c19934e) - Closing
[0m23:13:01.979446 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m23:13:01.980730 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m23:13:01.981785 [debug] [MainThread]: Databricks adapter: Connection(session-id=57ed3640-cff6-44bc-8cb4-536be547ff9c) - Closing
[0m23:13:02.061849 [debug] [MainThread]: Connection 'model.jra_dbt.stg_unified_test' was properly closed.
[0m23:13:02.063842 [debug] [MainThread]: On model.jra_dbt.stg_unified_test: Close
[0m23:13:02.064849 [debug] [MainThread]: Databricks adapter: Connection(session-id=b6c6e44d-6967-425b-b9e0-9f287cd3ab7e) - Closing
[0m23:13:02.411830 [info ] [MainThread]: 
[0m23:13:02.413935 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 8.00 seconds (8.00s).
[0m23:13:02.417052 [debug] [MainThread]: Command end result
[0m23:13:02.446748 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m23:13:02.448744 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m23:13:02.453257 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m23:13:02.453257 [info ] [MainThread]: 
[0m23:13:02.454263 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:13:02.455299 [info ] [MainThread]: 
[0m23:13:02.456605 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m23:13:02.456605 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m23:13:02.457692 [debug] [MainThread]: Command `dbt run` succeeded at 23:13:02.457692 after 10.01 seconds
[0m23:13:02.457692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AF1C6A80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249AEA75670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249D2699C40>]}
[0m23:13:02.458690 [debug] [MainThread]: Flushing usage events
[0m23:13:03.496991 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:39:06.582528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2E6504A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2E651EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2EAC2390>]}


============================== 16:39:06.587592 | 52aee2a3-5705-48eb-b893-dd2d2526f21c ==============================
[0m16:39:06.587592 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:39:06.587592 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'None', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt debug', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m16:39:06.610719 [info ] [MainThread]: dbt version: 1.10.4
[0m16:39:06.611723 [info ] [MainThread]: python version: 3.12.10
[0m16:39:06.611723 [info ] [MainThread]: python path: C:\Users\George\AppData\Local\Programs\Python\Python312\python.exe
[0m16:39:06.612732 [info ] [MainThread]: os info: Windows-11-10.0.26100-SP0
[0m16:39:09.240288 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:39:09.240288 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:39:09.241284 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:39:11.410389 [info ] [MainThread]: Using profiles dir at C:\Users\George\.dbt
[0m16:39:11.411384 [info ] [MainThread]: Using profiles.yml file at C:\Users\George\.dbt\profiles.yml
[0m16:39:11.411384 [info ] [MainThread]: Using dbt_project.yml file at C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\dbt_project.yml
[0m16:39:11.411384 [info ] [MainThread]: adapter type: databricks
[0m16:39:11.412382 [info ] [MainThread]: adapter version: 1.10.4
[0m16:39:11.518356 [info ] [MainThread]: Configuration:
[0m16:39:11.519359 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m16:39:11.519359 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m16:39:11.519359 [info ] [MainThread]: Required dependencies:
[0m16:39:11.520356 [debug] [MainThread]: Executing "git --help"
[0m16:39:11.545464 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--no-lazy-fetch]\n           [--no-optional-locks] [--no-advice] [--bare] [--git-dir=<path>]\n           [--work-tree=<path>] [--namespace=<name>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone      Clone a repository into a new directory\n   init       Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add        Add file contents to the index\n   mv         Move or rename a file, a directory, or a symlink\n   restore    Restore working tree files\n   rm         Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect     Use binary search to find the commit that introduced a bug\n   diff       Show changes between commits, commit and working tree, etc\n   grep       Print lines matching a pattern\n   log        Show commit logs\n   show       Show various types of objects\n   status     Show the working tree status\n\ngrow, mark and tweak your common history\n   backfill   Download missing objects in a partial clone\n   branch     List, create, or delete branches\n   commit     Record changes to the repository\n   merge      Join two or more development histories together\n   rebase     Reapply commits on top of another base tip\n   reset      Reset current HEAD to the specified state\n   switch     Switch branches\n   tag        Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch      Download objects and refs from another repository\n   pull       Fetch from and integrate with another repository or a local branch\n   push       Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m16:39:11.545464 [debug] [MainThread]: STDERR: "b''"
[0m16:39:11.546464 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m16:39:11.546464 [info ] [MainThread]: Connection:
[0m16:39:11.546464 [info ] [MainThread]:   host: https://dbc-22274c33-f88d.cloud.databricks.com
[0m16:39:11.547464 [info ] [MainThread]:   http_path: sql/protocolv1/o/551405405186610/0805-064911-zdmrk3ir
[0m16:39:11.547464 [info ] [MainThread]:   catalog: demo
[0m16:39:11.547464 [info ] [MainThread]:   schema: demo_schema
[0m16:39:11.547464 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:39:11.901445 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m16:39:11.901445 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m16:39:11.901445 [debug] [MainThread]: Using databricks connection "debug"
[0m16:39:11.901445 [debug] [MainThread]: On debug: select 1 as id
[0m16:39:11.902445 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:39:13.024139 [debug] [MainThread]: Databricks adapter: Connection(session-id=fb0ce488-6382-4b67-bfa4-8bfcfc4aa10e) - Created
[0m16:39:22.401657 [debug] [MainThread]: SQL status: OK in 10.500 seconds
[0m16:39:22.401657 [debug] [MainThread]: Databricks adapter: Cursor(session-id=fb0ce488-6382-4b67-bfa4-8bfcfc4aa10e, command-id=6c2b5001-add8-47e2-8aab-1918e9259f01) - Closing
[0m16:39:22.627128 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m16:39:22.628136 [info ] [MainThread]: [32mAll checks passed![0m
[0m16:39:22.630150 [debug] [MainThread]: Command `dbt debug` succeeded at 16:39:22.630150 after 16.17 seconds
[0m16:39:22.630150 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m16:39:22.631135 [debug] [MainThread]: On debug: Close
[0m16:39:22.631135 [debug] [MainThread]: Databricks adapter: Connection(session-id=fb0ce488-6382-4b67-bfa4-8bfcfc4aa10e) - Closing
[0m16:39:22.944728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2E8FE720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2FC3D940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF4FF6CDD0>]}
[0m16:39:22.945801 [debug] [MainThread]: Flushing usage events
[0m16:39:23.956153 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:39:52.595997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB52E55010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB52E552E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB52E571A0>]}


============================== 16:39:52.599070 | 5dad111a-0866-48b3-a7f5-e288a244a3ef ==============================
[0m16:39:52.599070 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:39:52.600022 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m16:39:53.314003 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:39:53.315004 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:39:53.315004 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:39:54.023460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5dad111a-0866-48b3-a7f5-e288a244a3ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB74BD95E0>]}
[0m16:39:54.085979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5dad111a-0866-48b3-a7f5-e288a244a3ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB76206480>]}
[0m16:39:54.085979 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:39:54.369285 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:39:54.459820 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m16:39:54.459820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5dad111a-0866-48b3-a7f5-e288a244a3ef', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB76C8CB90>]}
[0m16:39:55.622347 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 3 files added, 1 files changed.
[0m16:39:55.622347 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:39:55.622347 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:39:55.623362 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:39:55.623362 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:39:55.623362 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:39:55.623362 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:39:55.624345 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:39:55.916175 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.linkedin_cleaned' (models\test\linkedin_cleaned.sql) depends on a source named 'bronze_layer.stg_test2' which was not found
[0m16:39:55.916175 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m16:39:55.917222 [debug] [MainThread]: Command `dbt run` failed at 16:39:55.917222 after 3.46 seconds
[0m16:39:55.917222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB50746360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB782CFE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AB782CEF90>]}
[0m16:39:55.918220 [debug] [MainThread]: Flushing usage events
[0m16:39:56.868276 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:40:34.496841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023587391250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023587858740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235873911F0>]}


============================== 16:40:34.501184 | 5d79541a-d270-4146-a570-42bdd464f2ea ==============================
[0m16:40:34.501184 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:40:34.502180 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m16:40:35.221865 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:40:35.221865 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:40:35.221865 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:40:35.864795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5d79541a-d270-4146-a570-42bdd464f2ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023586D65A60>]}
[0m16:40:35.926909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5d79541a-d270-4146-a570-42bdd464f2ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AA8C23F0>]}
[0m16:40:35.926909 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:40:36.148299 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:40:36.207997 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key 'sources' in "<unicode string>", line 13, column 1 in file
`models\test\sources.yml`
[0m16:40:36.208999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5d79541a-d270-4146-a570-42bdd464f2ea', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AAF41DC0>]}
[0m16:40:36.257724 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 3 files added, 1 files changed.
[0m16:40:36.257724 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:40:36.258715 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:40:36.258715 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:40:36.258715 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:40:36.258715 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:40:36.259736 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:40:36.259736 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:40:36.550231 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.linkedin_cleaned' (models\test\linkedin_cleaned.sql) depends on a source named 'bronze_layer.stg_test2' which was not found
[0m16:40:36.551233 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m16:40:36.551233 [debug] [MainThread]: Command `dbt run` failed at 16:40:36.551233 after 2.16 seconds
[0m16:40:36.552228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235889511F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AC247D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000235AC0239E0>]}
[0m16:40:36.552228 [debug] [MainThread]: Flushing usage events
[0m16:40:37.554220 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:44:09.320278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAFEB13A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAFEB132C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAFEF30800>]}


============================== 16:44:09.323845 | 97106728-cdf2-4708-be6b-b45674166044 ==============================
[0m16:44:09.323845 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:44:09.324845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m16:44:10.036866 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:44:10.036866 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:44:10.036866 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:44:10.692822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97106728-cdf2-4708-be6b-b45674166044', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAA036C920>]}
[0m16:44:10.750334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97106728-cdf2-4708-be6b-b45674166044', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA81C7B830>]}
[0m16:44:10.751340 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:44:10.990756 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:44:11.117846 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 3 files added, 1 files changed.
[0m16:44:11.117846 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:44:11.117846 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:44:11.117846 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:44:11.118844 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:44:11.118844 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:44:11.118844 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:44:11.118844 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:44:11.450867 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.linkedin_cleaned' (models\test\linkedin_cleaned.sql) depends on a source named 'bronze_layer.stg_test2' which was not found
[0m16:44:11.451854 [debug] [MainThread]: Command `dbt run` failed at 16:44:11.451854 after 2.26 seconds
[0m16:44:11.451854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAFEEFD700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAA37B8E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAA399ADE0>]}
[0m16:44:11.452859 [debug] [MainThread]: Flushing usage events
[0m16:44:12.462070 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:44:55.749193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AC1191640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AC06195E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AC133A0C0>]}


============================== 16:44:55.753195 | 94760ee7-dafd-4223-b8f8-4abd5675359a ==============================
[0m16:44:55.753195 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:44:55.754232 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select linkedin_jobs', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'use_experimental_parser': 'False'}
[0m16:44:56.487928 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:44:56.487928 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:44:56.487928 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:44:57.168941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94760ee7-dafd-4223-b8f8-4abd5675359a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AE49B7FB0>]}
[0m16:44:57.226026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94760ee7-dafd-4223-b8f8-4abd5675359a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AE471FE00>]}
[0m16:44:57.227021 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:44:57.462657 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:44:57.574540 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 3 files added, 1 files changed.
[0m16:44:57.574540 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:44:57.574540 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:44:57.574540 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:44:57.575540 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:44:57.575540 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:44:57.575540 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:44:57.575540 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:44:57.887087 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.stg_jobs_description_new' (models\test\stg_jobs_description_new.sql) depends on a source named 'silver_layer.stg_cleaned_test' which was not found
[0m16:44:57.888152 [debug] [MainThread]: Command `dbt run` failed at 16:44:57.888152 after 2.26 seconds
[0m16:44:57.888746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AC199D850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AC147E810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020AE60EECF0>]}
[0m16:44:57.888746 [debug] [MainThread]: Flushing usage events
[0m16:44:58.844011 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:47:01.114330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023371D52F60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023371BEE420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023371BEC0E0>]}


============================== 16:47:01.118329 | 303ea3d6-3b31-4a4c-b1af-e1250f4eeb66 ==============================
[0m16:47:01.118329 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:47:01.119329 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m16:47:01.879929 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:47:01.879929 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:47:01.881224 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:47:02.567614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '303ea3d6-3b31-4a4c-b1af-e1250f4eeb66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002337F3DEE10>]}
[0m16:47:02.626650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '303ea3d6-3b31-4a4c-b1af-e1250f4eeb66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002337FAF5DC0>]}
[0m16:47:02.626650 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:47:02.855824 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:47:02.974454 [debug] [MainThread]: Partial parsing enabled: 5 files deleted, 5 files added, 1 files changed.
[0m16:47:02.974454 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:47:02.974454 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:47:02.974454 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\jobs_description.sql
[0m16:47:02.975449 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:47:02.975449 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\jobs_unified.sql
[0m16:47:02.975449 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:47:02.975449 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:47:02.976448 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_unified_test.sql
[0m16:47:02.976448 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:47:02.976448 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m16:47:02.976448 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:47:03.284874 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.jra_dbt.indeed_cleaned' (models\test\indeed_cleaned.sql) depends on a source named 'bronze_layer.indeed_test' which was not found
[0m16:47:03.285802 [debug] [MainThread]: Command `dbt run` failed at 16:47:03.284874 after 2.28 seconds
[0m16:47:03.285802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023371D50380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023316C3B830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023316C464B0>]}
[0m16:47:03.285802 [debug] [MainThread]: Flushing usage events
[0m16:47:04.308244 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:47:21.086042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC43B13890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC43FB9D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC43FBA4B0>]}


============================== 16:47:21.090428 | fba8aa8e-3cc7-49fc-b4c1-72249ae788cb ==============================
[0m16:47:21.090428 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:47:21.091427 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run --select linkedin_jobs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m16:47:21.813152 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:47:21.814152 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:47:21.814152 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:47:22.443681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC672A8530>]}
[0m16:47:22.501205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC66885EB0>]}
[0m16:47:22.501205 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:47:22.731552 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:47:22.841340 [debug] [MainThread]: Partial parsing enabled: 5 files deleted, 5 files added, 2 files changed.
[0m16:47:22.841340 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\indeed_jobs.sql
[0m16:47:22.841340 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_cleaned.sql
[0m16:47:22.842341 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\linkedin_jobs.sql
[0m16:47:22.842341 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\jobs_description.sql
[0m16:47:22.842341 [debug] [MainThread]: Partial parsing: added file: jra_dbt://models\test\jobs_unified.sql
[0m16:47:22.842341 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\sources.yml
[0m16:47:22.842341 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\indeed_test.sql
[0m16:47:22.843344 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_test2.sql
[0m16:47:22.843344 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_unified_test.sql
[0m16:47:22.843344 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_cleaned_test.sql
[0m16:47:22.843344 [debug] [MainThread]: Partial parsing: deleted file: jra_dbt://models\test\stg_jobs_description_new.sql
[0m16:47:22.843344 [debug] [MainThread]: Partial parsing: updated file: jra_dbt://models\test\indeed_cleaned.sql
[0m16:47:23.236825 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m16:47:23.246704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC43AA3E90>]}
[0m16:47:23.294227 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m16:47:23.309819 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m16:47:23.339471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC689A4710>]}
[0m16:47:23.339471 [info ] [MainThread]: Found 6 models, 6 sources, 682 macros
[0m16:47:23.339471 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC688FA930>]}
[0m16:47:23.340476 [info ] [MainThread]: 
[0m16:47:23.340476 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:47:23.341854 [info ] [MainThread]: 
[0m16:47:23.341854 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m16:47:23.341854 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:47:23.343940 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m16:47:23.343940 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m16:47:23.344886 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m16:47:23.344886 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m16:47:23.344886 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:23.691132 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=c602e2b7-65cb-4fac-a8df-3f68b6d72635) - Created
[0m16:47:24.920736 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m16:47:24.921748 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=c602e2b7-65cb-4fac-a8df-3f68b6d72635, command-id=a360223c-d4e3-43d5-ada7-34eaa837a3b5) - Closing
[0m16:47:24.931385 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_demo_schema, idle-time=0s, language=None, compute-name=) - Creating connection
[0m16:47:24.931385 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_demo_schema'
[0m16:47:24.946544 [debug] [ThreadPool]: Using databricks connection "list_demo_demo_schema"
[0m16:47:24.946544 [debug] [ThreadPool]: On list_demo_demo_schema: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_demo_schema"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'demo_schema'

  
[0m16:47:24.947544 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:25.194091 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=57b09daf-ea99-49a5-8f17-da5f22df95f2) - Created
[0m16:47:29.344292 [debug] [ThreadPool]: SQL status: OK in 4.400 seconds
[0m16:47:29.363018 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=57b09daf-ea99-49a5-8f17-da5f22df95f2, command-id=0ffdf657-746a-451d-84f2-38a9c4abe354) - Closing
[0m16:47:29.365046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC688F9A90>]}
[0m16:47:29.370812 [debug] [Thread-1 (]: Began running node model.jra_dbt.linkedin_jobs
[0m16:47:29.370812 [info ] [Thread-1 (]: 1 of 1 START sql incremental model demo_schema.linkedin_jobs ................... [RUN]
[0m16:47:29.371815 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.linkedin_jobs, idle-time=0s, language=None, compute-name=) - Creating connection
[0m16:47:29.371815 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.linkedin_jobs'
[0m16:47:29.372823 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.linkedin_jobs
[0m16:47:29.378981 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.linkedin_jobs"
[0m16:47:29.379884 [debug] [Thread-1 (]: Began executing node model.jra_dbt.linkedin_jobs
[0m16:47:29.404350 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m16:47:29.419373 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m16:47:29.420374 [debug] [Thread-1 (]: Safe create: False
[0m16:47:29.433041 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m16:47:29.434044 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m16:47:29.434044 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

      create or replace temporary view `linkedin_jobs__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)
    
[0m16:47:29.434963 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:47:29.674710 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686) - Created
[0m16:47:35.206951 [debug] [Thread-1 (]: SQL status: OK in 5.770 seconds
[0m16:47:35.207948 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686, command-id=d13a3dac-0630-4553-9b00-766dafbe6d2d) - Closing
[0m16:47:35.305611 [debug] [Thread-1 (]: No existing relation found
[0m16:47:35.321174 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m16:47:35.322171 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
DESCRIBE TABLE `linkedin_jobs__dbt_tmp`

  
[0m16:47:36.047720 [debug] [Thread-1 (]: SQL status: OK in 0.720 seconds
[0m16:47:36.051725 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686, command-id=c2b4f1e4-500f-4c8e-87a0-cedbec156ccd) - Closing
[0m16:47:36.079954 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m16:47:36.080916 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m16:47:36.080916 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
  create or replace table `demo`.`demo_schema`.`linkedin_jobs`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      base_salary struct<currency:string,max_amount:double,min_amount:double,payment_period:string>,
    
      timestamp timestamp,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/linkedin_jobs'
  
  

  
[0m16:47:45.421295 [debug] [Thread-1 (]: SQL status: OK in 9.340 seconds
[0m16:47:45.424298 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686, command-id=54c9fc04-e07d-42af-89b2-eef2b9082a6e) - Closing
[0m16:47:45.518087 [debug] [Thread-1 (]: Applying tags to relation None
[0m16:47:45.519145 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m16:47:45.519145 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    insert into `demo`.`demo_schema`.`linkedin_jobs` select * from `linkedin_jobs__dbt_tmp`
  
[0m16:47:57.818519 [debug] [Thread-1 (]: SQL status: OK in 12.300 seconds
[0m16:47:57.820462 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686, command-id=4e2dde86-a72f-4b44-89fa-a3f25140ab2a) - Closing
[0m16:47:57.923666 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fba8aa8e-3cc7-49fc-b4c1-72249ae788cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC673109B0>]}
[0m16:47:57.924684 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model demo_schema.linkedin_jobs .............. [[32mOK[0m in 28.55s]
[0m16:47:57.925746 [debug] [Thread-1 (]: Finished running node model.jra_dbt.linkedin_jobs
[0m16:47:57.928712 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=28.563666582107544s, language=None, compute-name=) - Reusing connection previously named master
[0m16:47:57.928712 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:47:57.928712 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m16:47:57.929712 [debug] [MainThread]: On list_demo: Close
[0m16:47:57.929712 [debug] [MainThread]: Databricks adapter: Connection(session-id=c602e2b7-65cb-4fac-a8df-3f68b6d72635) - Closing
[0m16:47:57.988466 [debug] [MainThread]: Connection 'list_demo_demo_schema' was properly closed.
[0m16:47:57.988466 [debug] [MainThread]: On list_demo_demo_schema: Close
[0m16:47:57.989604 [debug] [MainThread]: Databricks adapter: Connection(session-id=57b09daf-ea99-49a5-8f17-da5f22df95f2) - Closing
[0m16:47:58.062670 [debug] [MainThread]: Connection 'model.jra_dbt.linkedin_jobs' was properly closed.
[0m16:47:58.062670 [debug] [MainThread]: On model.jra_dbt.linkedin_jobs: Close
[0m16:47:58.063688 [debug] [MainThread]: Databricks adapter: Connection(session-id=99dfc29f-4e49-413e-bbed-07bee1dc2686) - Closing
[0m16:47:58.561837 [info ] [MainThread]: 
[0m16:47:58.562872 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 35.22 seconds (35.22s).
[0m16:47:58.563980 [debug] [MainThread]: Command end result
[0m16:47:58.588223 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m16:47:58.590227 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m16:47:58.593552 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m16:47:58.593552 [info ] [MainThread]: 
[0m16:47:58.594549 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:47:58.594549 [info ] [MainThread]: 
[0m16:47:58.595545 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m16:47:58.596668 [debug] [MainThread]: Command `dbt run` succeeded at 16:47:58.596668 after 37.61 seconds
[0m16:47:58.596668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC43BCEF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC46BECD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DC40E36B70>]}
[0m16:47:58.597681 [debug] [MainThread]: Flushing usage events
[0m16:47:59.559002 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:52:01.592631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C26DC33E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C26E06A120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C26E0691C0>]}


============================== 16:52:01.597264 | 8f587762-8652-49a8-be58-f6998d43ca29 ==============================
[0m16:52:01.597264 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:52:01.597264 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select linkedin_jobs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m16:52:02.408504 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:52:02.409499 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:52:02.409499 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:52:03.070828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8f587762-8652-49a8-be58-f6998d43ca29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2113FE990>]}
[0m16:52:03.132044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8f587762-8652-49a8-be58-f6998d43ca29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C270CFA150>]}
[0m16:52:03.133041 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:52:03.404813 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:52:03.471558 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:52:03.472488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8f587762-8652-49a8-be58-f6998d43ca29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C211858080>]}
[0m16:52:04.451345 [error] [MainThread]: Encountered an error:
Parsing Error
  at path ['schema']: None is not of type 'string'
[0m16:52:04.452298 [debug] [MainThread]: Command `dbt run` failed at 16:52:04.452298 after 2.98 seconds
[0m16:52:04.452298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C26DF774A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C27F2279B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C26DC33B00>]}
[0m16:52:04.453275 [debug] [MainThread]: Flushing usage events
[0m16:52:05.479767 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:53:08.390508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A028CB39B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A028CB2600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A028CB06B0>]}


============================== 16:53:08.394545 | ef816116-f9f0-4394-97e2-2b63eb29d8ec ==============================
[0m16:53:08.394545 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:53:08.395504 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m16:53:09.132204 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:53:09.133204 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:53:09.133204 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:53:09.781333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef816116-f9f0-4394-97e2-2b63eb29d8ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A04C384170>]}
[0m16:53:09.846025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef816116-f9f0-4394-97e2-2b63eb29d8ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A04C11A240>]}
[0m16:53:09.847027 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:53:10.118671 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:53:10.173179 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:53:10.174300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ef816116-f9f0-4394-97e2-2b63eb29d8ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A04D853E90>]}
[0m16:53:11.144078 [error] [MainThread]: Encountered an error:
Parsing Error
  at path ['schema']: None is not of type 'string'
[0m16:53:11.146025 [debug] [MainThread]: Command `dbt run` failed at 16:53:11.145109 after 2.86 seconds
[0m16:53:11.146025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A028E7A9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A04D848B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A0285AEC60>]}
[0m16:53:11.146025 [debug] [MainThread]: Flushing usage events
[0m16:53:12.175609 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:53:51.037355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04CC10560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04E1D2EA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04E1D2ED0>]}


============================== 16:53:51.041427 | 24c06186-b22f-4af0-8bb9-968dfd1e5d05 ==============================
[0m16:53:51.041427 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:53:51.041427 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select linkedin_jobs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m16:53:51.762292 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:53:51.762292 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:53:51.762292 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:53:52.409629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24c06186-b22f-4af0-8bb9-968dfd1e5d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04A125130>]}
[0m16:53:52.467394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24c06186-b22f-4af0-8bb9-968dfd1e5d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F06FFD5DC0>]}
[0m16:53:52.467394 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:53:52.697482 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:53:52.749472 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:53:52.751473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '24c06186-b22f-4af0-8bb9-968dfd1e5d05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04CC10D10>]}
[0m16:53:53.724847 [error] [MainThread]: Encountered an error:
Compilation Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  expected token ',', got 'schema'
    line 9
      schema='silver'
[0m16:53:53.728769 [debug] [MainThread]: Command `dbt run` failed at 16:53:53.727778 after 2.81 seconds
[0m16:53:53.729763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F04E1D1640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F07191D340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F070769F40>]}
[0m16:53:53.730762 [debug] [MainThread]: Flushing usage events
[0m16:53:54.704719 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:55:01.373205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EDF9ECA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EDF9ECD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EDF9EC0B0>]}


============================== 16:55:01.377324 | 5a471066-16fe-4838-abb4-79db44b2c89d ==============================
[0m16:55:01.377324 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:55:01.377324 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m16:55:02.673275 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:55:02.673275 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:55:02.673275 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:55:04.774633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a471066-16fe-4838-abb4-79db44b2c89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E84079910>]}
[0m16:55:04.831742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a471066-16fe-4838-abb4-79db44b2c89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E83E2CE90>]}
[0m16:55:04.832749 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:55:05.147431 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:55:05.208869 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:55:05.208869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5a471066-16fe-4838-abb4-79db44b2c89d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E8411E390>]}
[0m16:55:06.509678 [error] [MainThread]: Encountered an error:
Parsing Error
  at path ['schema']: None is not of type 'string'
[0m16:55:06.509678 [debug] [MainThread]: Command `dbt run` failed at 16:55:06.509678 after 5.25 seconds
[0m16:55:06.510772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EDEE75550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022EE257CE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E83FDE660>]}
[0m16:55:06.510772 [debug] [MainThread]: Flushing usage events
[0m16:55:07.649250 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:56:10.239557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B1D13A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B1D11730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B1494080>]}


============================== 16:56:10.243627 | dd85190f-819b-481a-adce-77edad8e7ef5 ==============================
[0m16:56:10.243627 [info ] [MainThread]: Running with dbt=1.10.4
[0m16:56:10.244626 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select linkedin_jobs', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'target_path': 'None'}
[0m16:56:11.006008 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m16:56:11.006008 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m16:56:11.007221 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m16:56:11.635209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd85190f-819b-481a-adce-77edad8e7ef5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B1A05A30>]}
[0m16:56:11.691823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd85190f-819b-481a-adce-77edad8e7ef5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9D55AD8B0>]}
[0m16:56:11.692838 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m16:56:11.977556 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m16:56:12.036799 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:56:12.038235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'dd85190f-819b-481a-adce-77edad8e7ef5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B2030D40>]}
[0m16:56:13.182430 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid sources config given in models\test\sources.yml @ sources: {'name': 'silver_layer', 'database': 'demo', 'schema': 'silver', 'tables': [{'name': 'indeed_cleaned', 'description': 'Cleaned Indeed jobs with standardized schema'}, {'name': 'linkedin_cleaned', 'description': 'Cleaned LinkedIn jobs with standardized schema'}, {'name': None}]} - at path ['tables'][2]['name']: None is not of type 'string'
[0m16:56:13.183383 [debug] [MainThread]: Command `dbt run` failed at 16:56:13.183383 after 3.05 seconds
[0m16:56:13.183383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9D6EC8F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9B1FF3230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A9D6EAF770>]}
[0m16:56:13.184447 [debug] [MainThread]: Flushing usage events
[0m16:56:14.196709 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:00:17.112541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFC6D1820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFC6D17C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFC610A70>]}


============================== 17:00:17.116577 | bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e ==============================
[0m17:00:17.116577 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:00:17.117580 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select linkedin_jobs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m17:00:17.848272 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:00:17.848272 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:00:17.849261 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:00:18.527384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA013F080>]}
[0m17:00:18.586880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ED9F800FB0>]}
[0m17:00:18.587883 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:00:18.855931 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:00:18.912945 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:00:18.913585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA16680B0>]}
[0m17:00:20.123492 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:00:20.130480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA18512B0>]}
[0m17:00:20.178624 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:00:20.181548 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:00:20.202515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA184E660>]}
[0m17:00:20.202515 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:00:20.203525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA1848DA0>]}
[0m17:00:20.204525 [info ] [MainThread]: 
[0m17:00:20.205531 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:00:20.205531 [info ] [MainThread]: 
[0m17:00:20.205531 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:20.205531 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:00:20.206782 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:20.206782 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:00:20.207861 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:00:20.207861 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:00:20.207861 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:00:20.647305 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=baa406e8-0a8d-4761-9dde-f3a0a4e13cc5) - Created
[0m17:00:20.935636 [debug] [ThreadPool]: SQL status: OK in 0.730 seconds
[0m17:00:20.936637 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=baa406e8-0a8d-4761-9dde-f3a0a4e13cc5, command-id=e5ca7edd-90c3-4b2a-9a09-0d51cc261b63) - Closing
[0m17:00:20.942758 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_bronze, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:20.942758 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_bronze'
[0m17:00:20.951297 [debug] [ThreadPool]: Using databricks connection "list_demo_bronze"
[0m17:00:20.952374 [debug] [ThreadPool]: On list_demo_bronze: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'bronze'

  
[0m17:00:20.952374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:00:21.164710 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ee1d94a1-a666-474d-a4cd-d1197c55b91e) - Created
[0m17:00:21.726053 [debug] [ThreadPool]: SQL status: OK in 0.770 seconds
[0m17:00:21.731677 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ee1d94a1-a666-474d-a4cd-d1197c55b91e, command-id=389c3a77-b371-4ee9-9eeb-5eea5b24ea28) - Closing
[0m17:00:21.731677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDA0566330>]}
[0m17:00:21.734685 [debug] [Thread-1 (]: Began running node model.jra_dbt.linkedin_jobs
[0m17:00:21.735678 [info ] [Thread-1 (]: 1 of 1 START sql incremental model bronze.linkedin_jobs ........................ [RUN]
[0m17:00:21.735678 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.linkedin_jobs, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:21.736677 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.linkedin_jobs'
[0m17:00:21.736677 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.linkedin_jobs
[0m17:00:21.742695 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.linkedin_jobs"
[0m17:00:21.743688 [debug] [Thread-1 (]: Began executing node model.jra_dbt.linkedin_jobs
[0m17:00:21.771489 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:00:21.779541 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:00:21.779541 [debug] [Thread-1 (]: Safe create: False
[0m17:00:21.794057 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m17:00:21.794057 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:21.795065 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

      create or replace temporary view `linkedin_jobs__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)
    
[0m17:00:21.795065 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:00:21.990836 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=da1da0ba-a349-43a2-91e6-8396e83968b5) - Created
[0m17:00:22.630958 [debug] [Thread-1 (]: SQL status: OK in 0.840 seconds
[0m17:00:22.633952 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=da1da0ba-a349-43a2-91e6-8396e83968b5, command-id=d3b799e5-74d0-4cc3-ab4c-a322749d1b3f) - Closing
[0m17:00:22.633952 [debug] [Thread-1 (]: No existing relation found
[0m17:00:22.646354 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:22.646354 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
DESCRIBE TABLE `linkedin_jobs__dbt_tmp`

  
[0m17:00:22.939822 [debug] [Thread-1 (]: SQL status: OK in 0.290 seconds
[0m17:00:22.943850 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=da1da0ba-a349-43a2-91e6-8396e83968b5, command-id=91f1d76c-8b94-4c58-892f-5bae782701a6) - Closing
[0m17:00:22.972612 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m17:00:22.973614 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:22.973614 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
  create or replace table `demo`.`bronze`.`linkedin_jobs`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      base_salary struct<currency:string,max_amount:double,min_amount:double,payment_period:string>,
    
      timestamp timestamp,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/linkedin_jobs'
  
  

  
[0m17:00:23.472176 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
  create or replace table `demo`.`bronze`.`linkedin_jobs`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      base_salary struct<currency:string,max_amount:double,min_amount:double,payment_period:string>,
    
      timestamp timestamp,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/linkedin_jobs'
  
  

  
: [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:118)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createTableDryRun(ManagedCatalogClientImpl.scala:1905)
	at com.databricks.sql.managedcatalog.PermissionEnforcingManagedCatalog.createTable(PermissionEnforcingManagedCatalog.scala:481)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createTable$1(ProfiledManagedCatalog.scala:197)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createTable(ProfiledManagedCatalog.scala:197)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableInternal(ManagedCatalogSessionCatalog.scala:1053)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableDryRun(ManagedCatalogSessionCatalog.scala:997)
	at com.databricks.sql.temptable.SessionCatalogTableAPIRouter$.createTableDryRun(SessionCatalogTableAPIRouter.scala:49)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:1382)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:925)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:118)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createTableDryRun(ManagedCatalogClientImpl.scala:1905)
		at com.databricks.sql.managedcatalog.PermissionEnforcingManagedCatalog.createTable(PermissionEnforcingManagedCatalog.scala:481)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createTable$1(ProfiledManagedCatalog.scala:197)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createTable(ProfiledManagedCatalog.scala:197)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableInternal(ManagedCatalogSessionCatalog.scala:1053)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableDryRun(ManagedCatalogSessionCatalog.scala:997)
		at com.databricks.sql.temptable.SessionCatalogTableAPIRouter$.createTableDryRun(SessionCatalogTableAPIRouter.scala:49)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:1382)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:925)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=0bba5f92-51e8-4d7e-8bbc-88ab430a7751
[0m17:00:23.618899 [debug] [Thread-1 (]: Database Error in model linkedin_jobs (models\test\linkedin_jobs.sql)
  [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
  compiled code at target\run\jra_dbt\models\test\linkedin_jobs.sql
[0m17:00:23.621858 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bf7c6c78-5ed2-4d45-aaab-ee751e2ada2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFBF361B0>]}
[0m17:00:23.622237 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model bronze.linkedin_jobs ............... [[31mERROR[0m in 1.88s]
[0m17:00:23.623237 [debug] [Thread-1 (]: Finished running node model.jra_dbt.linkedin_jobs
[0m17:00:23.623237 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.linkedin_jobs' to be skipped because of status 'error'.  Reason: Database Error in model linkedin_jobs (models\test\linkedin_jobs.sql)
  [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
  compiled code at target\run\jra_dbt\models\test\linkedin_jobs.sql.
[0m17:00:23.625430 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=1.8927528858184814s, language=None, compute-name=) - Reusing connection previously named master
[0m17:00:23.625430 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:00:23.626532 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:00:23.626532 [debug] [MainThread]: On list_demo: Close
[0m17:00:23.627464 [debug] [MainThread]: Databricks adapter: Connection(session-id=baa406e8-0a8d-4761-9dde-f3a0a4e13cc5) - Closing
[0m17:00:23.688802 [debug] [MainThread]: Connection 'list_demo_bronze' was properly closed.
[0m17:00:23.689902 [debug] [MainThread]: On list_demo_bronze: Close
[0m17:00:23.689902 [debug] [MainThread]: Databricks adapter: Connection(session-id=ee1d94a1-a666-474d-a4cd-d1197c55b91e) - Closing
[0m17:00:23.754923 [debug] [MainThread]: Connection 'model.jra_dbt.linkedin_jobs' was properly closed.
[0m17:00:23.755918 [debug] [MainThread]: On model.jra_dbt.linkedin_jobs: Close
[0m17:00:23.755918 [debug] [MainThread]: Databricks adapter: Connection(session-id=da1da0ba-a349-43a2-91e6-8396e83968b5) - Closing
[0m17:00:24.392514 [info ] [MainThread]: 
[0m17:00:24.394465 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 4.19 seconds (4.19s).
[0m17:00:24.395448 [debug] [MainThread]: Command end result
[0m17:00:24.421127 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:00:24.423612 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:00:24.428187 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:00:24.429243 [info ] [MainThread]: 
[0m17:00:24.430314 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:00:24.430987 [info ] [MainThread]: 
[0m17:00:24.430987 [error] [MainThread]: [31mFailure in model linkedin_jobs (models\test\linkedin_jobs.sql)[0m
[0m17:00:24.432077 [error] [MainThread]:   Database Error in model linkedin_jobs (models\test\linkedin_jobs.sql)
  [RequestId=3a53dbec-f7d2-427b-a89f-2bb7baa7c95e ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-bronze/linkedin_jobs' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.linkedin_jobs.
  compiled code at target\run\jra_dbt\models\test\linkedin_jobs.sql
[0m17:00:24.432077 [info ] [MainThread]: 
[0m17:00:24.433088 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\linkedin_jobs.sql
[0m17:00:24.433088 [info ] [MainThread]: 
[0m17:00:24.433088 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:00:24.434091 [debug] [MainThread]: Command `dbt run` failed at 17:00:24.434091 after 7.43 seconds
[0m17:00:24.434091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFBF5EC60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFBF5C0E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDFBF5C440>]}
[0m17:00:24.435074 [debug] [MainThread]: Flushing usage events
[0m17:00:25.468261 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:00:49.669110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5FEB0650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5FF474A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5FEB29F0>]}


============================== 17:00:49.673110 | 80a171d4-fa9b-45f3-8b54-41f7f7dd959e ==============================
[0m17:00:49.673110 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:00:49.673491 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_jobs', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m17:00:50.407742 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:00:50.408742 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:00:50.408742 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:00:51.032452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B62D0C560>]}
[0m17:00:51.088740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7F4279B0>]}
[0m17:00:51.088740 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:00:51.327756 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:00:51.443614 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:00:51.443614 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:00:51.448614 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:00:51.468829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7F625F10>]}
[0m17:00:51.519228 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:00:51.521392 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:00:51.543173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B04C18530>]}
[0m17:00:51.544172 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:00:51.544172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B037499D0>]}
[0m17:00:51.545180 [info ] [MainThread]: 
[0m17:00:51.545180 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:00:51.546174 [info ] [MainThread]: 
[0m17:00:51.546174 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:51.547174 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:00:51.547882 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:51.547882 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:00:51.547882 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:00:51.547882 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:00:51.549099 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:00:51.778316 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=2741347c-f95a-4023-88da-c1725b16041d) - Created
[0m17:00:51.924323 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m17:00:51.927398 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2741347c-f95a-4023-88da-c1725b16041d, command-id=3d294277-b92c-4c75-aa60-39256ca7b311) - Closing
[0m17:00:51.936372 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_bronze, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:51.936372 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_bronze'
[0m17:00:51.950675 [debug] [ThreadPool]: Using databricks connection "list_demo_bronze"
[0m17:00:51.951674 [debug] [ThreadPool]: On list_demo_bronze: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'bronze'

  
[0m17:00:51.951674 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:00:52.146573 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=2bfa51d5-75d4-4568-9797-4621865ec9d2) - Created
[0m17:00:52.486878 [debug] [ThreadPool]: SQL status: OK in 0.540 seconds
[0m17:00:52.495906 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2bfa51d5-75d4-4568-9797-4621865ec9d2, command-id=5d5d66db-eb11-41b8-bf5c-c644a21aec16) - Closing
[0m17:00:52.496934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5D6698B0>]}
[0m17:00:52.502900 [debug] [Thread-1 (]: Began running node model.jra_dbt.linkedin_jobs
[0m17:00:52.504007 [info ] [Thread-1 (]: 1 of 1 START sql incremental model bronze.linkedin_jobs ........................ [RUN]
[0m17:00:52.504905 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.linkedin_jobs, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:00:52.505866 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.linkedin_jobs'
[0m17:00:52.505866 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.linkedin_jobs
[0m17:00:52.514865 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.linkedin_jobs"
[0m17:00:52.515866 [debug] [Thread-1 (]: Began executing node model.jra_dbt.linkedin_jobs
[0m17:00:52.542195 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:00:52.550273 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:00:52.550273 [debug] [Thread-1 (]: Safe create: False
[0m17:00:52.563271 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m17:00:52.563271 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:52.563271 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

      create or replace temporary view `linkedin_jobs__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/linkedin/',
  format => 'parquet'
)
    
[0m17:00:52.564556 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:00:52.778762 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d) - Created
[0m17:00:53.326661 [debug] [Thread-1 (]: SQL status: OK in 0.760 seconds
[0m17:00:53.327663 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d, command-id=cfa5f90f-fb47-48d9-8ae8-577c1cbe5e91) - Closing
[0m17:00:53.328668 [debug] [Thread-1 (]: No existing relation found
[0m17:00:53.336669 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:53.336669 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
DESCRIBE TABLE `linkedin_jobs__dbt_tmp`

  
[0m17:00:53.618703 [debug] [Thread-1 (]: SQL status: OK in 0.280 seconds
[0m17:00:53.624703 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d, command-id=4d65ac7d-a13f-4bbc-a891-7d2a3f83f802) - Closing
[0m17:00:53.656379 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_jobs"
[0m17:00:53.656379 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:53.657421 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    
  create or replace table `demo`.`bronze`.`linkedin_jobs`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      base_salary struct<currency:string,max_amount:double,min_amount:double,payment_period:string>,
    
      timestamp timestamp,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/linkedin_jobs'
  
  

  
[0m17:00:54.407372 [debug] [Thread-1 (]: SQL status: OK in 0.750 seconds
[0m17:00:54.407372 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d, command-id=e82bd2eb-fdaa-4c34-a7fc-5588dee49741) - Closing
[0m17:00:54.411368 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:00:54.411368 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_jobs"
[0m17:00:54.412368 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_jobs"} */

    insert into `demo`.`bronze`.`linkedin_jobs` select * from `linkedin_jobs__dbt_tmp`
  
[0m17:00:58.587945 [debug] [Thread-1 (]: SQL status: OK in 4.180 seconds
[0m17:00:58.588943 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d, command-id=973e34fd-2c7d-4172-95ac-26dac5bf0b96) - Closing
[0m17:00:58.601993 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80a171d4-fa9b-45f3-8b54-41f7f7dd959e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5D3C41D0>]}
[0m17:00:58.602970 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model bronze.linkedin_jobs ................... [[32mOK[0m in 6.10s]
[0m17:00:58.604007 [debug] [Thread-1 (]: Finished running node model.jra_dbt.linkedin_jobs
[0m17:00:58.607926 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.110991477966309s, language=None, compute-name=) - Reusing connection previously named master
[0m17:00:58.608964 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:00:58.608964 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:00:58.609973 [debug] [MainThread]: On list_demo: Close
[0m17:00:58.610973 [debug] [MainThread]: Databricks adapter: Connection(session-id=2741347c-f95a-4023-88da-c1725b16041d) - Closing
[0m17:00:58.670119 [debug] [MainThread]: Connection 'list_demo_bronze' was properly closed.
[0m17:00:58.672116 [debug] [MainThread]: On list_demo_bronze: Close
[0m17:00:58.673111 [debug] [MainThread]: Databricks adapter: Connection(session-id=2bfa51d5-75d4-4568-9797-4621865ec9d2) - Closing
[0m17:00:58.744924 [debug] [MainThread]: Connection 'model.jra_dbt.linkedin_jobs' was properly closed.
[0m17:00:58.744924 [debug] [MainThread]: On model.jra_dbt.linkedin_jobs: Close
[0m17:00:58.744924 [debug] [MainThread]: Databricks adapter: Connection(session-id=66841cbc-9c6a-4a16-bcad-95c5b146a60d) - Closing
[0m17:00:59.156801 [info ] [MainThread]: 
[0m17:00:59.158884 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 7.61 seconds (7.61s).
[0m17:00:59.162177 [debug] [MainThread]: Command end result
[0m17:00:59.192665 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:00:59.195600 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:00:59.198989 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:00:59.199981 [info ] [MainThread]: 
[0m17:00:59.200391 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:00:59.200391 [info ] [MainThread]: 
[0m17:00:59.201449 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:00:59.201449 [debug] [MainThread]: Command `dbt run` succeeded at 17:00:59.201449 after 9.63 seconds
[0m17:00:59.202485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B5FF474A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B602E8C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B602EAED0>]}
[0m17:00:59.202485 [debug] [MainThread]: Flushing usage events
[0m17:01:00.161618 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:01:08.367560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0913BEFF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C091E5D040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C093061070>]}


============================== 17:01:08.371044 | 64bbf096-dc78-444b-aa18-11a16f178893 ==============================
[0m17:01:08.371044 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:01:08.372037 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_jobs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m17:01:09.080688 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:01:09.080688 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:01:09.080688 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:01:09.729801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C08F170230>]}
[0m17:01:09.789572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B45A0800>]}
[0m17:01:09.790574 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:01:10.027985 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:01:10.129923 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:01:10.130921 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:01:10.136043 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:01:10.163759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B54E9130>]}
[0m17:01:10.213657 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:01:10.216186 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:01:10.238870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B6758140>]}
[0m17:01:10.238870 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:01:10.238870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B54E8770>]}
[0m17:01:10.240371 [info ] [MainThread]: 
[0m17:01:10.241473 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:01:10.241473 [info ] [MainThread]: 
[0m17:01:10.242485 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:10.242485 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:01:10.243676 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:10.244181 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:01:10.244181 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:01:10.244181 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:01:10.244181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:10.489223 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=606edad5-c483-4d21-b372-adf58c402d63) - Created
[0m17:01:10.629836 [debug] [ThreadPool]: SQL status: OK in 0.390 seconds
[0m17:01:10.630836 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=606edad5-c483-4d21-b372-adf58c402d63, command-id=ee371772-525c-4d9a-95fb-a54cc8b13034) - Closing
[0m17:01:10.638789 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_bronze, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:10.638789 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_bronze'
[0m17:01:10.647830 [debug] [ThreadPool]: Using databricks connection "list_demo_bronze"
[0m17:01:10.648830 [debug] [ThreadPool]: On list_demo_bronze: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'bronze'

  
[0m17:01:10.648830 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:10.853957 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=8a7084dc-00a0-45e1-8c2a-c4a6fbd5ce8b) - Created
[0m17:01:11.436581 [debug] [ThreadPool]: SQL status: OK in 0.790 seconds
[0m17:01:11.440769 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=8a7084dc-00a0-45e1-8c2a-c4a6fbd5ce8b, command-id=56f71dbd-2631-4c85-910d-35300d2514de) - Closing
[0m17:01:11.441768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C08F1A32C0>]}
[0m17:01:11.443768 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_jobs
[0m17:01:11.445094 [info ] [Thread-1 (]: 1 of 1 START sql incremental model bronze.indeed_jobs .......................... [RUN]
[0m17:01:11.445094 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_jobs, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:11.446320 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_jobs'
[0m17:01:11.446320 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_jobs
[0m17:01:11.455342 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_jobs"
[0m17:01:11.456342 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_jobs
[0m17:01:11.481028 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:01:11.488735 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:01:11.489735 [debug] [Thread-1 (]: Safe create: False
[0m17:01:11.501437 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_jobs"
[0m17:01:11.502465 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_jobs"
[0m17:01:11.503418 [debug] [Thread-1 (]: On model.jra_dbt.indeed_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_jobs"} */

      create or replace temporary view `indeed_jobs__dbt_tmp` as
      SELECT 
  *,
  current_timestamp() AS ingest_dts,
  _metadata.file_path AS source_file,
  -- Add partition columns
  year(current_date()) as year,
  month(current_date()) as month,
  day(current_date()) as day
FROM read_files(
  's3://jla-raw-datalake/raw/Indeed/',
  format => 'parquet'
)
    
[0m17:01:11.503418 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:01:11.725007 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121) - Created
[0m17:01:12.525714 [debug] [Thread-1 (]: SQL status: OK in 1.020 seconds
[0m17:01:12.528127 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121, command-id=878e80d2-3138-4810-ac70-51ff02926c72) - Closing
[0m17:01:12.529114 [debug] [Thread-1 (]: No existing relation found
[0m17:01:12.544696 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_jobs"
[0m17:01:12.544696 [debug] [Thread-1 (]: On model.jra_dbt.indeed_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_jobs"} */

    
DESCRIBE TABLE `indeed_jobs__dbt_tmp`

  
[0m17:01:12.848805 [debug] [Thread-1 (]: SQL status: OK in 0.300 seconds
[0m17:01:12.854110 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121, command-id=a977d719-0b01-4950-82ec-d4e952e4c2d5) - Closing
[0m17:01:12.886578 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_jobs"
[0m17:01:12.887603 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_jobs"
[0m17:01:12.888597 [debug] [Thread-1 (]: On model.jra_dbt.indeed_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_jobs"} */

    
  create or replace table `demo`.`bronze`.`indeed_jobs`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      job_posted_date string,
    
      MIN_AMOUNT double,
    
      MAX_AMOUNT double,
    
      timestamp timestamp_ntz,
    
      _rescued_data string,
    
      ingest_dts timestamp,
    
      source_file string,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-bronze/indeed_jobs'
  
  

  
[0m17:01:14.156260 [debug] [Thread-1 (]: SQL status: OK in 1.270 seconds
[0m17:01:14.156260 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121, command-id=2d83bd32-f77b-45a5-9ee7-6a48b4ac3b74) - Closing
[0m17:01:14.159286 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:01:14.160302 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_jobs"
[0m17:01:14.160302 [debug] [Thread-1 (]: On model.jra_dbt.indeed_jobs: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_jobs"} */

    insert into `demo`.`bronze`.`indeed_jobs` select * from `indeed_jobs__dbt_tmp`
  
[0m17:01:17.723320 [debug] [Thread-1 (]: SQL status: OK in 3.560 seconds
[0m17:01:17.724336 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121, command-id=aecb3e85-847f-4aed-8530-92db30dd0cab) - Closing
[0m17:01:17.740214 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '64bbf096-dc78-444b-aa18-11a16f178893', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B6763050>]}
[0m17:01:17.741149 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model bronze.indeed_jobs ..................... [[32mOK[0m in 6.29s]
[0m17:01:17.742163 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_jobs
[0m17:01:17.744146 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=6.300366401672363s, language=None, compute-name=) - Reusing connection previously named master
[0m17:01:17.744146 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:01:17.744146 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:01:17.745141 [debug] [MainThread]: On list_demo: Close
[0m17:01:17.746141 [debug] [MainThread]: Databricks adapter: Connection(session-id=606edad5-c483-4d21-b372-adf58c402d63) - Closing
[0m17:01:17.804018 [debug] [MainThread]: Connection 'list_demo_bronze' was properly closed.
[0m17:01:17.805017 [debug] [MainThread]: On list_demo_bronze: Close
[0m17:01:17.805017 [debug] [MainThread]: Databricks adapter: Connection(session-id=8a7084dc-00a0-45e1-8c2a-c4a6fbd5ce8b) - Closing
[0m17:01:17.865735 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_jobs' was properly closed.
[0m17:01:17.866735 [debug] [MainThread]: On model.jra_dbt.indeed_jobs: Close
[0m17:01:17.866735 [debug] [MainThread]: Databricks adapter: Connection(session-id=8fb862b9-de60-4209-8640-f9d9f1b3e121) - Closing
[0m17:01:18.237408 [info ] [MainThread]: 
[0m17:01:18.239264 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 7.99 seconds (7.99s).
[0m17:01:18.239264 [debug] [MainThread]: Command end result
[0m17:01:18.259822 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:01:18.261823 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:01:18.266041 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:01:18.266041 [info ] [MainThread]: 
[0m17:01:18.267041 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:01:18.267041 [info ] [MainThread]: 
[0m17:01:18.268057 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:01:18.268057 [debug] [MainThread]: Command `dbt run` succeeded at 17:01:18.268057 after 10.01 seconds
[0m17:01:18.269055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C091F19C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C091394A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C0B528AE10>]}
[0m17:01:18.269055 [debug] [MainThread]: Flushing usage events
[0m17:01:19.224993 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:01:36.198557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218ABF10560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218ABF137D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218ABF130B0>]}


============================== 17:01:36.203203 | de963d25-07f8-43c9-998e-fed2753c9822 ==============================
[0m17:01:36.203203 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:01:36.204233 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m17:01:36.945435 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:01:36.945435 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:01:36.945435 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:01:37.585794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218AC4FD520>]}
[0m17:01:37.643631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218CF4327B0>]}
[0m17:01:37.643631 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:01:37.859649 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:01:37.911391 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:01:37.912536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218ABF10D10>]}
[0m17:01:39.118807 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:01:39.125243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D0CBD130>]}
[0m17:01:39.173642 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:01:39.175620 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:01:39.197461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D0CC38C0>]}
[0m17:01:39.197461 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:01:39.198473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D0C88890>]}
[0m17:01:39.199469 [info ] [MainThread]: 
[0m17:01:39.200501 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:01:39.200501 [info ] [MainThread]: 
[0m17:01:39.200501 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:39.201898 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:01:39.202497 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:39.202497 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:01:39.203506 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:01:39.203506 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:01:39.204508 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:39.463776 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=0a0ee92e-e5a6-4852-8241-3ef3ab8f7b8a) - Created
[0m17:01:39.635128 [debug] [ThreadPool]: SQL status: OK in 0.430 seconds
[0m17:01:39.637123 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=0a0ee92e-e5a6-4852-8241-3ef3ab8f7b8a, command-id=a984d9f0-7e9c-4e0a-b801-d6c739560786) - Closing
[0m17:01:39.649209 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:39.649209 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:01:39.661673 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:01:39.661673 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:01:39.661673 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:39.884894 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=65fe2833-11ec-4d1e-80d7-4c479668fe4e) - Created
[0m17:01:40.159938 [debug] [ThreadPool]: SQL status: OK in 0.500 seconds
[0m17:01:40.164897 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=65fe2833-11ec-4d1e-80d7-4c479668fe4e, command-id=c83330e8-3eac-4f52-9f5b-ab46b3b4acba) - Closing
[0m17:01:40.165908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D0C88B00>]}
[0m17:01:40.171627 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m17:01:40.171627 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.indeed_cleaned ....................... [RUN]
[0m17:01:40.172673 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:01:40.172673 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m17:01:40.172673 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m17:01:40.178694 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m17:01:40.179651 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m17:01:40.204924 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:01:40.213346 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:01:40.213346 [debug] [Thread-1 (]: Safe create: False
[0m17:01:40.225821 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m17:01:40.226854 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:01:40.226854 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(cast(job_posted_date As timestamp), 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`bronze`.`indeed_jobs`
    
[0m17:01:40.227907 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:01:40.517934 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=14d5c7d0-3a55-485d-a97d-e57494962ff5) - Created
[0m17:01:40.953086 [debug] [Thread-1 (]: SQL status: OK in 0.730 seconds
[0m17:01:40.954448 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=14d5c7d0-3a55-485d-a97d-e57494962ff5, command-id=fb3daf7f-de1f-4117-adc9-42f3c2808d5b) - Closing
[0m17:01:40.954448 [debug] [Thread-1 (]: No existing relation found
[0m17:01:40.961473 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:01:40.961473 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m17:01:41.205745 [debug] [Thread-1 (]: SQL status: OK in 0.240 seconds
[0m17:01:41.210818 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=14d5c7d0-3a55-485d-a97d-e57494962ff5, command-id=47264dd1-9a2e-4990-9a8d-80ab5e439b6b) - Closing
[0m17:01:41.242645 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m17:01:41.243594 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:01:41.243594 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`silver`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
[0m17:01:42.290455 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`silver`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
: [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.managedcatalog.UnityCatalogServiceException: [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:118)
	at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
	at com.databricks.managedcatalog.ManagedCatalogClientImpl.createTableDryRun(ManagedCatalogClientImpl.scala:1905)
	at com.databricks.sql.managedcatalog.PermissionEnforcingManagedCatalog.createTable(PermissionEnforcingManagedCatalog.scala:481)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createTable$1(ProfiledManagedCatalog.scala:197)
	at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
	at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createTable(ProfiledManagedCatalog.scala:197)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableInternal(ManagedCatalogSessionCatalog.scala:1053)
	at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableDryRun(ManagedCatalogSessionCatalog.scala:997)
	at com.databricks.sql.temptable.SessionCatalogTableAPIRouter$.createTableDryRun(SessionCatalogTableAPIRouter.scala:49)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:1382)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:925)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:118)
		at com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:63)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:251)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7378)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7364)
		at com.databricks.managedcatalog.ManagedCatalogClientImpl.createTableDryRun(ManagedCatalogClientImpl.scala:1905)
		at com.databricks.sql.managedcatalog.PermissionEnforcingManagedCatalog.createTable(PermissionEnforcingManagedCatalog.scala:481)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$createTable$1(ProfiledManagedCatalog.scala:197)
		at org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1688)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)
		at com.databricks.sql.managedcatalog.ProfiledManagedCatalog.createTable(ProfiledManagedCatalog.scala:197)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableInternal(ManagedCatalogSessionCatalog.scala:1053)
		at com.databricks.sql.managedcatalog.ManagedCatalogSessionCatalog.createTableDryRun(ManagedCatalogSessionCatalog.scala:997)
		at com.databricks.sql.temptable.SessionCatalogTableAPIRouter$.createTableDryRun(SessionCatalogTableAPIRouter.scala:49)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.updateCatalog(CreateDeltaTableCommand.scala:1382)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:925)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=effa7876-b1e0-4efa-8058-f48363f0d2f7
[0m17:01:42.295539 [debug] [Thread-1 (]: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m17:01:42.297719 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'de963d25-07f8-43c9-998e-fed2753c9822', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D0CE8650>]}
[0m17:01:42.297719 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model silver.indeed_cleaned .............. [[31mERROR[0m in 2.12s]
[0m17:01:42.298970 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m17:01:42.298970 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.indeed_cleaned' to be skipped because of status 'error'.  Reason: Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql.
[0m17:01:42.301326 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=2.1344454288482666s, language=None, compute-name=) - Reusing connection previously named master
[0m17:01:42.301326 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:01:42.302494 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:01:42.302494 [debug] [MainThread]: On list_demo: Close
[0m17:01:42.302494 [debug] [MainThread]: Databricks adapter: Connection(session-id=0a0ee92e-e5a6-4852-8241-3ef3ab8f7b8a) - Closing
[0m17:01:42.361342 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:01:42.362519 [debug] [MainThread]: On list_demo_silver: Close
[0m17:01:42.362519 [debug] [MainThread]: Databricks adapter: Connection(session-id=65fe2833-11ec-4d1e-80d7-4c479668fe4e) - Closing
[0m17:01:42.432940 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m17:01:42.432940 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m17:01:42.433939 [debug] [MainThread]: Databricks adapter: Connection(session-id=14d5c7d0-3a55-485d-a97d-e57494962ff5) - Closing
[0m17:01:42.708683 [info ] [MainThread]: 
[0m17:01:42.710728 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.51 seconds (3.51s).
[0m17:01:42.712663 [debug] [MainThread]: Command end result
[0m17:01:42.735438 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:01:42.736492 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:01:42.740469 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:01:42.740469 [info ] [MainThread]: 
[0m17:01:42.741473 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:01:42.742479 [info ] [MainThread]: 
[0m17:01:42.742479 [error] [MainThread]: [31mFailure in model indeed_cleaned (models\test\indeed_cleaned.sql)[0m
[0m17:01:42.742479 [error] [MainThread]:   Database Error in model indeed_cleaned (models\test\indeed_cleaned.sql)
  [RequestId=a1d9a50b-9478-4a81-b500-292655709aab ErrorClass=INVALID_PARAMETER_VALUE.LOCATION_OVERLAP] Input path url 's3://jla-data-silver/indeed_cleaned' overlaps with other external tables or volumes within 'CreateTable' call. Conflicting tables/volumes: demo.demo_schema.indeed_cleaned.
  compiled code at target\run\jra_dbt\models\test\indeed_cleaned.sql
[0m17:01:42.742479 [info ] [MainThread]: 
[0m17:01:42.743961 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\indeed_cleaned.sql
[0m17:01:42.743961 [info ] [MainThread]: 
[0m17:01:42.743961 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:01:42.744999 [debug] [MainThread]: Command `dbt run` failed at 17:01:42.744999 after 6.68 seconds
[0m17:01:42.744999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218ABF3EE70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218AB85C470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218AB85C1D0>]}
[0m17:01:42.744999 [debug] [MainThread]: Flushing usage events
[0m17:01:43.783062 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:02:03.154604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BED6807D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDB872C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDB87290>]}


============================== 17:02:03.159024 | e1fc40ff-32f2-4373-a3ee-882def1e2f81 ==============================
[0m17:02:03.159024 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:02:03.160018 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select indeed_cleaned', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m17:02:03.887703 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:02:03.887703 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:02:03.887703 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:02:04.560837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEEC96EA0>]}
[0m17:02:04.621343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B9038E480>]}
[0m17:02:04.622344 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:02:04.892714 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:02:05.007606 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:02:05.007606 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:02:05.012606 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:02:05.031675 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B911A0D10>]}
[0m17:02:05.083323 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:02:05.085621 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:02:05.106412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B923448C0>]}
[0m17:02:05.106412 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:02:05.107548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B90E49A00>]}
[0m17:02:05.108568 [info ] [MainThread]: 
[0m17:02:05.109080 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:02:05.110110 [info ] [MainThread]: 
[0m17:02:05.111114 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:05.111114 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:02:05.112102 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:05.112102 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:02:05.112102 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:02:05.112102 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:02:05.112102 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:02:05.390213 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=ee233bfb-8d81-4746-9267-6412c57ac776) - Created
[0m17:02:05.533002 [debug] [ThreadPool]: SQL status: OK in 0.420 seconds
[0m17:02:05.535986 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=ee233bfb-8d81-4746-9267-6412c57ac776, command-id=c72cf83c-de4a-47c3-addd-ff9436025c81) - Closing
[0m17:02:05.544821 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:05.544821 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:02:05.552824 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:02:05.553824 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:02:05.553824 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:02:05.771671 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f569404a-f8fd-4287-97a2-5d1a04d989b9) - Created
[0m17:02:06.106953 [debug] [ThreadPool]: SQL status: OK in 0.550 seconds
[0m17:02:06.111954 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f569404a-f8fd-4287-97a2-5d1a04d989b9, command-id=f5293ae8-e932-40ef-bd24-ad4d3c029af6) - Closing
[0m17:02:06.112959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEAD974D0>]}
[0m17:02:06.117953 [debug] [Thread-1 (]: Began running node model.jra_dbt.indeed_cleaned
[0m17:02:06.118954 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.indeed_cleaned ....................... [RUN]
[0m17:02:06.119955 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.indeed_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:06.119955 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.indeed_cleaned'
[0m17:02:06.120955 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.indeed_cleaned
[0m17:02:06.131950 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.indeed_cleaned"
[0m17:02:06.132951 [debug] [Thread-1 (]: Began executing node model.jra_dbt.indeed_cleaned
[0m17:02:06.157123 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:02:06.166081 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:02:06.166081 [debug] [Thread-1 (]: Safe create: False
[0m17:02:06.179176 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m17:02:06.180158 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:02:06.181087 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

      create or replace temporary view `indeed_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 

    job_industries,
    
    MIN_AMOUNT as min_salary, 
    MAX_AMOUNT as max_salary, 
    date_format(to_date(cast(job_posted_date As timestamp), 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    0 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`bronze`.`indeed_jobs`
    
[0m17:02:06.181087 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:02:06.374954 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=b6329e36-6c2a-4339-a469-76a07db310c5) - Created
[0m17:02:06.829736 [debug] [Thread-1 (]: SQL status: OK in 0.650 seconds
[0m17:02:06.830750 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6329e36-6c2a-4339-a469-76a07db310c5, command-id=09112775-7b90-4024-ad4d-1f562130df0e) - Closing
[0m17:02:06.831753 [debug] [Thread-1 (]: No existing relation found
[0m17:02:06.838818 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:02:06.838818 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
DESCRIBE TABLE `indeed_cleaned__dbt_tmp`

  
[0m17:02:07.112704 [debug] [Thread-1 (]: SQL status: OK in 0.270 seconds
[0m17:02:07.114737 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6329e36-6c2a-4339-a469-76a07db310c5, command-id=9770d64f-0a0b-4eb1-9e77-89a082c5c3e3) - Closing
[0m17:02:07.146289 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.indeed_cleaned"
[0m17:02:07.146289 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:02:07.147254 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    
  create or replace table `demo`.`silver`.`indeed_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp_ntz,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/indeed_cleaned'
  
  

  
[0m17:02:07.945244 [debug] [Thread-1 (]: SQL status: OK in 0.800 seconds
[0m17:02:07.949238 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6329e36-6c2a-4339-a469-76a07db310c5, command-id=a1602cba-599a-47bc-acc2-942bc7d27590) - Closing
[0m17:02:07.955325 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:02:07.955325 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.indeed_cleaned"
[0m17:02:07.955325 [debug] [Thread-1 (]: On model.jra_dbt.indeed_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.indeed_cleaned"} */

    insert into `demo`.`silver`.`indeed_cleaned` select * from `indeed_cleaned__dbt_tmp`
  
[0m17:02:12.034515 [debug] [Thread-1 (]: SQL status: OK in 4.080 seconds
[0m17:02:12.034515 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=b6329e36-6c2a-4339-a469-76a07db310c5, command-id=4d752133-4990-4443-a210-1a1c6d1e5cb7) - Closing
[0m17:02:12.047597 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1fc40ff-32f2-4373-a3ee-882def1e2f81', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEAB94110>]}
[0m17:02:12.047987 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model silver.indeed_cleaned .................. [[32mOK[0m in 5.93s]
[0m17:02:12.049064 [debug] [Thread-1 (]: Finished running node model.jra_dbt.indeed_cleaned
[0m17:02:12.050007 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=5.936053514480591s, language=None, compute-name=) - Reusing connection previously named master
[0m17:02:12.050007 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:02:12.050007 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:02:12.051191 [debug] [MainThread]: On list_demo: Close
[0m17:02:12.051191 [debug] [MainThread]: Databricks adapter: Connection(session-id=ee233bfb-8d81-4746-9267-6412c57ac776) - Closing
[0m17:02:12.121809 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:02:12.122808 [debug] [MainThread]: On list_demo_silver: Close
[0m17:02:12.123807 [debug] [MainThread]: Databricks adapter: Connection(session-id=f569404a-f8fd-4287-97a2-5d1a04d989b9) - Closing
[0m17:02:12.196471 [debug] [MainThread]: Connection 'model.jra_dbt.indeed_cleaned' was properly closed.
[0m17:02:12.196471 [debug] [MainThread]: On model.jra_dbt.indeed_cleaned: Close
[0m17:02:12.196471 [debug] [MainThread]: Databricks adapter: Connection(session-id=b6329e36-6c2a-4339-a469-76a07db310c5) - Closing
[0m17:02:12.462030 [info ] [MainThread]: 
[0m17:02:12.463179 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 7.35 seconds (7.35s).
[0m17:02:12.465272 [debug] [MainThread]: Command end result
[0m17:02:12.488625 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:02:12.489669 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:02:12.494482 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:02:12.494482 [info ] [MainThread]: 
[0m17:02:12.495480 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:02:12.496167 [info ] [MainThread]: 
[0m17:02:12.497172 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:02:12.498174 [debug] [MainThread]: Command `dbt run` succeeded at 17:02:12.498174 after 9.44 seconds
[0m17:02:12.498174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDB29880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015BEDB282C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015B91157AA0>]}
[0m17:02:12.498174 [debug] [MainThread]: Flushing usage events
[0m17:02:13.451928 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:02:29.079033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD56402240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD564021E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD56401D00>]}


============================== 17:02:29.083042 | 9eae2445-0099-44d5-aff6-a51f35aef71b ==============================
[0m17:02:29.083042 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:02:29.084040 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select jobs_description', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'write_json': 'True'}
[0m17:02:29.812049 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:02:29.813023 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:02:29.813023 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:02:30.491969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD77203EF0>]}
[0m17:02:30.549807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD78B5DDC0>]}
[0m17:02:30.550855 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:02:30.789900 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:02:30.892291 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:02:30.893291 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:02:30.897561 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:02:30.921018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD79998800>]}
[0m17:02:30.972802 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:02:30.974768 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:02:31.012219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD7AC08050>]}
[0m17:02:31.012219 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:02:31.012855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD7AC03CE0>]}
[0m17:02:31.013863 [info ] [MainThread]: 
[0m17:02:31.013863 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:02:31.014873 [info ] [MainThread]: 
[0m17:02:31.014873 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:31.014873 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:02:31.016420 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:31.016420 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:02:31.016420 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:02:31.016420 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:02:31.017564 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:02:31.324937 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=370a6fd1-fa5b-406d-a4b2-c68079d0638b) - Created
[0m17:02:31.870364 [debug] [ThreadPool]: SQL status: OK in 0.850 seconds
[0m17:02:31.871366 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=370a6fd1-fa5b-406d-a4b2-c68079d0638b, command-id=63b4c00d-213a-4fc0-8689-09274d3e0915) - Closing
[0m17:02:31.878027 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:31.878027 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:02:31.887026 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:02:31.888029 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:02:31.888029 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:02:32.078530 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=f7e127c5-7de7-49d9-a272-59283448b950) - Created
[0m17:02:32.671992 [debug] [ThreadPool]: SQL status: OK in 0.780 seconds
[0m17:02:32.683906 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=f7e127c5-7de7-49d9-a272-59283448b950, command-id=14c6a9ed-b3d2-4191-8a37-b5f4a38ecfc5) - Closing
[0m17:02:32.685958 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD536A3140>]}
[0m17:02:32.693405 [debug] [Thread-1 (]: Began running node model.jra_dbt.jobs_description
[0m17:02:32.693405 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.jobs_description ..................... [RUN]
[0m17:02:32.694404 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.jobs_description, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:02:32.694404 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.jobs_description'
[0m17:02:32.695419 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.jobs_description
[0m17:02:32.707415 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.jobs_description"
[0m17:02:32.707415 [debug] [Thread-1 (]: Began executing node model.jra_dbt.jobs_description
[0m17:02:32.738498 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:02:32.746869 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:02:32.746869 [debug] [Thread-1 (]: Safe create: False
[0m17:02:32.762149 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_description"
[0m17:02:32.763171 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_description"
[0m17:02:32.763171 [debug] [Thread-1 (]: On model.jra_dbt.jobs_description: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

      create or replace temporary view `jobs_description__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`linkedin_cleaned`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`indeed_cleaned`
    
[0m17:02:32.763171 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:02:32.970511 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=f127f6a0-1182-480e-9630-a5fe357c7abc) - Created
[0m17:02:33.306605 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

      create or replace temporary view `jobs_description__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`linkedin_cleaned`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`indeed_cleaned`
    
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:959)
	... 43 more
, operation-id=c677ab04-e1e7-4e38-bf49-8366b99e0b88
[0m17:02:33.311119 [debug] [Thread-1 (]: Database Error in model jobs_description (models\test\jobs_description.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
  compiled code at target\run\jra_dbt\models\test\jobs_description.sql
[0m17:02:33.313734 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9eae2445-0099-44d5-aff6-a51f35aef71b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD7B053CB0>]}
[0m17:02:33.313734 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model silver.jobs_description ............ [[31mERROR[0m in 0.62s]
[0m17:02:33.315155 [debug] [Thread-1 (]: Finished running node model.jra_dbt.jobs_description
[0m17:02:33.316174 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.jobs_description' to be skipped because of status 'error'.  Reason: Database Error in model jobs_description (models\test\jobs_description.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
  compiled code at target\run\jra_dbt\models\test\jobs_description.sql.
[0m17:02:33.317452 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0.6304762363433838s, language=None, compute-name=) - Reusing connection previously named master
[0m17:02:33.317452 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:02:33.317452 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:02:33.318467 [debug] [MainThread]: On list_demo: Close
[0m17:02:33.318467 [debug] [MainThread]: Databricks adapter: Connection(session-id=370a6fd1-fa5b-406d-a4b2-c68079d0638b) - Closing
[0m17:02:33.396090 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:02:33.397172 [debug] [MainThread]: On list_demo_silver: Close
[0m17:02:33.398176 [debug] [MainThread]: Databricks adapter: Connection(session-id=f7e127c5-7de7-49d9-a272-59283448b950) - Closing
[0m17:02:33.483388 [debug] [MainThread]: Connection 'model.jra_dbt.jobs_description' was properly closed.
[0m17:02:33.484396 [debug] [MainThread]: On model.jra_dbt.jobs_description: Close
[0m17:02:33.485401 [debug] [MainThread]: Databricks adapter: Connection(session-id=f127f6a0-1182-480e-9630-a5fe357c7abc) - Closing
[0m17:02:33.570988 [info ] [MainThread]: 
[0m17:02:33.572007 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 2.56 seconds (2.56s).
[0m17:02:33.573988 [debug] [MainThread]: Command end result
[0m17:02:33.600695 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:02:33.602666 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:02:33.606652 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:02:33.606652 [info ] [MainThread]: 
[0m17:02:33.607634 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:02:33.608760 [info ] [MainThread]: 
[0m17:02:33.609656 [error] [MainThread]: [31mFailure in model jobs_description (models\test\jobs_description.sql)[0m
[0m17:02:33.609656 [error] [MainThread]:   Database Error in model jobs_description (models\test\jobs_description.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`silver`.`linkedin_cleaned` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 8 pos 5
  compiled code at target\run\jra_dbt\models\test\jobs_description.sql
[0m17:02:33.610977 [info ] [MainThread]: 
[0m17:02:33.610977 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\jobs_description.sql
[0m17:02:33.612028 [info ] [MainThread]: 
[0m17:02:33.612028 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:02:33.613029 [debug] [MainThread]: Command `dbt run` failed at 17:02:33.613029 after 4.64 seconds
[0m17:02:33.613029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD79B1D2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD5643B9E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AD564397C0>]}
[0m17:02:33.613029 [debug] [MainThread]: Flushing usage events
[0m17:02:34.556638 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:03:17.813700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A168BD2660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A168523A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A168522E40>]}


============================== 17:03:17.817700 | 643ea9a4-394d-49dc-a55b-30ba5bbf4c03 ==============================
[0m17:03:17.817700 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:03:17.818693 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'invocation_command': 'dbt run --select linkedin_cleaned', 'fail_fast': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'target_path': 'None', 'write_json': 'True'}
[0m17:03:18.513240 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:03:18.514873 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:03:18.514873 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:03:19.119750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A10B690890>]}
[0m17:03:19.177706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A169042150>]}
[0m17:03:19.178706 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:03:19.401051 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:03:19.501737 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:03:19.501737 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:03:19.505818 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:03:19.526070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A16BCA7DA0>]}
[0m17:03:19.574413 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:19.577467 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:19.602746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A10C656750>]}
[0m17:03:19.604076 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:03:19.604432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A10C6F5E20>]}
[0m17:03:19.605517 [info ] [MainThread]: 
[0m17:03:19.605517 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:03:19.605517 [info ] [MainThread]: 
[0m17:03:19.606552 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:19.606552 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:03:19.607771 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:19.607771 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:03:19.608772 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:03:19.608772 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:03:19.608772 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:19.840960 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=60f50755-19b6-4410-909e-36e869738d42) - Created
[0m17:03:19.992060 [debug] [ThreadPool]: SQL status: OK in 0.380 seconds
[0m17:03:19.994058 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=60f50755-19b6-4410-909e-36e869738d42, command-id=8ef0845d-db30-4103-8c26-b668195484c7) - Closing
[0m17:03:20.008239 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:20.008239 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:03:20.016870 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:03:20.017916 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:03:20.017916 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:20.207887 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=cb1f78ba-a09c-496f-8e0c-992cfe8bd1f9) - Created
[0m17:03:20.582157 [debug] [ThreadPool]: SQL status: OK in 0.560 seconds
[0m17:03:20.588135 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=cb1f78ba-a09c-496f-8e0c-992cfe8bd1f9, command-id=9a248216-ef41-485c-a24b-ebac9c33d1ae) - Closing
[0m17:03:20.589431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A10D710E90>]}
[0m17:03:20.593443 [debug] [Thread-1 (]: Began running node model.jra_dbt.linkedin_cleaned
[0m17:03:20.593443 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.linkedin_cleaned ..................... [RUN]
[0m17:03:20.594427 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.linkedin_cleaned, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:20.594427 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.linkedin_cleaned'
[0m17:03:20.594427 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.linkedin_cleaned
[0m17:03:20.603739 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.linkedin_cleaned"
[0m17:03:20.604690 [debug] [Thread-1 (]: Began executing node model.jra_dbt.linkedin_cleaned
[0m17:03:20.632776 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:03:20.640809 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:03:20.640809 [debug] [Thread-1 (]: Safe create: False
[0m17:03:20.653576 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_cleaned"
[0m17:03:20.655508 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_cleaned"
[0m17:03:20.655508 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_cleaned"} */

      create or replace temporary view `linkedin_cleaned__dbt_tmp` as
      SELECT 
    url, 
    job_posting_id, 
    job_title, 
    company_name, 
    job_location, 
    job_summary,
    job_seniority_level, 
    job_function, 
    job_employment_type, 
    job_industries,
    base_salary.min_amount AS min_salary, 
    base_salary.max_amount AS max_salary,
    date_format(to_date(job_posted_date, 'yyyy-MM-dd\'T\'HH:mm:ss.SSS\'Z\''), 'yyyy-MM-dd') AS job_posted_date, 
    timestamp AS scraped_dts, 
    ingest_dts,
    _rescued_data,
    0 AS is_enriched, 
    1 AS job_source, 
    1 AS is_active,
    -- Partition columns based on scraped_time
    year(current_date()) as year,
    month(current_date()) as month,
    day(current_date()) as day

FROM `demo`.`bronze`.`linkedin_jobs`
    
[0m17:03:20.655508 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:03:20.865576 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19) - Created
[0m17:03:21.156129 [debug] [Thread-1 (]: SQL status: OK in 0.500 seconds
[0m17:03:21.159317 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19, command-id=8321e02a-fc2d-4669-b9f1-810b2b39a73b) - Closing
[0m17:03:21.159317 [debug] [Thread-1 (]: No existing relation found
[0m17:03:21.169733 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_cleaned"
[0m17:03:21.169733 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_cleaned"} */

    
DESCRIBE TABLE `linkedin_cleaned__dbt_tmp`

  
[0m17:03:21.419882 [debug] [Thread-1 (]: SQL status: OK in 0.250 seconds
[0m17:03:21.422299 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19, command-id=89ece49e-baeb-49bc-90e8-6ea997475b17) - Closing
[0m17:03:21.455919 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.linkedin_cleaned"
[0m17:03:21.456966 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_cleaned"
[0m17:03:21.456966 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_cleaned"} */

    
  create or replace table `demo`.`silver`.`linkedin_cleaned`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_summary string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int,
    
      year int,
    
      month int,
    
      day int
    
    
  )

  
  using delta
  
  partitioned by (year,month,day)
  
  
  location 's3://jla-data-silver/linkedin_cleaned'
  
  

  
[0m17:03:22.619790 [debug] [Thread-1 (]: SQL status: OK in 1.160 seconds
[0m17:03:22.619790 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19, command-id=aaa495e5-966e-485a-b880-0a8761f5ff7a) - Closing
[0m17:03:22.623819 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:03:22.623819 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.linkedin_cleaned"
[0m17:03:22.623819 [debug] [Thread-1 (]: On model.jra_dbt.linkedin_cleaned: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.linkedin_cleaned"} */

    insert into `demo`.`silver`.`linkedin_cleaned` select * from `linkedin_cleaned__dbt_tmp`
  
[0m17:03:25.599582 [debug] [Thread-1 (]: SQL status: OK in 2.970 seconds
[0m17:03:25.600566 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19, command-id=938bee31-d7fa-4b05-b642-903d74cda1a9) - Closing
[0m17:03:25.616296 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '643ea9a4-394d-49dc-a55b-30ba5bbf4c03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1660E40E0>]}
[0m17:03:25.616296 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model silver.linkedin_cleaned ................ [[32mOK[0m in 5.02s]
[0m17:03:25.617676 [debug] [Thread-1 (]: Finished running node model.jra_dbt.linkedin_cleaned
[0m17:03:25.620246 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=5.030815601348877s, language=None, compute-name=) - Reusing connection previously named master
[0m17:03:25.620246 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:03:25.620246 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:03:25.620246 [debug] [MainThread]: On list_demo: Close
[0m17:03:25.621246 [debug] [MainThread]: Databricks adapter: Connection(session-id=60f50755-19b6-4410-909e-36e869738d42) - Closing
[0m17:03:25.692968 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:03:25.692968 [debug] [MainThread]: On list_demo_silver: Close
[0m17:03:25.693984 [debug] [MainThread]: Databricks adapter: Connection(session-id=cb1f78ba-a09c-496f-8e0c-992cfe8bd1f9) - Closing
[0m17:03:25.769668 [debug] [MainThread]: Connection 'model.jra_dbt.linkedin_cleaned' was properly closed.
[0m17:03:25.769668 [debug] [MainThread]: On model.jra_dbt.linkedin_cleaned: Close
[0m17:03:25.770643 [debug] [MainThread]: Databricks adapter: Connection(session-id=7f9235b0-3fc2-4d7a-ab2a-59906df84a19) - Closing
[0m17:03:26.022993 [info ] [MainThread]: 
[0m17:03:26.022993 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.42 seconds (6.42s).
[0m17:03:26.023992 [debug] [MainThread]: Command end result
[0m17:03:26.048237 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:26.050230 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:26.054662 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:03:26.054662 [info ] [MainThread]: 
[0m17:03:26.055689 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:03:26.055689 [info ] [MainThread]: 
[0m17:03:26.056713 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:03:26.056713 [debug] [MainThread]: Command `dbt run` succeeded at 17:03:26.056713 after 8.36 seconds
[0m17:03:26.056713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A168BD2660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A16A207140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A16A1C3DA0>]}
[0m17:03:26.057729 [debug] [MainThread]: Flushing usage events
[0m17:03:27.070499 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:03:31.541611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40A2F4920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40886A0C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40A7C7CE0>]}


============================== 17:03:31.545816 | 169f9d49-73b4-4d0e-a64b-eecfe12cb45d ==============================
[0m17:03:31.545816 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:03:31.545816 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select jobs_description', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m17:03:32.259216 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:03:32.259216 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:03:32.259216 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:03:32.881819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42DFB29F0>]}
[0m17:03:32.939263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40AF3EBD0>]}
[0m17:03:32.939263 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:03:33.165639 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:03:33.268467 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:03:33.269466 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:03:33.274018 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:03:33.292498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42E358D40>]}
[0m17:03:33.345186 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:33.347187 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:33.366435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42F5C82F0>]}
[0m17:03:33.367437 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:03:33.367437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42F5C0110>]}
[0m17:03:33.368446 [info ] [MainThread]: 
[0m17:03:33.369436 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:03:33.369436 [info ] [MainThread]: 
[0m17:03:33.370445 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:33.370445 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:03:33.371669 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:33.371669 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:03:33.371669 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:03:33.371669 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:03:33.372725 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:33.688029 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=efa0e6b0-3178-46de-9260-f7038aa65fd9) - Created
[0m17:03:33.909880 [debug] [ThreadPool]: SQL status: OK in 0.540 seconds
[0m17:03:33.913868 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=efa0e6b0-3178-46de-9260-f7038aa65fd9, command-id=dfd57efa-4cce-4c90-add5-172da794c7e8) - Closing
[0m17:03:33.924501 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:33.925489 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:03:33.933485 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:03:33.934486 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:03:33.934486 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:03:34.132007 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=3f8ad6ea-1a2e-4205-92d3-d807581cfb56) - Created
[0m17:03:34.709641 [debug] [ThreadPool]: SQL status: OK in 0.780 seconds
[0m17:03:34.719559 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=3f8ad6ea-1a2e-4205-92d3-d807581cfb56, command-id=b0a6b933-2957-4f63-8e47-564ee307bbf0) - Closing
[0m17:03:34.720974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42E1598B0>]}
[0m17:03:34.726058 [debug] [Thread-1 (]: Began running node model.jra_dbt.jobs_description
[0m17:03:34.726058 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.jobs_description ..................... [RUN]
[0m17:03:34.727058 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.jobs_description, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:03:34.727058 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.jobs_description'
[0m17:03:34.728058 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.jobs_description
[0m17:03:34.738203 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.jobs_description"
[0m17:03:34.739057 [debug] [Thread-1 (]: Began executing node model.jra_dbt.jobs_description
[0m17:03:34.763890 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:03:34.772372 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:03:34.772372 [debug] [Thread-1 (]: Safe create: False
[0m17:03:34.784815 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_description"
[0m17:03:34.785846 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_description"
[0m17:03:34.785846 [debug] [Thread-1 (]: On model.jra_dbt.jobs_description: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

      create or replace temporary view `jobs_description__dbt_tmp` as
      SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`linkedin_cleaned`

UNION ALL 

SELECT 
    
    job_posting_id, job_summary,scraped_dts
    
FROM `demo`.`silver`.`indeed_cleaned`
    
[0m17:03:34.785846 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:03:34.989159 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed) - Created
[0m17:03:35.414714 [debug] [Thread-1 (]: SQL status: OK in 0.630 seconds
[0m17:03:35.416720 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed, command-id=84ddcb49-2005-4206-810c-5c2032a29163) - Closing
[0m17:03:35.416720 [debug] [Thread-1 (]: No existing relation found
[0m17:03:35.424718 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_description"
[0m17:03:35.424718 [debug] [Thread-1 (]: On model.jra_dbt.jobs_description: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

    
DESCRIBE TABLE `jobs_description__dbt_tmp`

  
[0m17:03:35.686510 [debug] [Thread-1 (]: SQL status: OK in 0.260 seconds
[0m17:03:35.691579 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed, command-id=8384b2b9-fe91-48dc-aa8c-996a341ff568) - Closing
[0m17:03:35.724953 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_description"
[0m17:03:35.725960 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_description"
[0m17:03:35.725960 [debug] [Thread-1 (]: On model.jra_dbt.jobs_description: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

    
  create or replace table `demo`.`silver`.`jobs_description`
  
  (
    
      job_posting_id string,
    
      job_summary string,
    
      scraped_dts timestamp
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/jobs_description'
  
  

  
[0m17:03:36.856937 [debug] [Thread-1 (]: SQL status: OK in 1.130 seconds
[0m17:03:36.857984 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed, command-id=1a5395f1-aa1e-4b20-9951-02520c5be13a) - Closing
[0m17:03:36.861434 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:03:36.861434 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_description"
[0m17:03:36.861434 [debug] [Thread-1 (]: On model.jra_dbt.jobs_description: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_description"} */

    insert into `demo`.`silver`.`jobs_description` select * from `jobs_description__dbt_tmp`
  
[0m17:03:39.401629 [debug] [Thread-1 (]: SQL status: OK in 2.540 seconds
[0m17:03:39.404624 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed, command-id=8c517378-151f-4e32-a62e-b96a35c1f795) - Closing
[0m17:03:39.420675 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '169f9d49-73b4-4d0e-a64b-eecfe12cb45d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C407E3FE90>]}
[0m17:03:39.420675 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model silver.jobs_description ................ [[32mOK[0m in 4.69s]
[0m17:03:39.423008 [debug] [Thread-1 (]: Finished running node model.jra_dbt.jobs_description
[0m17:03:39.424086 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=4.703111886978149s, language=None, compute-name=) - Reusing connection previously named master
[0m17:03:39.425087 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:03:39.425087 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:03:39.425087 [debug] [MainThread]: On list_demo: Close
[0m17:03:39.426156 [debug] [MainThread]: Databricks adapter: Connection(session-id=efa0e6b0-3178-46de-9260-f7038aa65fd9) - Closing
[0m17:03:39.503502 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:03:39.504565 [debug] [MainThread]: On list_demo_silver: Close
[0m17:03:39.505552 [debug] [MainThread]: Databricks adapter: Connection(session-id=3f8ad6ea-1a2e-4205-92d3-d807581cfb56) - Closing
[0m17:03:39.568895 [debug] [MainThread]: Connection 'model.jra_dbt.jobs_description' was properly closed.
[0m17:03:39.569923 [debug] [MainThread]: On model.jra_dbt.jobs_description: Close
[0m17:03:39.570922 [debug] [MainThread]: Databricks adapter: Connection(session-id=989e7e3c-470f-431a-8c90-0c021dde34ed) - Closing
[0m17:03:39.802100 [info ] [MainThread]: 
[0m17:03:39.802100 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.43 seconds (6.43s).
[0m17:03:39.803406 [debug] [MainThread]: Command end result
[0m17:03:39.827245 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:39.829778 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:39.833828 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:03:39.834791 [info ] [MainThread]: 
[0m17:03:39.834791 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:03:39.834791 [info ] [MainThread]: 
[0m17:03:39.835856 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:03:39.836856 [debug] [MainThread]: Command `dbt run` succeeded at 17:03:39.836856 after 8.40 seconds
[0m17:03:39.836856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40A2F4920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C42E3C7AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C40AE1DE80>]}
[0m17:03:39.836856 [debug] [MainThread]: Flushing usage events
[0m17:03:40.793246 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:03:47.207950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C9E9D31A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C9E4D97C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C9E963890>]}


============================== 17:03:47.211481 | f7f90920-93a5-48dc-970b-a190107bba47 ==============================
[0m17:03:47.211481 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:03:47.211481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'quiet': 'False', 'log_format': 'default', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'no_print': 'None', 'fail_fast': 'False', 'invocation_command': 'dbt run --select jobs_unfied', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m17:03:47.928937 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:03:47.928937 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:03:47.929927 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:03:48.552618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7f90920-93a5-48dc-970b-a190107bba47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C9E3A58E0>]}
[0m17:03:48.610913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7f90920-93a5-48dc-970b-a190107bba47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC1A0E330>]}
[0m17:03:48.610913 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:03:48.840147 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:03:48.939518 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:03:48.939518 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:03:48.944513 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:03:48.964538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7f90920-93a5-48dc-970b-a190107bba47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC21EAE10>]}
[0m17:03:49.015561 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:49.017563 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:49.040132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7f90920-93a5-48dc-970b-a190107bba47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC3643B00>]}
[0m17:03:49.040132 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:03:49.040132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7f90920-93a5-48dc-970b-a190107bba47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC243F110>]}
[0m17:03:49.041134 [warn ] [MainThread]: The selection criterion 'jobs_unfied' does not match any enabled nodes
[0m17:03:49.042132 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m17:03:49.043132 [debug] [MainThread]: Command end result
[0m17:03:49.061927 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:03:49.062897 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:03:49.065889 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:03:49.066870 [debug] [MainThread]: Command `dbt run` succeeded at 17:03:49.066870 after 1.96 seconds
[0m17:03:49.066870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC2242AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CC21DE4B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C9EDBCA40>]}
[0m17:03:49.066870 [debug] [MainThread]: Flushing usage events
[0m17:03:50.030393 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:04:01.205961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241780916A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024178091A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024178091970>]}


============================== 17:04:01.210489 | 5b7aa518-f75f-4d58-b316-02a90b171ee7 ==============================
[0m17:04:01.210489 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:04:01.210489 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'target_path': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'empty': 'False', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'invocation_command': 'dbt run --select jobs_unified', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_experimental_parser': 'False'}
[0m17:04:01.917757 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:04:01.917757 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:04:01.917757 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:04:02.559846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241764A9730>]}
[0m17:04:02.619068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241764A97F0>]}
[0m17:04:02.619068 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:04:02.859098 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:04:02.958267 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:04:02.958267 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:04:02.963267 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:04:02.982873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002411A552240>]}
[0m17:04:03.034384 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:04:03.036427 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:04:03.054993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002411B7481A0>]}
[0m17:04:03.054993 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:04:03.054993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002411A2E3980>]}
[0m17:04:03.056410 [info ] [MainThread]: 
[0m17:04:03.057613 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:04:03.057613 [info ] [MainThread]: 
[0m17:04:03.057613 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:04:03.058690 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:04:03.059366 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:04:03.059877 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:04:03.059877 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:04:03.059877 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:04:03.060885 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:03.298818 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=4efb4f87-0060-4ba9-a14a-f8fc7c0fbc69) - Created
[0m17:04:03.426157 [debug] [ThreadPool]: SQL status: OK in 0.370 seconds
[0m17:04:03.429205 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=4efb4f87-0060-4ba9-a14a-f8fc7c0fbc69, command-id=640aaf73-cd4c-491a-acd5-095e18dfa366) - Closing
[0m17:04:03.440954 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:04:03.440954 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:04:03.452883 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:04:03.452883 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:04:03.453883 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:03.728863 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=1d9a4914-8cba-4f76-b2b2-0619874e8ca3) - Created
[0m17:04:04.125664 [debug] [ThreadPool]: SQL status: OK in 0.670 seconds
[0m17:04:04.131035 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=1d9a4914-8cba-4f76-b2b2-0619874e8ca3, command-id=21d766a0-7b56-4821-920f-4a67ed8044b4) - Closing
[0m17:04:04.133588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024179BA73E0>]}
[0m17:04:04.137795 [debug] [Thread-1 (]: Began running node model.jra_dbt.jobs_unified
[0m17:04:04.137795 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.jobs_unified ......................... [RUN]
[0m17:04:04.139043 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.jobs_unified, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:04:04.139043 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.jobs_unified'
[0m17:04:04.139043 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.jobs_unified
[0m17:04:04.148685 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.jobs_unified"
[0m17:04:04.149644 [debug] [Thread-1 (]: Began executing node model.jra_dbt.jobs_unified
[0m17:04:04.177371 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:04:04.185650 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:04:04.185650 [debug] [Thread-1 (]: Safe create: False
[0m17:04:04.198965 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_unified"
[0m17:04:04.199959 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:04:04.199959 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

      create or replace temporary view `jobs_unified__dbt_tmp` as
      with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`silver`.`linkedin_cleaned`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`silver`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件
    
[0m17:04:04.200966 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:04:04.423929 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=6b8d77a6-f2f2-420c-bb31-817171e71f1a) - Created
[0m17:04:05.019078 [debug] [Thread-1 (]: SQL status: OK in 0.820 seconds
[0m17:04:05.022225 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b8d77a6-f2f2-420c-bb31-817171e71f1a, command-id=c6ac29dc-810b-40d8-9c39-f78eae65cbb0) - Closing
[0m17:04:05.023223 [debug] [Thread-1 (]: No existing relation found
[0m17:04:05.032769 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:04:05.032769 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    
DESCRIBE TABLE `jobs_unified__dbt_tmp`

  
[0m17:04:05.411416 [debug] [Thread-1 (]: SQL status: OK in 0.380 seconds
[0m17:04:05.415452 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=6b8d77a6-f2f2-420c-bb31-817171e71f1a, command-id=3af15558-499a-4530-a386-ed654ceda1c2) - Closing
[0m17:04:05.445923 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_unified"
[0m17:04:05.446849 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:04:05.446849 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    
  create or replace table `demo`.`silver`.`jobs_unified`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/jobs_unified'
  
  

  
[0m17:04:06.138362 [debug] [Thread-1 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    
  create or replace table `demo`.`silver`.`jobs_unified`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/jobs_unified'
  
  

  
: [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.

== Specified ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: double (nullable = true)
 |-- max_salary: double (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_dts: timestamp (nullable = true)
 |-- ingest_dts: timestamp (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)


== Existing ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: long (nullable = true)
 |-- max_salary: long (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_time: string (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)
 |-- source_file: string (nullable = true)
 |-- ingest_dts: date (nullable = true)


== Differences ==
- Specified schema is missing field(s): scraped_time, source_file
- Specified schema has additional field(s): scraped_dts
- Specified type for max_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for min_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for ingest_dts is different from existing schema:
  Specified: timestamp
  Existing:  date

If your intention is to keep the existing schema, you can omit the
schema from the create table command. Otherwise please ensure that
the schema matches.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_CREATE_TABLE_SCHEME_MISMATCH] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.

== Specified ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: double (nullable = true)
 |-- max_salary: double (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_dts: timestamp (nullable = true)
 |-- ingest_dts: timestamp (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)


== Existing ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: long (nullable = true)
 |-- max_salary: long (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_time: string (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)
 |-- source_file: string (nullable = true)
 |-- ingest_dts: date (nullable = true)


== Differences ==
- Specified schema is missing field(s): scraped_time, source_file
- Specified schema has additional field(s): scraped_dts
- Specified type for max_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for min_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for ingest_dts is different from existing schema:
  Specified: timestamp
  Existing:  date

If your intention is to keep the existing schema, you can omit the
schema from the create table command. Otherwise please ensure that
the schema matches.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:998)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:764)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:688)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:558)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:124)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:232)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:73)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:535)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:521)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:571)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.

== Specified ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: double (nullable = true)
 |-- max_salary: double (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_dts: timestamp (nullable = true)
 |-- ingest_dts: timestamp (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)


== Existing ==
root
 |-- url: string (nullable = true)
 |-- job_posting_id: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- company_name: string (nullable = true)
 |-- job_location: string (nullable = true)
 |-- job_seniority_level: string (nullable = true)
 |-- job_function: string (nullable = true)
 |-- job_employment_type: string (nullable = true)
 |-- job_industries: string (nullable = true)
 |-- min_salary: long (nullable = true)
 |-- max_salary: long (nullable = true)
 |-- job_posted_date: string (nullable = true)
 |-- scraped_time: string (nullable = true)
 |-- _rescued_data: string (nullable = true)
 |-- is_enriched: integer (nullable = true)
 |-- job_source: integer (nullable = true)
 |-- is_active: integer (nullable = true)
 |-- source_file: string (nullable = true)
 |-- ingest_dts: date (nullable = true)


== Differences ==
- Specified schema is missing field(s): scraped_time, source_file
- Specified schema has additional field(s): scraped_dts
- Specified type for max_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for min_salary is different from existing schema:
  Specified: double
  Existing:  long
- Specified type for ingest_dts is different from existing schema:
  Specified: timestamp
  Existing:  date

If your intention is to keep the existing schema, you can omit the
schema from the create table command. Otherwise please ensure that
the schema matches.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.createTableWithDifferentSchemaException(DeltaErrors.scala:1509)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.createTableWithDifferentSchemaException$(DeltaErrors.scala:1504)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.createTableWithDifferentSchemaException(DeltaErrors.scala:3955)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.verifyTableMetadata(CreateDeltaTableCommand.scala:1100)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:838)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:854)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:759)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:684)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:824)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:824)
	... 43 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.createTableWithDifferentSchemaException(DeltaErrors.scala:1509)
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.createTableWithDifferentSchemaException$(DeltaErrors.scala:1504)
		at com.databricks.sql.transaction.tahoe.DeltaErrors$.createTableWithDifferentSchemaException(DeltaErrors.scala:3955)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.verifyTableMetadata(CreateDeltaTableCommand.scala:1100)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.createActionsForNewTableOrVerify$1(CreateDeltaTableCommand.scala:838)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTable(CreateDeltaTableCommand.scala:854)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:447)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:254)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:371)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$9(CreateDeltaTableCommand.scala:294)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:56)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$5(CreateDeltaTableCommand.scala:290)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:88)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:251)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:644)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:266)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1641)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:140)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1600)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:314)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:140)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:139)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:132)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)
		at scala.util.Try$.apply(Try.scala:213)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 54 more
, operation-id=1a6a67cd-0b5a-4e9a-ae17-a102f2dd49bc
[0m17:04:06.144934 [debug] [Thread-1 (]: Database Error in model jobs_unified (models\test\jobs_unified.sql)
  [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.
  
  == Specified ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: double (nullable = true)
   |-- max_salary: double (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_dts: timestamp (nullable = true)
   |-- ingest_dts: timestamp (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
  
  
  == Existing ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: long (nullable = true)
   |-- max_salary: long (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_time: string (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
   |-- source_file: string (nullable = true)
   |-- ingest_dts: date (nullable = true)
  
  
  == Differences ==
  - Specified schema is missing field(s): scraped_time, source_file
  - Specified schema has additional field(s): scraped_dts
  - Specified type for max_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for min_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for ingest_dts is different from existing schema:
    Specified: timestamp
    Existing:  date
  
  If your intention is to keep the existing schema, you can omit the
  schema from the create table command. Otherwise please ensure that
  the schema matches.
  compiled code at target\run\jra_dbt\models\test\jobs_unified.sql
[0m17:04:06.146946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b7aa518-f75f-4d58-b316-02a90b171ee7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241763F6030>]}
[0m17:04:06.146946 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model silver.jobs_unified ................ [[31mERROR[0m in 2.01s]
[0m17:04:06.148871 [debug] [Thread-1 (]: Finished running node model.jra_dbt.jobs_unified
[0m17:04:06.148871 [debug] [Thread-4 (]: Marking all children of 'model.jra_dbt.jobs_unified' to be skipped because of status 'error'.  Reason: Database Error in model jobs_unified (models\test\jobs_unified.sql)
  [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.
  
  == Specified ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: double (nullable = true)
   |-- max_salary: double (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_dts: timestamp (nullable = true)
   |-- ingest_dts: timestamp (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
  
  
  == Existing ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: long (nullable = true)
   |-- max_salary: long (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_time: string (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
   |-- source_file: string (nullable = true)
   |-- ingest_dts: date (nullable = true)
  
  
  == Differences ==
  - Specified schema is missing field(s): scraped_time, source_file
  - Specified schema has additional field(s): scraped_dts
  - Specified type for max_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for min_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for ingest_dts is different from existing schema:
    Specified: timestamp
    Existing:  date
  
  If your intention is to keep the existing schema, you can omit the
  schema from the create table command. Otherwise please ensure that
  the schema matches.
  compiled code at target\run\jra_dbt\models\test\jobs_unified.sql.
[0m17:04:06.150867 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=2.0163190364837646s, language=None, compute-name=) - Reusing connection previously named master
[0m17:04:06.150867 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:04:06.151901 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:04:06.151901 [debug] [MainThread]: On list_demo: Close
[0m17:04:06.153018 [debug] [MainThread]: Databricks adapter: Connection(session-id=4efb4f87-0060-4ba9-a14a-f8fc7c0fbc69) - Closing
[0m17:04:06.240371 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:04:06.240371 [debug] [MainThread]: On list_demo_silver: Close
[0m17:04:06.240371 [debug] [MainThread]: Databricks adapter: Connection(session-id=1d9a4914-8cba-4f76-b2b2-0619874e8ca3) - Closing
[0m17:04:06.296475 [debug] [MainThread]: Connection 'model.jra_dbt.jobs_unified' was properly closed.
[0m17:04:06.296475 [debug] [MainThread]: On model.jra_dbt.jobs_unified: Close
[0m17:04:06.297570 [debug] [MainThread]: Databricks adapter: Connection(session-id=6b8d77a6-f2f2-420c-bb31-817171e71f1a) - Closing
[0m17:04:06.522150 [info ] [MainThread]: 
[0m17:04:06.524108 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 3.46 seconds (3.46s).
[0m17:04:06.528158 [debug] [MainThread]: Command end result
[0m17:04:06.558189 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:04:06.560199 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:04:06.565497 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:04:06.565497 [info ] [MainThread]: 
[0m17:04:06.565497 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m17:04:06.566512 [info ] [MainThread]: 
[0m17:04:06.566512 [error] [MainThread]: [31mFailure in model jobs_unified (models\test\jobs_unified.sql)[0m
[0m17:04:06.566512 [error] [MainThread]:   Database Error in model jobs_unified (models\test\jobs_unified.sql)
  [DELTA_CREATE_TABLE_SCHEME_MISMATCH] The specified schema does not match the existing schema at s3://jla-data-silver/jobs_unified.
  
  == Specified ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: double (nullable = true)
   |-- max_salary: double (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_dts: timestamp (nullable = true)
   |-- ingest_dts: timestamp (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
  
  
  == Existing ==
  root
   |-- url: string (nullable = true)
   |-- job_posting_id: string (nullable = true)
   |-- job_title: string (nullable = true)
   |-- company_name: string (nullable = true)
   |-- job_location: string (nullable = true)
   |-- job_seniority_level: string (nullable = true)
   |-- job_function: string (nullable = true)
   |-- job_employment_type: string (nullable = true)
   |-- job_industries: string (nullable = true)
   |-- min_salary: long (nullable = true)
   |-- max_salary: long (nullable = true)
   |-- job_posted_date: string (nullable = true)
   |-- scraped_time: string (nullable = true)
   |-- _rescued_data: string (nullable = true)
   |-- is_enriched: integer (nullable = true)
   |-- job_source: integer (nullable = true)
   |-- is_active: integer (nullable = true)
   |-- source_file: string (nullable = true)
   |-- ingest_dts: date (nullable = true)
  
  
  == Differences ==
  - Specified schema is missing field(s): scraped_time, source_file
  - Specified schema has additional field(s): scraped_dts
  - Specified type for max_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for min_salary is different from existing schema:
    Specified: double
    Existing:  long
  - Specified type for ingest_dts is different from existing schema:
    Specified: timestamp
    Existing:  date
  
  If your intention is to keep the existing schema, you can omit the
  schema from the create table command. Otherwise please ensure that
  the schema matches.
  compiled code at target\run\jra_dbt\models\test\jobs_unified.sql
[0m17:04:06.567836 [info ] [MainThread]: 
[0m17:04:06.567836 [info ] [MainThread]:   compiled code at target\compiled\jra_dbt\models\test\jobs_unified.sql
[0m17:04:06.568868 [info ] [MainThread]: 
[0m17:04:06.568868 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m17:04:06.570031 [debug] [MainThread]: Command `dbt run` failed at 17:04:06.568868 after 5.46 seconds
[0m17:04:06.570031 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002411A1CB1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024119B90770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002411A5EE4E0>]}
[0m17:04:06.570031 [debug] [MainThread]: Flushing usage events
[0m17:04:07.536778 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m17:05:33.473676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180AC1112B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180AC111880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180AC1117C0>]}


============================== 17:05:33.477852 | 2bc0c921-3b0f-457e-be20-126fda6c1405 ==============================
[0m17:05:33.477852 [info ] [MainThread]: Running with dbt=1.10.4
[0m17:05:33.477852 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'write_json': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': 'C:\\Users\\George\\.dbt', 'debug': 'False', 'static_parser': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'partial_parse': 'True', 'version_check': 'True', 'log_path': 'C:\\Users\\George\\Desktop\\JR AI engineer\\JRDE16-JLA\\jra_dbt\\logs', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'no_print': 'None', 'invocation_command': 'dbt run --select jobs_unified', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'target_path': 'None'}
[0m17:05:34.236104 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m17:05:34.236104 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m17:05:34.237097 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m17:05:34.925732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180A8065100>]}
[0m17:05:34.987465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180ADC20050>]}
[0m17:05:34.988471 [info ] [MainThread]: Registered adapter: databricks=1.10.4
[0m17:05:35.277813 [debug] [MainThread]: checksum: 073cb5a296270136b24f9c040fdebd0642ef9c263903ef463bfe7e190c538da2, vars: {}, profile: , target: , version: 1.10.4
[0m17:05:35.386595 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:05:35.387598 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:05:35.391719 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.jra_dbt.example
[0m17:05:35.410638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CE2EBDA0>]}
[0m17:05:35.467362 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:05:35.468395 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:05:35.489643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CF7B0320>]}
[0m17:05:35.489643 [info ] [MainThread]: Found 6 models, 8 sources, 682 macros
[0m17:05:35.489643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CE5494F0>]}
[0m17:05:35.490633 [info ] [MainThread]: 
[0m17:05:35.491671 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:05:35.491671 [info ] [MainThread]: 
[0m17:05:35.492633 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:05:35.492633 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:05:35.493637 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:05:35.493637 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo'
[0m17:05:35.493637 [debug] [ThreadPool]: Using databricks connection "list_demo"
[0m17:05:35.494668 [debug] [ThreadPool]: On list_demo: GetSchemas(database=demo, schema=None)
[0m17:05:35.494668 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:05:35.726176 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=2595f984-d120-461e-925e-9d9c32fa1a8d) - Created
[0m17:05:35.907122 [debug] [ThreadPool]: SQL status: OK in 0.410 seconds
[0m17:05:35.908126 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=2595f984-d120-461e-925e-9d9c32fa1a8d, command-id=b109a9dc-9143-4d3c-856c-4b41f0162c50) - Closing
[0m17:05:35.913768 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_demo_silver, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:05:35.914766 [debug] [ThreadPool]: Acquiring new databricks connection 'list_demo_silver'
[0m17:05:35.929197 [debug] [ThreadPool]: Using databricks connection "list_demo_silver"
[0m17:05:35.930198 [debug] [ThreadPool]: On list_demo_silver: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "connection_name": "list_demo_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'demo' 
  AND table_schema = 'silver'

  
[0m17:05:35.930198 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:05:36.118007 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=a3651085-b399-4b4c-a9c8-a8784bc2698d) - Created
[0m17:05:36.567040 [debug] [ThreadPool]: SQL status: OK in 0.640 seconds
[0m17:05:36.572040 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=a3651085-b399-4b4c-a9c8-a8784bc2698d, command-id=3dbc76e0-7ee9-4943-b212-e3e3920a90a0) - Closing
[0m17:05:36.573040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CE651FD0>]}
[0m17:05:36.576318 [debug] [Thread-1 (]: Began running node model.jra_dbt.jobs_unified
[0m17:05:36.576318 [info ] [Thread-1 (]: 1 of 1 START sql incremental model silver.jobs_unified ......................... [RUN]
[0m17:05:36.577376 [debug] [Thread-1 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.jra_dbt.jobs_unified, idle-time=0s, language=None, compute-name=) - Creating connection
[0m17:05:36.577376 [debug] [Thread-1 (]: Acquiring new databricks connection 'model.jra_dbt.jobs_unified'
[0m17:05:36.577376 [debug] [Thread-1 (]: Began compiling node model.jra_dbt.jobs_unified
[0m17:05:36.586366 [debug] [Thread-1 (]: Writing injected SQL for node "model.jra_dbt.jobs_unified"
[0m17:05:36.587376 [debug] [Thread-1 (]: Began executing node model.jra_dbt.jobs_unified
[0m17:05:36.612354 [debug] [Thread-1 (]: MATERIALIZING INCREMENTAL
[0m17:05:36.620355 [debug] [Thread-1 (]: USING V2 MATERIALIZATION
[0m17:05:36.621355 [debug] [Thread-1 (]: Safe create: False
[0m17:05:36.633884 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_unified"
[0m17:05:36.634884 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:05:36.634884 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

      create or replace temporary view `jobs_unified__dbt_tmp` as
      with source_unioned as (

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`silver`.`linkedin_cleaned`

    UNION ALL

    SELECT 
        url, 
        job_posting_id, 
        job_title, 
        company_name, 
        job_location,
        job_seniority_level, 
        job_function, 
        job_employment_type, 
        job_industries,
        min_salary,
        max_salary,
        job_posted_date,
        scraped_dts, 
        ingest_dts,
        _rescued_data,
        is_enriched,
        job_source,
        is_active
    FROM `demo`.`silver`.`indeed_cleaned`

)


SELECT * FROM source_unioned

-- 第 3 步：添加增量加载的过滤条件
    
[0m17:05:36.634884 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:05:36.845487 [debug] [Thread-1 (]: Databricks adapter: Connection(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38) - Created
[0m17:05:37.212819 [debug] [Thread-1 (]: SQL status: OK in 0.580 seconds
[0m17:05:37.214825 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38, command-id=6b037f25-c7b6-428a-898d-7d6bd12d560c) - Closing
[0m17:05:37.214825 [debug] [Thread-1 (]: No existing relation found
[0m17:05:37.227841 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:05:37.227841 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    
DESCRIBE TABLE `jobs_unified__dbt_tmp`

  
[0m17:05:37.455189 [debug] [Thread-1 (]: SQL status: OK in 0.230 seconds
[0m17:05:37.460164 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38, command-id=f76419ca-34a0-4d2f-9ad5-3c6e320b82fd) - Closing
[0m17:05:37.490933 [debug] [Thread-1 (]: Writing runtime sql for node "model.jra_dbt.jobs_unified"
[0m17:05:37.490933 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:05:37.491929 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    
  create or replace table `demo`.`silver`.`jobs_unified`
  
  (
    
      url string,
    
      job_posting_id string,
    
      job_title string,
    
      company_name string,
    
      job_location string,
    
      job_seniority_level string,
    
      job_function string,
    
      job_employment_type string,
    
      job_industries string,
    
      min_salary double,
    
      max_salary double,
    
      job_posted_date string,
    
      scraped_dts timestamp,
    
      ingest_dts timestamp,
    
      _rescued_data string,
    
      is_enriched int,
    
      job_source int,
    
      is_active int
    
    
  )

  
  using delta
  
  
  
  
  location 's3://jla-data-silver/jobs_unified'
  
  

  
[0m17:05:38.514657 [debug] [Thread-1 (]: SQL status: OK in 1.020 seconds
[0m17:05:38.515600 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38, command-id=50090d20-c1f7-401e-b038-893cc6fb3e5d) - Closing
[0m17:05:38.519260 [debug] [Thread-1 (]: Applying tags to relation None
[0m17:05:38.519260 [debug] [Thread-1 (]: Using databricks connection "model.jra_dbt.jobs_unified"
[0m17:05:38.520302 [debug] [Thread-1 (]: On model.jra_dbt.jobs_unified: /* {"app": "dbt", "dbt_version": "1.10.4", "dbt_databricks_version": "1.10.4", "databricks_sql_connector_version": "4.0.5", "profile_name": "jra_dbt", "target_name": "dev", "node_id": "model.jra_dbt.jobs_unified"} */

    insert into `demo`.`silver`.`jobs_unified` select * from `jobs_unified__dbt_tmp`
  
[0m17:05:41.161509 [debug] [Thread-1 (]: SQL status: OK in 2.640 seconds
[0m17:05:41.162570 [debug] [Thread-1 (]: Databricks adapter: Cursor(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38, command-id=02074f7f-cb05-4872-a588-d19a6d22c5ec) - Closing
[0m17:05:41.175688 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bc0c921-3b0f-457e-be20-126fda6c1405', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180A8064410>]}
[0m17:05:41.175688 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model silver.jobs_unified .................... [[32mOK[0m in 4.60s]
[0m17:05:41.176697 [debug] [Thread-1 (]: Finished running node model.jra_dbt.jobs_unified
[0m17:05:41.179131 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master, idle-time=4.605091333389282s, language=None, compute-name=) - Reusing connection previously named master
[0m17:05:41.179131 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:05:41.179131 [debug] [MainThread]: Connection 'list_demo' was properly closed.
[0m17:05:41.179131 [debug] [MainThread]: On list_demo: Close
[0m17:05:41.180192 [debug] [MainThread]: Databricks adapter: Connection(session-id=2595f984-d120-461e-925e-9d9c32fa1a8d) - Closing
[0m17:05:41.241561 [debug] [MainThread]: Connection 'list_demo_silver' was properly closed.
[0m17:05:41.242551 [debug] [MainThread]: On list_demo_silver: Close
[0m17:05:41.243550 [debug] [MainThread]: Databricks adapter: Connection(session-id=a3651085-b399-4b4c-a9c8-a8784bc2698d) - Closing
[0m17:05:41.299234 [debug] [MainThread]: Connection 'model.jra_dbt.jobs_unified' was properly closed.
[0m17:05:41.300234 [debug] [MainThread]: On model.jra_dbt.jobs_unified: Close
[0m17:05:41.300234 [debug] [MainThread]: Databricks adapter: Connection(session-id=f79f88c2-9da1-4d1e-b72f-324c17cf4d38) - Closing
[0m17:05:41.545949 [info ] [MainThread]: 
[0m17:05:41.546958 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 6.05 seconds (6.05s).
[0m17:05:41.547966 [debug] [MainThread]: Command end result
[0m17:05:41.568509 [debug] [MainThread]: Wrote artifact WritableManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\manifest.json
[0m17:05:41.570524 [debug] [MainThread]: Wrote artifact SemanticManifest to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\semantic_manifest.json
[0m17:05:41.574523 [debug] [MainThread]: Wrote artifact RunExecutionResult to C:\Users\George\Desktop\JR AI engineer\JRDE16-JLA\jra_dbt\target\run_results.json
[0m17:05:41.575526 [info ] [MainThread]: 
[0m17:05:41.575526 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:05:41.575526 [info ] [MainThread]: 
[0m17:05:41.576525 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m17:05:41.577555 [debug] [MainThread]: Command `dbt run` succeeded at 17:05:41.576525 after 8.21 seconds
[0m17:05:41.577555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CF671790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180AD8F4470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000180CC217BC0>]}
[0m17:05:41.577555 [debug] [MainThread]: Flushing usage events
[0m17:05:42.556345 [debug] [MainThread]: An error was encountered while trying to flush usage events
